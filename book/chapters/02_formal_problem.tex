\chapter{Formal Problem Setup}\label{ch:formal-problem}

This chapter lays the mathematical groundwork for the entire book.  We
introduce every object, every constraint, every operator, and every solver
semantic that the subsequent chapters will analyse, extend, or break.
Nothing here is assumed from prior chapters; every definition is
self-contained.  The reader who has internalised this chapter will have a
complete formal vocabulary for the endogenous pivot problem.

%% ═══════════════════════════════════════════════════════════════════
\section{The Event Graph}\label{sec:event-graph}
%% ═══════════════════════════════════════════════════════════════════

The basic data structure is a directed acyclic graph whose vertices are
events and whose edges encode causal or temporal dependencies.

\begin{definition}[Event graph]\label{def:event-graph}
An \textbf{event graph} is a tuple
\[
  G = (V,\, E,\, t,\, w,\, a),
\]
where:
\begin{enumerate}[label=(\roman*)]
  \item $V$ is a finite set of \textbf{events}.
  \item $E \subseteq V \times V$ is a set of directed \textbf{causal edges}.
        The pair $(u, v) \in E$ asserts that event $u$ is a causal
        antecedent of event~$v$.
  \item $t \colon V \to \R$ is a \textbf{timestamp function}.  For every
        edge $(u, v) \in E$ we require $t(u) < t(v)$, so that $(V, E)$
        is a DAG consistent with temporal ordering.
  \item $w \colon V \to \R_{\ge 0}$ is a \textbf{weight function}.  The
        value $w(v)$ measures the dramatic tension, importance, or
        information content of event~$v$.
  \item $a \colon V \to A$ is an \textbf{actor assignment}, where $A$ is
        a finite set of \textbf{agents} (characters, participants).  Each
        event is attributed to exactly one actor.
\end{enumerate}
\end{definition}

\begin{remark}[DAG structure]\label{rem:dag-structure}
The condition $t(u) < t(v)$ for every edge $(u, v) \in E$ guarantees
acyclicity: if a directed path $v_1 \to v_2 \to \cdots \to v_m$ exists,
then $t(v_1) < t(v_2) < \cdots < t(v_m)$, so $v_1 \neq v_m$ and no
directed cycle can form.  The timestamp function therefore provides a
topological ordering of the DAG.
\end{remark}

\paragraph{Deterministic tie-breaking.}
Multiple events may share the same weight.  To ensure that every
$\argmax$ operation in the sequel has a unique, deterministic outcome,
we impose a total order on events.

\begin{definition}[Event ordering]\label{def:event-ordering}
Every event $v \in V$ carries a unique identifier $\mathrm{id}(v) \in \N$.
Events are totally ordered by the lexicographic comparison on the tuple
\[
  \bigl(\, w(v),\; -t(v),\; \mathrm{id}(v) \,\bigr).
\]
That is, higher weight wins; among events of equal weight, the
\emph{later} event wins (since we negate the timestamp); and if both
weight and timestamp coincide, the event identifier breaks the tie.
\end{definition}

This ordering ensures that every invocation of $\argmax$ over any
non-empty subset of $V$ returns a single, well-defined event.

\begin{example}[A dinner-party event graph]\label{ex:dinner-party}
Consider a dinner-party simulation with $|A| = 6$ agents (Alice, Bob,
Carol, Dave, Eve, Frank) producing $|V| = 200$ events over the course
of an evening.  The event types include:
\begin{center}
\begin{tabular}{@{}llr@{}}
  \toprule
  Type & Description & Typical weight \\
  \midrule
  \textsc{Chat}        & Casual conversation    & $1$--$3$  \\
  \textsc{Observe}     & Noticing another actor  & $2$--$4$  \\
  \textsc{Conflict}    & Verbal disagreement     & $5$--$8$  \\
  \textsc{Catastrophe} & Major dramatic incident & $9$--$10$ \\
  \textsc{Reconcile}   & Resolution of conflict  & $4$--$6$  \\
  \bottomrule
\end{tabular}
\end{center}
Edges encode causal structure: a \textsc{Conflict} between Alice and Bob
has incoming edges from any preceding \textsc{Chat} or \textsc{Observe}
events that triggered it, and outgoing edges to subsequent
\textsc{Reconcile} events.  The resulting DAG has roughly $|E| \approx
350$ edges.  Weights reflect dramatic tension: a \textsc{Catastrophe}
(weight~$10$) dwarfs routine \textsc{Chat} events (weight~$1$--$3$).

The extraction problem, defined in \cref{sec:extraction-problem}, asks:
given a focal actor (say Alice), select a subset of events that tells
the most compelling story arc for Alice while obeying structural
grammar constraints.
\end{example}

%% ═══════════════════════════════════════════════════════════════════
\section{Focal Actor and Candidate Pool}\label{sec:focal-actor}
%% ═══════════════════════════════════════════════════════════════════

Not every event in the graph is relevant to every actor's story.  We
narrow the search space by fixing a protagonist and constructing the
set of events that could plausibly appear in that protagonist's
narrative arc.

\begin{definition}[Focal actor]\label{def:focal-actor}
The \textbf{focal actor} $a^* \in A$ is the agent whose story arc we
seek to extract.  An event $v \in V$ is called \textbf{focal} if
$a(v) = a^*$, and \textbf{non-focal} otherwise.
\end{definition}

\begin{definition}[Candidate pool]\label{def:candidate-pool}
Given a focal actor $a^*$ and an event graph $G$, the \textbf{candidate
pool} $P \subseteq V$ is constructed as follows.
\begin{enumerate}[label=\textbf{Step \arabic*.},leftmargin=3.5em]
  \item \textbf{Anchor selection.}\;
    Choose an anchor event $e_0 \in V$ with $a(e_0) = a^*$, typically
    the highest-weight focal event:
    \[
      e_0 = \argmax_{v \in V:\, a(v) = a^*} w(v).
    \]
  \item \textbf{Causal reachability.}\;
    Initialize $P \leftarrow \varnothing$.  Starting from $e_0$,
    perform a bidirectional BFS along the edges of $G$ (following both
    incoming and outgoing causal edges) to collect all events reachable
    from $e_0$ within the causal structure.  Add these events to $P$.
  \item \textbf{Focal injection.}\;
    Compute the focal actor's maximum-weight event:
    \[
      e^* = \argmax_{v \in V:\, a(v) = a^*} w(v).
    \]
    If $e^* \notin P$, add it: $P \leftarrow P \cup \{e^*\}$.
\end{enumerate}
\end{definition}

\begin{remark}[Structural necessity of injection]\label{rem:injection-necessity}
The injection in Step~3 is structurally necessary.  Without it, a
peripheral actor---one whose events are causally disconnected from the
main action---may have an empty or nearly empty candidate pool,
rendering the extraction problem trivial or vacuous.  Injection
guarantees that the candidate pool always contains at least one focal
event, namely the highest-weight one.

However, injection creates a subtle problem: the injected event $e^*$
may not be causally connected to the rest of the pool.  This
\emph{contamination} introduces a potential semantic discontinuity in
the extracted narrative.  We defer the full analysis of this issue to
\cref{ch:narrative}.
\end{remark}

\begin{remark}[Focal vs.\ non-focal events in $P$]\label{rem:focal-nonfocal}
The candidate pool $P$ typically contains both \textbf{focal events}
(involving $a^*$) and \textbf{non-focal events} (involving other actors
but causally connected to $a^*$'s story).  Non-focal events provide
context, setup, and development; focal events carry the protagonist's
direct actions and the potential turning points.
\end{remark}

%% ═══════════════════════════════════════════════════════════════════
\section{The Endogenous Turning Point}\label{sec:endogenous-tp}
%% ═══════════════════════════════════════════════════════════════════

The turning point---the single most dramatic moment in the
protagonist's arc---is not given exogenously.  It is determined by the
selection itself: the turning point is the highest-weight focal event
\emph{in the selected set}.  This endogenous coupling is the central
structural feature of the problem.

\begin{definition}[Endogenous turning point]\label{def:endogenous-tp}
Given a selected subset $S \subseteq P$, the \textbf{endogenous turning
point} is
\[
  \tp(S)
  \;=\;
  \argmax_{v \in S:\, a(v) = a^*} w(v),
\]
where ties are broken by the deterministic ordering of
\cref{def:event-ordering}.  If $S$ contains no focal events, $\tp(S)$
is undefined.
\end{definition}

The critical property of the endogenous turning point is that it
depends on $S$: changing which events are selected can change which
event serves as the turning point.

\begin{proposition}[Endogenous coupling]\label{prop:endogenous-coupling}
The turning-point function $\tp$ is not a constant of the problem
instance.  That is, there exist candidate pools $P$ and subsets
$S_1, S_2 \subseteq P$ such that $\tp(S_1) \neq \tp(S_2)$.
\end{proposition}

\begin{proof}
We construct an explicit example.  Let $P = \{e_1, e_2, e_3, e_4\}$
with the following attributes:
\begin{center}
\begin{tabular}{@{}cccc@{}}
  \toprule
  Event & Focal? & Weight & Timestamp \\
  \midrule
  $e_1$ & No  & 4 & 1 \\
  $e_2$ & Yes & 5 & 2 \\
  $e_3$ & Yes & 3 & 3 \\
  $e_4$ & Yes & 8 & 4 \\
  \bottomrule
\end{tabular}
\end{center}
Let $S_1 = \{e_1, e_2, e_3\}$.  The focal events in $S_1$ are $e_2$
(weight~5) and $e_3$ (weight~3), so $\tp(S_1) = e_2$.

Now let $S_2 = \{e_1, e_3, e_4\}$.  The focal events in $S_2$ are $e_3$
(weight~3) and $e_4$ (weight~8), so $\tp(S_2) = e_4$.

Since $\tp(S_1) = e_2 \neq e_4 = \tp(S_2)$, the turning point shifted
from $e_2$ to $e_4$ solely by changing the selected set.
\end{proof}

\begin{remark}[Why endogeneity matters]\label{rem:why-endogeneity}
In a standard constrained optimisation problem, the constraints are
fixed functions of the decision variables.  Here, the constraint
\emph{itself}---specifically, the identity of the turning point and the
resulting phase labelling---is a function of the solution.  This
circular dependency is what makes the extraction problem fundamentally
different from subset selection under monotone constraints, and it is
the root cause of every pathology analysed in this book.
\end{remark}

%% ═══════════════════════════════════════════════════════════════════
\section{The Phase Grammar}\label{sec:phase-grammar}
%% ═══════════════════════════════════════════════════════════════════

A well-formed narrative arc must progress through a sequence of dramatic
phases: setup, development, a turning point, and resolution.  We encode
this structural requirement as a deterministic finite automaton.

\begin{definition}[Phase grammar]\label{def:phase-grammar}
The \textbf{phase grammar} is a DFA
\[
  \mathcal{A} = (Q,\, \Sigma,\, \delta,\, q_0,\, F),
\]
with the following components.
\begin{enumerate}[label=(\roman*)]
  \item The \textbf{phase alphabet} is
        \[
          \Sigma = \{\,
            \textsc{Setup},\;
            \textsc{Development},\;
            \textsc{Turning\_Point},\;
            \textsc{Resolution}\,\}.
        \]
  \item The \textbf{state set} is
        \[
          Q = \{\,
            q_{\mathrm{setup}},\;
            q_{\mathrm{dev}},\;
            q_{\mathrm{tp}},\;
            q_{\mathrm{res}},\;
            q_{\mathrm{reject}}\,\}.
        \]
  \item The \textbf{initial state} is $q_0 = q_{\mathrm{setup}}$.
  \item The \textbf{accepting states} are $F = \{q_{\mathrm{res}}\}$.
  \item The \textbf{transition function} $\delta \colon Q \times \Sigma
        \to Q$ is defined by the following table, where any transition
        not listed sends the DFA to the reject state $q_{\mathrm{reject}}$,
        which is absorbing (all transitions from $q_{\mathrm{reject}}$
        lead back to $q_{\mathrm{reject}}$).
\end{enumerate}

\begin{center}
\begin{tabular}{@{}lll@{}}
  \toprule
  Current state & Input symbol & Next state \\
  \midrule
  $q_{\mathrm{setup}}$  & \textsc{Setup}          & $q_{\mathrm{setup}}$ \\
  $q_{\mathrm{setup}}$  & \textsc{Development}    & $q_{\mathrm{dev}}$ \\
  $q_{\mathrm{dev}}$    & \textsc{Development}    & $q_{\mathrm{dev}}$ \\
  $q_{\mathrm{dev}}$    & \textsc{Turning\_Point}  & $q_{\mathrm{tp}}$
    \quad\textnormal{(only if $\ge k$ \textsc{Development} symbols
    have been consumed)} \\
  $q_{\mathrm{tp}}$     & \textsc{Resolution}     & $q_{\mathrm{res}}$ \\
  $q_{\mathrm{res}}$    & \textsc{Resolution}     & $q_{\mathrm{res}}$ \\
  \bottomrule
\end{tabular}
\end{center}
The parameter $k \ge 1$ is the \textbf{prefix requirement}: the minimum
number of \textsc{Development} events that must appear before the
turning point.
\end{definition}

\begin{remark}[Monotonicity of transitions]\label{rem:monotonicity}
The DFA is \textbf{monotonic}: the implicit ordering
\[
  q_{\mathrm{setup}} \prec q_{\mathrm{dev}} \prec q_{\mathrm{tp}}
  \prec q_{\mathrm{res}}
\]
is never violated by a legal transition.  There is no edge from
$q_{\mathrm{dev}}$ back to $q_{\mathrm{setup}}$, no edge from
$q_{\mathrm{tp}}$ back to $q_{\mathrm{dev}}$, and no edge from
$q_{\mathrm{res}}$ to any earlier state.  Phase transitions only move
forward.  Any input that would require regression sends the DFA to the
absorbing reject state $q_{\mathrm{reject}}$.
\end{remark}

\begin{definition}[Grammar acceptance]\label{def:grammar-acceptance}
A sequence of phase labels $\sigma_1 \sigma_2 \cdots \sigma_n \in
\Sigma^*$ is \textbf{accepted} by $\mathcal{A}$ if and only if
\[
  \delta^*(q_0,\, \sigma_1 \sigma_2 \cdots \sigma_n) \in F,
\]
where $\delta^* \colon Q \times \Sigma^* \to Q$ is the extended
transition function defined recursively by $\delta^*(q, \varepsilon) = q$
and $\delta^*(q, \sigma_1 \cdots \sigma_n) = \delta^*(\delta(q,
\sigma_1),\, \sigma_2 \cdots \sigma_n)$.

Equivalently, a sequence is accepted if and only if it matches the
pattern
\[
  \textsc{Setup}^{\,*}\;
  \textsc{Development}^{\,\ge k}\;
  \textsc{Turning\_Point}\;
  \textsc{Resolution}^{\,\ge 1}.
\]
\end{definition}

\begin{example}[Accepted and rejected sequences]\label{ex:grammar-sequences}
Fix $k = 3$.  The following sequences illustrate the grammar:
\begin{enumerate}[label=(\alph*)]
  \item $\textsc{S},\, \textsc{D},\, \textsc{D},\, \textsc{D},\,
        \textsc{TP},\, \textsc{R}$: accepted (1~setup, 3~development,
        1~turning point, 1~resolution).
  \item $\textsc{D},\, \textsc{D},\, \textsc{D},\, \textsc{D},\,
        \textsc{TP},\, \textsc{R},\, \textsc{R}$: accepted (0~setup,
        4~development, 1~turning point, 2~resolution).
  \item $\textsc{S},\, \textsc{D},\, \textsc{D},\, \textsc{TP},\,
        \textsc{R}$: \textbf{rejected} --- only 2~development events
        before the turning point, but $k = 3$ is required.
  \item $\textsc{S},\, \textsc{D},\, \textsc{D},\, \textsc{D},\,
        \textsc{TP}$: \textbf{rejected} --- no resolution event after
        the turning point.
  \item $\textsc{S},\, \textsc{D},\, \textsc{TP},\, \textsc{D},\,
        \textsc{R}$: \textbf{rejected} --- a development event appears
        after the turning point, violating monotonicity.
\end{enumerate}
\end{example}

%% ═══════════════════════════════════════════════════════════════════
\section{The Phase Classifier}\label{sec:phase-classifier}
%% ═══════════════════════════════════════════════════════════════════

The grammar operates on phase labels, but the raw data consists of
events with timestamps and weights.  A \emph{phase classifier} bridges
the gap: it assigns each selected event a phase label based on its
temporal position relative to the turning point.

\begin{definition}[Position-based phase classifier]
\label{def:position-classifier}
Let $S \subseteq P$ be a selected subset with endogenous turning point
$\tp(S)$.  Order the events of $S$ by timestamp:
$v_1, v_2, \ldots, v_n$ with $t(v_1) \le t(v_2) \le \cdots \le t(v_n)$.
Let $\tp(S) = v_j$ for some index $j$.

The \textbf{position-based phase classifier}
$\varphi_{\tp} \colon S \to \Sigma$ assigns:
\[
  \varphi_{\tp}(v_i) =
  \begin{cases}
    \textsc{Setup} \text{ or } \textsc{Development}
      & \text{if } i < j, \\[3pt]
    \textsc{Turning\_Point}
      & \text{if } i = j, \\[3pt]
    \textsc{Resolution}
      & \text{if } i > j.
  \end{cases}
\]
For events before the turning point, the distinction between
\textsc{Setup} and \textsc{Development} is determined by a partition of
the pre-turning-point events: the first $s$ events (for some
$s \ge 0$) are labelled \textsc{Setup}, and the remaining events before
the turning point are labelled \textsc{Development}.
\end{definition}

\begin{definition}[Grammar-aware phase classifier]
\label{def:grammar-aware-classifier}
The \textbf{grammar-aware phase classifier} assigns phase labels to
maximise the probability of grammar acceptance.  Given a selected set
$S$ and turning point $\tp(S)$, it solves:
\[
  \varphi^*_{\tp}
  \;=\;
  \argmax_{\varphi \colon S \to \Sigma}
  \mathbf{1}\!\bigl[\,
    \delta^*(q_0,\, \varphi(v_1)\, \varphi(v_2)\, \cdots\, \varphi(v_n))
    \in F
  \,\bigr],
\]
subject to the constraint that $\varphi(v_j) = \textsc{Turning\_Point}$
where $v_j = \tp(S)$, and that the labelling respects the monotonic
phase ordering.

In practice, this reduces to choosing the optimal number of setup
events so that exactly $k$ or more development events appear before the
turning point.  The grammar-aware classifier resolves Class~B failures
(\cref{ch:taxonomy}) that arise when the position-based classifier
produces a label sequence that the DFA rejects.
\end{definition}

\begin{remark}[Circular dependency]\label{rem:circular-dependency}
Both classifiers depend on $\tp(S)$, which in turn depends on~$S$.  The
phase labels therefore depend on the selection, but the set of
\emph{valid} selections is precisely those sets whose phase labels
satisfy the grammar.  This circular dependency is the formal
manifestation of the endogenous coupling described informally in
\cref{prop:endogenous-coupling}: we cannot evaluate the constraint
without knowing the solution, and we cannot construct the solution
without evaluating the constraint.
\end{remark}

%% ═══════════════════════════════════════════════════════════════════
\section{The Extraction Problem}\label{sec:extraction-problem}
%% ═══════════════════════════════════════════════════════════════════

We now state the optimisation problem that the entire book studies.

\begin{definition}[Narrative extraction problem]\label{def:extraction-problem}
Given an event graph $G = (V, E, t, w, a)$, a focal actor $a^*$, a
candidate pool $P \subseteq V$, and a phase grammar $\mathcal{A}$ with
prefix requirement $k$, the \textbf{narrative extraction problem} is:
\begin{equation}\label{eq:extraction-problem}
  S^*
  \;=\;
  \argmax_{S \subseteq P}
  \sum_{v \in S} w(v)
\end{equation}
subject to:
\begin{enumerate}[label=(\alph*)]
  \item \textbf{Grammar constraint.}\;
    The phase-labelled sequence
    $\varphi_{\tp(S)}(v_1)\, \varphi_{\tp(S)}(v_2)\, \cdots\,
    \varphi_{\tp(S)}(v_{|S|})$ is accepted by $\mathcal{A}$,
    where $v_1, \ldots, v_{|S|}$ are the events of $S$ ordered by
    timestamp.
  \item \textbf{Cardinality bounds.}\;
    $n_{\min} \le |S| \le n_{\max}$, where $n_{\min}$ and $n_{\max}$
    are the minimum and maximum number of beats (events) in the
    extracted arc.
  \item \textbf{Protagonist coverage.}\;
    The fraction of focal events in $S$ is at least $\rho$:
    \[
      \frac{|\{v \in S : a(v) = a^*\}|}{|S|} \;\ge\; \rho.
    \]
  \item \textbf{Timespan constraint.}\;
    The extracted arc spans at least a fraction $\gamma$ of the total
    timeline:
    \[
      \frac{\max_{v \in S} t(v) - \min_{v \in S} t(v)}
           {\max_{v \in P} t(v) - \min_{v \in P} t(v)}
      \;\ge\; \gamma.
    \]
\end{enumerate}
\end{definition}

\begin{remark}[Endogenous constraint coupling]\label{rem:endogenous-constraint}
The grammar constraint~(a) is the source of all difficulty.  The
constraints~(b)--(d) are standard monotone set constraints: they can be
checked independently of $\tp(S)$ and do not exhibit endogenous
coupling.  Constraint~(a), by contrast, depends on $S$ through both the
selection itself (which events appear) and the turning point $\tp(S)$
(which determines the phase labelling).  The extraction problem is
therefore a \textbf{constrained combinatorial optimisation where the
constraint function depends on the solution through $\tp(S)$}.
\end{remark}

%% ═══════════════════════════════════════════════════════════════════
\section{Solver Semantics}\label{sec:solver-semantics}
%% ═══════════════════════════════════════════════════════════════════

The extraction problem admits multiple resolution strategies, differing
in how they handle the endogenous coupling.  We define four solver
semantics precisely.  Each corresponds to a distinct algorithmic
approach and to a distinct code path in the reference implementation.

%% ── 7.1  Committed ──────────────────────────────────────────────
\subsection{Committed Semantics ($M=1$)}\label{sec:committed}

\begin{definition}[Committed solver]\label{def:committed-semantics}
The \textbf{committed solver} proceeds in two stages:
\begin{enumerate}[label=\textbf{Stage \arabic*.}]
  \item \textbf{Pivot commitment.}\;
    Compute the turning point over the \emph{entire} candidate pool:
    \[
      \tp^{\mathrm{commit}}
      \;=\;
      \argmax_{v \in P:\, a(v) = a^*} w(v).
    \]
    This is the highest-weight focal event in $P$, determined before
    any selection takes place.
  \item \textbf{Constrained selection.}\;
    Fix $\tp^{\mathrm{commit}}$ as the turning point.  Find the
    subset $S^* \subseteq P$ that maximises $\sum_{v \in S} w(v)$
    subject to the grammar being satisfied with
    $\tp^{\mathrm{commit}}$ in the \textsc{Turning\_Point} role,
    plus all auxiliary constraints.
\end{enumerate}
The pivot identity is fixed at Stage~1 and \textbf{cannot be replaced}
during Stage~2.

\smallskip
\noindent
\textit{Code path:}
\texttt{solve\_with\_budget(events, k, M=1)} in
\texttt{src/compression.py}.
\end{definition}

%% ── 7.2  Endogenous composition ─────────────────────────────────
\subsection{Endogenous Composition Semantics}\label{sec:endo-comp-semantics}

\begin{definition}[Endogenous composition solver]
\label{def:endogenous-composition-semantics}
Under \textbf{endogenous composition}, the pivot is not fixed in
advance.  Events are processed in temporal order via a left fold.  At
each step, the current event is composed with the running context
element using the endogenous composition operator $\opendo$
(\cref{ch:context-algebra}).  If the new event is focal and has higher
weight than the current pivot, the pivot \textbf{shifts rightward} to
the new event.  The pivot is not locked until the full sequence has been
processed.

\smallskip
\noindent
\textit{Code path:}
\texttt{build\_tropical\_context()} in
\texttt{src/tropical\_semiring.py}.
\end{definition}

%% ── 7.3  Enumerative ────────────────────────────────────────────
\subsection{Enumerative Semantics (Top-$M$)}\label{sec:enumerative}

\begin{definition}[Enumerative solver]\label{def:enumerative-semantics}
The \textbf{enumerative solver} with parameter $M > 1$ proceeds as
follows:
\begin{enumerate}[label=\textbf{Step \arabic*.}]
  \item Rank all focal events in $P$ by weight.  Let
        $c_1, c_2, \ldots, c_M$ be the top-$M$ candidates (with
        $w(c_1) \ge w(c_2) \ge \cdots \ge w(c_M)$).
  \item For each candidate $c_i$, $i = 1, \ldots, M$: fix $c_i$ as
        the turning point and solve the constrained extraction problem
        from \cref{def:extraction-problem} with $\tp$ locked to $c_i$.
        Let $S_i^*$ denote the optimal solution for candidate~$c_i$
        (or $\varnothing$ if no feasible solution exists).
  \item Return the solution with the highest total weight:
        \[
          S^*
          \;=\;
          \argmax_{i \in \{1,\ldots,M\}:\, S_i^* \neq \varnothing}
          \sum_{v \in S_i^*} w(v).
        \]
\end{enumerate}

\smallskip
\noindent
\textit{Code path:}
\texttt{solve\_with\_budget(events, k, M=M)} in
\texttt{src/compression.py}, with $M > 1$.
\end{definition}

%% ── 7.4  Fixed-pivot ────────────────────────────────────────────
\subsection{Fixed-Pivot Semantics}\label{sec:fixed-pivot}

\begin{definition}[Fixed-pivot solver]\label{def:fixed-pivot}
The \textbf{fixed-pivot solver} is a diagnostic semantic.  It constrains
the solver to use the \textbf{full-sequence dominant pivot}: the turning
point that would be selected if the entire candidate pool $P$ were used
without any compression or truncation.  Formally:
\[
  \tp^{\mathrm{fixed}}
  \;=\;
  \argmax_{v \in P:\, a(v) = a^*} w(v).
\]
The solver then finds the best $S \subseteq P$ (or $S \subseteq P'$
for a compressed pool $P' \subseteq P$) with $\tp^{\mathrm{fixed}}$ as
the mandatory turning point.

The fixed-pivot semantic answers the question: \emph{can the original
narrative meaning be preserved under this compression?}  If the
fixed-pivot solver produces a feasible solution, the original turning
point survives.  If it does not, the compression has destroyed the
original meaning.
\end{definition}

%% ── 7.5  Relationships ─────────────────────────────────────────
\subsection{Relationships Between Semantics}\label{sec:semantics-relations}

\begin{remark}[The validity mirage and solver semantics]
\label{rem:mirage-semantics}
The relationship between these four semantics is the source of the
\textbf{validity mirage} analysed in \cref{ch:mirage}.  When the
committed or enumerative solver with $M > 1$ produces a valid solution,
the output satisfies every stated constraint.  But the turning point in
the output may differ from the full-sequence dominant pivot.  The
solution is \emph{technically valid but semantically different} from
what the full-sequence solution intended.

Concretely: the enumerative solver may find that candidate $c_1$
(the highest-weight focal event) cannot serve as turning point after
compression---there are too few development events before it.  It
then falls back to candidate $c_2$, which \emph{can} serve as turning
point.  The resulting arc is valid (it satisfies the grammar with
$c_2$ as turning point) but tells a \emph{different story} than the
original sequence.

The fixed-pivot semantic exposes this substitution: if the fixed-pivot
solver fails while the enumerative solver succeeds, a pivot
substitution has occurred.  This diagnostic is the principal tool for
detecting the validity mirage empirically.
\end{remark}

%% ═══════════════════════════════════════════════════════════════════
\section{The Greedy Policy}\label{sec:greedy-policy}
%% ═══════════════════════════════════════════════════════════════════

The simplest concrete algorithm for the extraction problem is the
greedy policy, which commits to a pivot and then fills slots in
weight-descending order.

\begin{definition}[Greedy extraction policy]\label{def:greedy-policy}
The \textbf{greedy policy} for the extraction problem operates as
follows.
\begin{enumerate}[label=\textbf{Step \arabic*.}]
  \item \textbf{Pivot pre-selection.}\;
    Compute $e^* = \argmax_{v \in P:\, a(v) = a^*} w(v)$.  Pre-select
    $e^*$ into the solution and assign it the label
    \textsc{Turning\_Point}.  Initialize $S \leftarrow \{e^*\}$.
  \item \textbf{Candidate ranking.}\;
    Sort the remaining events $P \setminus \{e^*\}$ in descending
    order of weight (using the deterministic tie-breaking of
    \cref{def:event-ordering}).
  \item \textbf{Greedy filling.}\;
    For each event $v$ in the sorted order:
    \begin{enumerate}[label=(\alph*)]
      \item Determine the best-available phase label for $v$ given the
            current DFA state and the temporal position of $v$ relative
            to $e^*$.
      \item If a valid label exists (i.e., adding $v$ with that label
            does not send the DFA to $q_{\mathrm{reject}}$), add $v$
            to $S$ and advance the DFA state.
      \item If no valid label exists, skip $v$.
    \end{enumerate}
    Continue until $|S| = n_{\max}$ or all candidates have been
    considered.
  \item \textbf{No backtracking.}\;
    Once an event is selected and labelled, the decision is
    \textbf{irrevocable}.  The greedy policy never reconsiders a
    previous choice.
\end{enumerate}
\end{definition}

\begin{remark}[Greedy as committed $M = 1$]\label{rem:greedy-committed}
The greedy policy is a \textbf{committed ($M = 1$) strategy} with the
additional property that it makes locally optimal weight choices at each
step.  The pivot is fixed at Step~1 (committed semantics), and the
remaining selections are made greedily.  The absence of backtracking
means the greedy policy can fail to find a feasible solution even when
one exists: it may consume all available development slots on
high-weight events that are temporally positioned after the turning
point, leaving too few development events before the turning point to
satisfy the prefix requirement~$k$.
\end{remark}

%% ═══════════════════════════════════════════════════════════════════
\section{Non-Matroid Structure}\label{sec:non-matroid}
%% ═══════════════════════════════════════════════════════════════════

The greedy policy has no performance guarantee for the extraction
problem.  This is not an artefact of our particular greedy strategy; it
is a structural property of the problem itself.  The feasible family
violates the axioms that would be needed to guarantee greedy optimality.

\begin{definition}[Feasible family]\label{def:feasible-family}
The \textbf{feasible family} for the extraction problem is
\[
  \mathcal{F}
  \;=\;
  \bigl\{\,
    S \subseteq P
    \;:\;
    \text{the phase-labelled sequence of } S
    \text{ is accepted by } \mathcal{A}
  \,\bigr\}.
\]
That is, $\mathcal{F}$ is the collection of all subsets of $P$ whose
induced phase labelling (using the endogenous turning point $\tp(S)$
and the grammar-aware classifier) satisfies the phase grammar.
\end{definition}

We recall the two axioms that define a matroid on ground set $P$.

\begin{itemize}
  \item \textbf{Hereditary axiom.}\; If $S \in \mathcal{F}$ and
        $S' \subseteq S$, then $S' \in \mathcal{F}$.
  \item \textbf{Exchange axiom.}\; If $S_1, S_2 \in \mathcal{F}$ with
        $|S_1| < |S_2|$, then there exists $v \in S_2 \setminus S_1$
        such that $S_1 \cup \{v\} \in \mathcal{F}$.
\end{itemize}

\begin{proposition}[Non-matroid structure]\label{prop:non-matroid}
The feasible family $\mathcal{F}$ violates both the hereditary axiom
and the exchange axiom.  Consequently, $\mathcal{F}$ is not a matroid,
not a greedoid, and not a polymatroid.
\end{proposition}

\begin{proof}
We prove each violation separately with explicit constructions.

\bigskip
\noindent\textbf{Part~1: Violation of the hereditary axiom.}

\smallskip
Fix $k = 3$.  Define the following six events, where the focal actor
is~$a^*$:
\begin{center}
\begin{tabular}{@{}ccccc@{}}
  \toprule
  Event & Actor & Weight & Timestamp & Intended role \\
  \midrule
  $e_s$    & other & 1 & 1 & Setup \\
  $e_{d1}$ & other & 2 & 2 & Development \\
  $e_{d2}$ & other & 2 & 3 & Development \\
  $e_{d3}$ & other & 2 & 4 & Development \\
  $e_{tp}$ & $a^*$ & 9 & 5 & Turning point \\
  $e_r$    & other & 3 & 6 & Resolution \\
  \bottomrule
\end{tabular}
\end{center}

Consider the full set
\[
  S = \{e_s,\, e_{d1},\, e_{d2},\, e_{d3},\, e_{tp},\, e_r\}.
\]
The event $e_{tp}$ is the sole focal event in $S$, so
$\tp(S) = e_{tp}$.  The phase-labelled sequence (in temporal order) is
\[
  \underbrace{\textsc{S}}_{e_s},\;\;
  \underbrace{\textsc{D}}_{e_{d1}},\;\;
  \underbrace{\textsc{D}}_{e_{d2}},\;\;
  \underbrace{\textsc{D}}_{e_{d3}},\;\;
  \underbrace{\textsc{TP}}_{e_{tp}},\;\;
  \underbrace{\textsc{R}}_{e_r}.
\]
This has 1~setup, 3~development events (meeting $k = 3$), 1~turning
point, and 1~resolution.  The DFA accepts.  Therefore
$S \in \mathcal{F}$.

Now remove $e_{d1}$ to form $S' = S \setminus \{e_{d1}\}$.  The
turning point is still $\tp(S') = e_{tp}$ (still the unique focal
event).  The phase-labelled sequence becomes
\[
  \underbrace{\textsc{S}}_{e_s},\;\;
  \underbrace{\textsc{D}}_{e_{d2}},\;\;
  \underbrace{\textsc{D}}_{e_{d3}},\;\;
  \underbrace{\textsc{TP}}_{e_{tp}},\;\;
  \underbrace{\textsc{R}}_{e_r}.
\]
Now there are only 2~development events before the turning point.  The
grammar-aware classifier cannot improve on this: regardless of whether
$e_s$ is labelled \textsc{Setup} or \textsc{Development}, the maximum
number of development events before $e_{tp}$ is~$3$ (if $e_s$ is
relabelled as \textsc{Development})---but wait: if we relabel $e_s$ as
\textsc{Development}, we obtain
$\textsc{D},\, \textsc{D},\, \textsc{D},\, \textsc{TP},\, \textsc{R}$,
which has exactly 3~development events and satisfies $k = 3$.

To close this escape route, we remove \emph{two} development events.
Let $S'' = S \setminus \{e_{d1}, e_{d2}\}$:
\[
  S'' = \{e_s,\, e_{d3},\, e_{tp},\, e_r\}.
\]
$\tp(S'') = e_{tp}$.  Before $e_{tp}$: events $e_s$ and $e_{d3}$.
Even relabelling both as \textsc{Development}, we get only
2~development events.  Since $k = 3$, the DFA rejects.
$S'' \notin \mathcal{F}$.

We have $S \in \mathcal{F}$ and $S'' \subset S$ with
$S'' \notin \mathcal{F}$.  The hereditary axiom is violated.

\bigskip
\noindent\textbf{Part~2: Violation of the exchange axiom.}

\smallskip
We use a grammar with both a minimum and maximum prefix requirement:
at least $k_{\min} = 3$ and at most $k_{\max} = 4$ development events
must appear before the turning point.  This bounded-prefix grammar is a
natural specialisation of the phase grammar (it adds an upper bound on
the development phase) and is used in practice to prevent excessively
long setup portions of a narrative arc.

Define the following events:
\begin{center}
\begin{tabular}{@{}cccc@{}}
  \toprule
  Event & Actor & Weight & Timestamp \\
  \midrule
  $c_1$  & other & 1 & 1 \\
  $c_2$  & other & 1 & 2 \\
  $c_3$  & other & 1 & 3 \\
  $q$    & $a^*$ & 9 & 3.5 \\
  $c_4$  & other & 1 & 4 \\
  $c_5$  & other & 1 & 5 \\
  $p$    & $a^*$ & 7 & 6 \\
  $r$    & other & 1 & 7 \\
  \bottomrule
\end{tabular}
\end{center}

\textit{Feasible set $S_1$.}\;
Let $S_1 = \{c_1,\, c_3,\, c_4,\, c_5,\, p,\, r\}$, with
$|S_1| = 6$.  The only focal event is $p$ (weight~7), so
$\tp(S_1) = p$ (timestamp~6).  Events before $p$ in temporal order:
$c_1(1),\, c_3(3),\, c_4(4),\, c_5(5)$---all non-focal, all labelled
\textsc{Development}.  That gives 4~development events:
$k_{\min} = 3 \le 4 \le k_{\max} = 4$.  After $p$: $r$ labelled
\textsc{Resolution}.  Phase sequence:
$\textsc{D},\, \textsc{D},\, \textsc{D},\, \textsc{D},\, \textsc{TP},\,
\textsc{R}$.
Accepted.  $S_1 \in \mathcal{F}$.

\smallskip
\textit{Feasible set $S_2$.}\;
Let $S_2 = \{c_1,\, c_2,\, c_3,\, q,\, c_4,\, c_5,\, p,\, r\}$, with
$|S_2| = 8$.  Focal events: $q$ (weight~9) and $p$ (weight~7).
$\tp(S_2) = q$ (timestamp~$3.5$).  Events before $q$:
$c_1(1),\, c_2(2),\, c_3(3)$---3~development events.
$k_{\min} = 3 \le 3 \le k_{\max} = 4$.  After $q$:
$c_4,\, c_5,\, p,\, r$ all labelled \textsc{Resolution}.  Phase
sequence:
$\textsc{D},\, \textsc{D},\, \textsc{D},\, \textsc{TP},\,
\textsc{R},\, \textsc{R},\, \textsc{R},\, \textsc{R}$.
Accepted.  $S_2 \in \mathcal{F}$.

\smallskip
\textit{Exchange check.}\;
We have $|S_1| = 6 < 8 = |S_2|$ and
$S_2 \setminus S_1 = \{c_2,\, q\}$.  The exchange axiom requires the
existence of some $v \in \{c_2, q\}$ with
$S_1 \cup \{v\} \in \mathcal{F}$.  We show that neither works.

\begin{enumerate}[label=(\roman*)]
  \item \textbf{Adding $q$} (focal, weight~9, timestamp~$3.5$) to
    $S_1$.  The augmented set is
    $S_1 \cup \{q\} = \{c_1, c_3, q, c_4, c_5, p, r\}$.
    Now $\tp(S_1 \cup \{q\}) = q$ because $w(q) = 9 > 7 = w(p)$---the
    pivot shifts from $p$ to $q$.  Events before $q$ (timestamp~$3.5$):
    $c_1(1)$ and $c_3(3)$ ---only 2~non-focal events.  Even labelling
    both as \textsc{Development}, we have $2 < k_{\min} = 3$.  The
    prefix requirement is \textbf{not met}.  The DFA rejects.
    $S_1 \cup \{q\} \notin \mathcal{F}$.

  \item \textbf{Adding $c_2$} (non-focal, timestamp~2) to $S_1$.
    The pivot remains $\tp(S_1 \cup \{c_2\}) = p$ (no new focal events).
    Events before $p$: $c_1(1),\, c_2(2),\, c_3(3),\, c_4(4),\, c_5(5)$
    ---5~development events.  But $k_{\max} = 4$, and $5 > 4$.  The
    grammar \textbf{rejects}: too many development events before the
    turning point.
    $S_1 \cup \{c_2\} \notin \mathcal{F}$.
\end{enumerate}

Neither element of $S_2 \setminus S_1$ can be added to $S_1$ to yield
a feasible set.  The exchange axiom is violated.
\end{proof}

\begin{remark}[Consequences of non-matroid structure]
\label{rem:non-matroid-consequences}
\cref{prop:non-matroid} has three immediate consequences:

\begin{enumerate}[label=(\arabic*)]
  \item \textbf{No greedy guarantee.}\;
    The classical result that the greedy algorithm is optimal for
    matroids (and, more generally, for greedoids) does not apply.  The
    greedy policy of \cref{def:greedy-policy} may produce arbitrarily
    suboptimal solutions.

  \item \textbf{No exchange property.}\;
    The exchange axiom failure means there is no guarantee that a small
    feasible set can be augmented element-by-element to a larger
    feasible set.  Local augmentation strategies---which underlie many
    combinatorial optimisation algorithms---cannot be relied upon.

  \item \textbf{No polymatroid structure.}\;
    Since the hereditary axiom fails, the feasible family is not even a
    simplicial complex, let alone a matroid or polymatroid.  The
    extraction problem falls outside the scope of submodular
    optimisation theory.
\end{enumerate}

These failures motivate the algorithmic hierarchy developed in
\cref{ch:taxonomy}: since no generic optimisation framework applies,
we must develop problem-specific methods tailored to the endogenous
coupling structure.
\end{remark}
