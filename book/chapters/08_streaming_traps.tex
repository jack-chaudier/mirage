% ══════════════════════════════════════════════════════════════
%  Chapter 8 — Streaming Oscillation Traps
% ══════════════════════════════════════════════════════════════
\chapter{Streaming Oscillation Traps}\label{ch:streaming}

The preceding chapters analysed validity and absorption in an
\emph{offline} setting: the full event sequence is available before any
labelling decision is made.  In practice, however, events often arrive
one at a time, and the system must emit labels incrementally.  This
chapter studies what goes wrong when the endogenous pivot problem is
solved under streaming constraints.

The core phenomenon is an \emph{oscillation trap}: the streaming
policy commits to a pivot, then encounters a stronger focal event that
invalidates the commitment.  Because earlier labels are irrevocable, the
policy is left in a state from which no continuation can produce a
grammar-valid output.  We prove that such traps are inevitable---the
minimum inter-record gap in the pivot process is $O(1)$---and that they
affect more than half of organically generated sequences.  We then
develop a \emph{deferred commitment} policy that trades a small amount
of latency for a dramatic reduction in trap rate, and characterise the
quality--latency Pareto frontier.

We proceed as follows.  \Cref{sec:streaming-defs} formalises the
streaming extraction model and two concrete policies.
\Cref{sec:record-process} analyses the running-max pivot as a record
process.  \Cref{sec:adversarial-traps} constructs adversarial sequences
that trap the commit-now policy and proves the oscillation trap
threshold.  \Cref{sec:organic-prevalence} reports the organic trap rate
(Experiment~43).  \Cref{sec:mechanism-verification} validates the
$\mingap < k$ predictor (Experiment~44).  \Cref{sec:scale-behaviour}
demonstrates scale invariance (Experiment~45).
\Cref{sec:record-gap-scaling} proves that the minimum inter-record gap
is $O(1)$ under i.i.d.\ weights.  \Cref{sec:deferred-commitment}
develops the deferred commitment policy and its Pareto analysis.
\Cref{sec:streaming-exercises} offers exercises.


% ══════════════════════════════════════════════════════════════
\section{Streaming Model Definitions}\label{sec:streaming-defs}
% ══════════════════════════════════════════════════════════════

We begin with the formal streaming extraction model.  The key
distinction from the offline setting of \cref{ch:absorbing-states} is
\emph{irrevocability}: once a label has been emitted, it cannot be
changed.

\begin{definition}[Streaming extraction policy]
\label{def:streaming-policy}
A \emph{streaming extraction policy} is a function that processes an
event sequence $e_1, e_2, \ldots, e_n$ in temporal order, maintaining a
running state $S_t$ at each step $t$.  At each step the policy may
assign an irrevocable label $\ell(e_t) \in
\{\textsc{development},\, \textsc{turning\_point},\,
\textsc{resolution},\, \textsc{pending}\}$ to the current event.
Labels assigned in previous steps cannot be modified:
\[
  \text{if } \ell(e_s) \neq \textsc{pending} \text{ at step } t,
  \quad
  \text{then } \ell(e_s) \text{ is fixed for all } t' > t.
\]
The policy's output is the final label assignment
$(\ell(e_1), \ldots, \ell(e_n))$.
\end{definition}

\begin{definition}[Commit-now policy]
\label{def:commit-now}
The \emph{commit-now policy} is a streaming extraction policy that
operates as follows.  Let $a^*$ denote the focal actor and let
$\tp_t = \argmax_{s \le t,\, a(e_s) = a^*} w(e_s)$ be the running-max
pivot at step~$t$.  At each step~$t$:
\begin{enumerate}[label=(\roman*)]
  \item If $e_t$ is a focal event with $w(e_t) > w(\tp_{t-1})$ (or
        $t = 1$ and $e_t$ is the first focal event), then $e_t$ becomes
        the new running-max pivot: $\tp_t = e_t$.  The policy
        immediately commits: it assigns $\ell(e_t) =
        \textsc{turning\_point}$ and labels all pending non-focal events
        relative to $e_t$ as the new pivot.
  \item Labels assigned in previous steps are \textbf{irrevocable}.  If
        $\tp_{t-1}$ was previously labelled $\textsc{turning\_point}$
        and a new pivot $\tp_t \neq \tp_{t-1}$ is chosen, the old label
        cannot be retracted.
\end{enumerate}
\end{definition}

\begin{definition}[Buffered policy with patience $f$]
\label{def:buffered-policy}
The \emph{buffered policy with patience $f \in (0,1)$} buffers events
until step $\lceil f \cdot n \rceil$, then commits to the running-max
pivot at that point.  Formally:
\begin{enumerate}[label=(\roman*)]
  \item For $t \le \lceil f \cdot n \rceil$, all labels remain
        $\textsc{pending}$.  The policy tracks the running-max pivot
        $\tp_t$ but does not emit any irrevocable labels.
  \item At step $t = \lceil f \cdot n \rceil$, the policy commits to
        $\tp_t$ as the final pivot.  All buffered events receive their
        labels relative to $\tp_t$.  Events arriving after step
        $\lceil f \cdot n \rceil$ receive committed labels immediately.
\end{enumerate}
Events before the buffer point receive tentative labels that are
finalised at commitment time; events after receive committed labels
on arrival.
\end{definition}

\begin{definition}[Streaming absorbing trap]
\label{def:streaming-trap}
A \emph{streaming absorbing trap} is a state reached during streaming
in which no continuation of the event sequence can produce a
grammar-valid output under the commit-now policy.  Formally, the
policy is trapped at step~$t$ if:
\begin{enumerate}[label=(\roman*)]
  \item The running-max pivot has shifted from $\tp_{t-1}$ to
        $\tp_t \neq \tp_{t-1}$ (a \emph{pivot shift}).
  \item The number of non-focal events between $\tp_{t-1}$ and $\tp_t$
        in the sequence is strictly less than $k$ (the grammar's prefix
        requirement).
  \item The labels assigned to events before $\tp_{t-1}$ under the old
        pivot assignment are irrevocable and cannot be reassigned as
        $\textsc{development}$ for $\tp_t$.
\end{enumerate}
The trap is ``sprung'' when a pivot shift leaves too few development
events between the old and new pivot to satisfy the prefix requirement.
\end{definition}


% ══════════════════════════════════════════════════════════════
\section{Running-Max Pivot as Record Process}
\label{sec:record-process}
% ══════════════════════════════════════════════════════════════

The running-max pivot $\tp_t$ updates only when a new maximum is
encountered among focal events.  This is precisely the definition of a
\emph{record} in the theory of order statistics.

The running-max pivot at step~$t$ is
\[
  \tp_t \;=\; \argmax_{s \le t,\; a(e_s) = a^*} w(e_s).
\]
The pivot updates at step~$t$ if and only if $w(e_t) > w(\tp_{t-1})$
and $a(e_t) = a^*$---that is, if $e_t$ is a \emph{record} among the
focal events seen so far.  Each such update is a \emph{pivot shift}.

Under i.i.d.\ continuous weights, the theory of records gives sharp
predictions about the frequency and timing of pivot shifts.

\begin{proposition}[Expected number of records]
\label{prop:expected-records}
If the weights of $n$ focal events are drawn i.i.d.\ from a continuous
distribution, the expected number of records (pivot shifts) is the
$n$-th harmonic number:
\[
  \mathbb{E}[\text{number of records}] \;=\; H_n
  \;=\; \sum_{i=1}^{n} \frac{1}{i}
  \;\approx\; \ln n + \gamma,
\]
where $\gamma \approx 0.5772$ is the Euler--Mascheroni constant.
\end{proposition}

\begin{proof}
The $i$-th observation is a record if and only if it is the maximum of
the first $i$ observations.  For i.i.d.\ continuous random variables,
each of the $i$ observations is equally likely to be the maximum, so
$\Pr(\text{$i$-th is a record}) = 1/i$.  By linearity of expectation,
the expected number of records in $n$ observations is
$\sum_{i=1}^{n} 1/i = H_n$.
\end{proof}

In practice, the event weights are not i.i.d.: the bursty generators
used in our experiments impose a \emph{front-loading} structure
controlled by the parameter $\varepsilon$.  Front-loading concentrates
higher weights among earlier focal events, which affects both the
number of pivot shifts and their timing.

\begin{table}[ht]
\centering
\caption{Pivot stability profile.  Mean number of pivot shifts and
         median position of the last shift (as a fraction of the
         timeline), across front-loading parameter~$\varepsilon$.
         Higher front-loading reduces the number of shifts and causes
         earlier stabilisation.}
\label{tab:pivot-stability}
\begin{tabular}{@{}ccc@{}}
\toprule
\textbf{Front-loading $\varepsilon$}
  & \textbf{Mean shifts}
  & \textbf{Median last-shift position} \\
\midrule
$0.05$ & $4.64$ & $0.481$ \\
$0.10$ & $4.16$ & $0.436$ \\
$0.20$ & $3.72$ & $0.382$ \\
$0.40$ & $3.31$ & $0.312$ \\
$0.60$ & $3.09$ & $0.265$ \\
$0.80$ & $2.95$ & $0.224$ \\
\bottomrule
\end{tabular}
\end{table}

\Cref{tab:pivot-stability} summarises the pivot stability profile.  At
low front-loading ($\varepsilon = 0.05$), the pivot shifts an average
of $4.64$ times, with the last shift occurring at a median position of
$0.481$ along the timeline---nearly halfway through.  At high
front-loading ($\varepsilon = 0.80$), the mean number of shifts drops
to $2.95$ and the pivot stabilises by roughly the first quarter.

Each pivot shift is a record in the weight sequence.  The \emph{cost} of
a shift is proportional to the number of events labelled since the last
shift---the ``blast radius.''  Events labelled under the previous pivot
assignment have irrevocable labels that may now conflict with the
grammar's requirements under the new pivot.  The larger the blast
radius, the more damage a pivot shift inflicts on the streaming policy's
output.


% ══════════════════════════════════════════════════════════════
\section{Adversarial Oscillation Traps}\label{sec:adversarial-traps}
% ══════════════════════════════════════════════════════════════

We now construct sequences that deterministically trap the commit-now
policy.  The construction makes the trapping mechanism explicit:
record-setting focal events with strictly increasing weights are
interleaved with non-focal events, and the oscillation period controls
the spacing.

\begin{definition}[Adversarial oscillation generator]
\label{def:adversarial-generator}
The \emph{adversarial oscillation generator} with parameters
$(n, p, a^*)$ produces a sequence of $n$ events as follows.  Let $p$
denote the \emph{oscillation period}: the number of events between
consecutive focal spikes.  The generator places focal events at
positions $1, p+1, 2p+1, \ldots$ with strictly increasing weights
$w_1 < w_2 < w_3 < \cdots$.  All other positions are filled with
non-focal events.  The result is a sequence where every focal event is
a record (a new running-max), and there are exactly $p - 1$ non-focal
events between consecutive focal events.
\end{definition}

The key quantity is the \emph{effective pre-pivot capacity}: the
maximum number of non-focal events available between consecutive
record-setting focal events to serve as \textsc{development} for the
new pivot.

\begin{theorem}[Oscillation trap threshold]
\label{thm:oscillation-trap}
Consider a sequence produced by the adversarial oscillation generator
(\cref{def:adversarial-generator}) with oscillation period~$p$.  Under
the commit-now policy (\cref{def:commit-now}) with grammar prefix
requirement~$k$, the policy is trapped if and only if
\[
  \peff \;<\; k,
\]
where $\peff$ is the effective pre-pivot capacity: the maximum number
of non-focal events between consecutive record-setting focal events
that are available for \textsc{development} assignment.  For adversarial
alternating-spike traces,
\[
  \peff \;=\; \max\!\bigl(1,\; \lfloor p/2 \rfloor\bigr).
\]
\end{theorem}

\begin{proof}
We prove both directions.

\medskip
\noindent$(\Rightarrow)$\; Suppose $\peff < k$.  Each time the
running-max pivot shifts to a new focal event $e_{\mathrm{new}}$, the
commit-now policy fixes $e_{\mathrm{new}}$ as $\textsc{turning\_point}$.
Between the previous pivot $e_{\mathrm{old}}$ and $e_{\mathrm{new}}$,
there are at most $\peff$ non-focal events that could serve as
$\textsc{development}$.

Since $\peff < k$, the prefix requirement is not met for
$e_{\mathrm{new}}$: the grammar demands at least $k$
$\textsc{development}$ events before the $\textsc{turning\_point}$, but
only $\peff < k$ are available in the interval between the two pivots.

Moreover, labels assigned to events before $e_{\mathrm{old}}$ under the
old pivot assignment are irrevocable.  These events were labelled
relative to $e_{\mathrm{old}}$---some as $\textsc{development}$ for
$e_{\mathrm{old}}$, some as $\textsc{resolution}$---and these labels
cannot be changed to $\textsc{development}$ for $e_{\mathrm{new}}$.
Therefore the commit-now policy is trapped: the grammar requires $k$
$\textsc{development}$ events before the $\textsc{turning\_point}$, but
only $\peff < k$ are available, and no continuation of the sequence can
supply additional pre-pivot events because the $\textsc{turning\_point}$
is already fixed at $e_{\mathrm{new}}$.

\medskip
\noindent$(\Leftarrow)$\; Suppose $\peff \ge k$.  Consider the final
record---the last pivot shift in the sequence.  Let $e_{\mathrm{final}}$
be the final pivot and $e_{\mathrm{prev}}$ the penultimate pivot.  By
assumption, there are at least $\peff \ge k$ non-focal events between
$e_{\mathrm{prev}}$ and $e_{\mathrm{final}}$.

The commit-now policy can assign $k$ of these non-focal events as
$\textsc{development}$, satisfying the prefix requirement for
$e_{\mathrm{final}}$.  Events after $e_{\mathrm{final}}$ can be labelled
$\textsc{resolution}$.  Since $e_{\mathrm{final}}$ is the last record,
no further pivot shift will occur, so these labels remain valid.  The
grammar is satisfied.
\end{proof}

\Cref{tab:adversarial-boundary} reproduces the adversarial boundary
for the commit-now policy across oscillation periods and prefix
requirements.  The boundary is sharp: validity transitions from $0\%$
to $100\%$ at the exact threshold predicted by
\cref{thm:oscillation-trap}.

\begin{table}[ht]
\centering
\caption{Commit-now validity (\%) versus oscillation period for
         prefix requirements $k = 1, 2, 3$.  The $0\% \to 100\%$
         boundary occurs at $\peff = k$, exactly as predicted by
         \cref{thm:oscillation-trap}.}
\label{tab:adversarial-boundary}
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{Oscillation period $p$}
  & \textbf{$k = 1$}
  & \textbf{$k = 2$}
  & \textbf{$k = 3$} \\
\midrule
$1$ & $0$   & $0$   & $0$   \\
$2$ & $100$ & $0$   & $0$   \\
$3$ & $100$ & $0$   & $0$   \\
$4$ & $100$ & $100$ & $0$   \\
$5$ & $100$ & $100$ & $0$   \\
$6$ & $100$ & $100$ & $100$ \\
$7$ & $100$ & $100$ & $100$ \\
$8$ & $100$ & $100$ & $100$ \\
\bottomrule
\end{tabular}
\end{table}


% ══════════════════════════════════════════════════════════════
\section{Organic Prevalence (Experiment~43)}
\label{sec:organic-prevalence}
% ══════════════════════════════════════════════════════════════

The adversarial construction of \cref{sec:adversarial-traps}
demonstrates that traps are possible, but one might hope that they are
pathological---artefacts of worst-case engineering that rarely occur in
practice.  Experiment~43 tests this hope against organically generated
sequences.

We generated 4{,}200 sequences using the bursty event generator across
a grid of front-loading parameters $\varepsilon \in
\{0.05, 0.10, 0.20, 0.40, 0.60, 0.80\}$ and prefix requirements
$k \in \{1, 2, 3\}$, with 200 sequences per configuration.  Each
sequence was processed by the commit-now policy, and the output was
checked for grammar validity.

\begin{table}[ht]
\centering
\caption{Commit-now trap rates on organic (bursty-generated) sequences.
         Trap rate is the fraction of sequences where the commit-now
         policy produces a grammar-invalid output, despite the existence
         of a valid offline solution.}
\label{tab:organic-traps}
\begin{tabular}{@{}cccc@{}}
\toprule
& \textbf{$k = 1$} & \textbf{$k = 2$} & \textbf{$k = 3$} \\
\midrule
\textbf{Overall trap rate} & $38.9\%$ & $58.0\%$ & $67.9\%$ \\
\midrule
\textbf{Peak trap rate} & \multicolumn{3}{c}{$77\%$ at $\varepsilon = 0.40$, $k = 3$} \\
\midrule
\textbf{Offline validity} & \multicolumn{3}{c}{$83.5\%$--$97.5\%$} \\
\bottomrule
\end{tabular}
\end{table}

The headline result: \textbf{2{,}306 out of 4{,}200 organic sequences
exhibit commit-now traps}---a trap rate of $54.9\%$.
\Cref{tab:organic-traps} breaks down the trap rate by prefix
requirement~$k$.  Higher $k$ means greater vulnerability: the grammar
demands more pre-pivot development events, so a smaller inter-record gap
is sufficient to spring the trap.

The peak trap rate of $77\%$ occurs at $\varepsilon = 0.40$, $k = 3$.
This is not a coincidence: moderate front-loading produces enough pivot
shifts to create opportunities for trapping while concentrating records
early where inter-record gaps tend to be small.

Meanwhile, finite (offline) validity remains in the $83.5\%$--$97.5\%$
range across the same configurations.  The gap between offline
feasibility and streaming feasibility is precisely the trap rate: these
are sequences that \emph{have} valid solutions, but the commit-now
policy cannot find them because irrevocable early commitments block the
path.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{%
  figures/test_16_trap_rate_heatmaps.png}
\caption{Organic trap rates by $(\varepsilon, k)$.  Commit-now traps
         affect more than half of organic sequences.  The trap rate
         increases with $k$ (rows) and peaks at moderate front-loading
         (columns).  The gap between offline validity (high) and
         streaming validity (low) is the region where irrevocable
         commitment destroys feasibility.}
\label{fig:trap-rate-heatmaps}
\end{figure}

\Cref{fig:trap-rate-heatmaps} presents the full heatmap.


% ══════════════════════════════════════════════════════════════
\section{Mechanism Verification (Experiment~44)}
\label{sec:mechanism-verification}
% ══════════════════════════════════════════════════════════════

The oscillation trap threshold (\cref{thm:oscillation-trap}) identifies
$\peff < k$ as the trapping condition for adversarial sequences.  For
organic sequences, the analogous predictor is the \emph{minimum
inter-record gap}: for each sequence, compute the minimum number of
non-focal events between consecutive pivot shifts, and predict a trap
whenever $\mingap < k$.

\begin{definition}[Minimum inter-record gap]
\label{def:min-gap}
Given a sequence with records (pivot shifts) at positions
$r_1, r_2, \ldots, r_m$, the \emph{inter-record gap} before the $i$-th
record is the number of non-focal events between $r_{i-1}$ and $r_i$.
The \emph{minimum inter-record gap} is
\[
  \mingap \;=\; \min_{2 \le i \le m} (\text{non-focal events between }
  r_{i-1} \text{ and } r_i).
\]
\end{definition}

Experiment~44 evaluates the $\mingap < k$ predictor as a binary
classifier for commit-now traps on the same 4{,}200 organic sequences
from Experiment~43.

\begin{table}[ht]
\centering
\caption{Confusion matrix for the $\mingap < k$ trap predictor
         (Experiment~44).  The predictor achieves $100\%$ recall: every
         trapped sequence has $\mingap < k$.}
\label{tab:confusion-matrix}
\begin{tabular}{@{}lcc@{}}
\toprule
& \textbf{Predicted: Trap} & \textbf{Predicted: No Trap} \\
\midrule
\textbf{Actual: Trap}    & $1{,}040$ (TP) & $0$ (FN)   \\
\textbf{Actual: No Trap} & $124$ (FP)     & $490$ (TN) \\
\bottomrule
\end{tabular}
\end{table}

\Cref{tab:confusion-matrix} presents the confusion matrix.  The key
results:
\begin{itemize}[itemsep=3pt]
  \item \textbf{Accuracy:} $92.5\%$ \;
        $\bigl((1{,}040 + 490) / (1{,}040 + 124 + 0 + 490)\bigr)$.
  \item \textbf{Recall:} $100\%$ --- zero false negatives.  Every
        sequence that is actually trapped has $\mingap < k$.
  \item \textbf{Precision:} $89.3\%$ \;
        $\bigl(1{,}040 / (1{,}040 + 124)\bigr)$.
\end{itemize}

The zero false negatives confirm that $\mingap < k$ is a
\textbf{necessary condition} for trapping: if the minimum gap is at
least $k$, the commit-now policy is guaranteed to succeed.  The $124$
false positives occur because having a small gap does not guarantee that
the trap is sprung---the sequence may recover if later events provide
sufficient development material after the final pivot shift.

\begin{remark}[Necessary versus sufficient]
\label{rem:necessary-not-sufficient}
The $\mingap < k$ condition is necessary but not sufficient for
trapping.  Sufficiency fails because a small gap at an intermediate
record does not necessarily propagate to the final pivot assignment.  If
the \emph{last} record has a gap $\ge k$, the final pivot has enough
pre-pivot development events regardless of earlier gaps.  The false
positives are sequences where an intermediate record had $\mingap < k$
but the final pivot shift had a sufficient gap.
\end{remark}


% ══════════════════════════════════════════════════════════════
\section{Scale Behaviour (Experiment~45)}
\label{sec:scale-behaviour}
% ══════════════════════════════════════════════════════════════

One might conjecture that streaming traps are a small-sample artefact:
perhaps as $n$ grows, the increasing spacing between records gives the
commit-now policy room to breathe.  Experiment~45 tests this
conjecture by sweeping $n$ from $100$ to $1{,}000$.

The result is decisive: \textbf{trap rates remain in the $52\%$--$78\%$
band from $n = 100$ to $n = 1{,}000$}.  There is no decay with scale.
The mean minimum gap stays near $1.0$ regardless of $n$.

\begin{remark}[No escape through scale]
\label{rem:no-scale-escape}
The persistence of traps across scales is a direct consequence of the
$O(1)$ minimum inter-record gap established in
\cref{prop:record-gap-scaling}.  Although the number of records grows as
$\ln n$ and the \emph{average} gap between records grows as $n / \ln n$,
the \emph{minimum} gap does not grow.  The trap mechanism---which
depends on the minimum gap, not the average---remains active at every
practical scale.
\end{remark}


% ══════════════════════════════════════════════════════════════
\section{Record-Gap Scaling (Proposition~2)}
\label{sec:record-gap-scaling}
% ══════════════════════════════════════════════════════════════

We now establish the theoretical foundation for the scale-invariance of
streaming traps.  The key result is that the minimum inter-record gap is
$O(1)$ under i.i.d.\ continuous weights---it does not grow with $n$.

\begin{proposition}[Minimum inter-record gap is $O(1)$]
\label{prop:record-gap-scaling}
Let $X_1, X_2, \ldots, X_n$ be i.i.d.\ draws from a continuous
distribution (no ties almost surely).  Let $G_1, G_2, \ldots$ denote
the inter-record gaps: $G_i$ is the number of observations between the
$(i{-}1)$-th and $i$-th records.  Then:
\begin{enumerate}[label=(\roman*)]
  \item For the first gap: $\Pr(G_1 = 1) = 1/2$.
  \item The minimum gap satisfies
        $\min_i G_i = O(1)$ almost surely.  In particular,
        $\Pr(\min_i G_i = 1) \to 1$ as $n \to \infty$.
\end{enumerate}
\end{proposition}

\begin{proof}
We proceed in two parts.

\medskip
\noindent\textbf{Part (i): The first gap.}\quad
The first record is always $X_1$ (the first observation is trivially a
record).  The second record occurs at index $j$ where $X_j > X_1$.  The
first inter-record gap is $G_1 = j - 1$.

The gap equals $1$ if and only if $X_2 > X_1$---that is, the second
observation is itself a new record.  By exchangeability of continuous
i.i.d.\ random variables, $\Pr(X_2 > X_1) = 1/2$.  Therefore
$\Pr(G_1 = 1) = 1/2$.

\medskip
\noindent\textbf{Part (ii): The minimum gap.}\quad
Let $R_1 < R_2 < \cdots < R_M$ denote the positions of the $M$ records
among $X_1, \ldots, X_n$.  We have $R_1 = 1$ always, and $M \approx
\ln n$ in expectation.  The gaps are $G_i = R_{i+1} - R_i$ for
$i = 1, \ldots, M - 1$.

We need to show that $\Pr(\min_{1 \le i \le M-1} G_i \ge g) \to 0$ for
any fixed $g \ge 2$ as $n \to \infty$, which implies
$\Pr(\min_i G_i = 1) \to 1$.

\emph{Key insight.}\; The gaps $G_1, G_2, \ldots$ are not independent,
but each has a positive probability of equalling~$1$.  Specifically, if
the $i$-th record occurs at position $R_i = j$, then the next
observation $X_{j+1}$ is a new record (giving $G_i = 1$) if and only if
$X_{j+1}$ is the maximum of $X_1, \ldots, X_{j+1}$.  By
exchangeability,
\[
  \Pr(G_i = 1 \mid R_i = j) \;=\; \frac{1}{j + 1}.
\]
For the early records (small $j$), this probability is large: $1/2$ for
$j = 1$, $1/3$ for $j = 2$, and so on.

Since there are $M \approx \ln n$ records, the number of independent
``chances'' for a gap of $1$ grows without bound.  Even though the
individual probabilities $1/(j+1)$ decrease, the sum
$\sum_{i} 1/(R_i + 1)$ diverges (it is bounded below by a constant
fraction of $\sum_{j=1}^{n} 1/j^2$, and the first few terms alone give
a constant bounded away from zero).  By a standard Borel--Cantelli
argument, infinitely many gaps equal~$1$ almost surely, and therefore
$\min_i G_i = 1$ almost surely as $n \to \infty$.

More concretely, the probability that \emph{none} of the first $\ell$
gaps equals~$1$ is at most
\[
  \prod_{i=1}^{\ell} \Bigl(1 - \frac{1}{R_i + 1}\Bigr)
  \;\le\;
  \prod_{i=1}^{\ell} \Bigl(1 - \frac{1}{n}\Bigr)
  \;\le\;
  \exp\!\Bigl(-\frac{\ell}{n}\Bigr),
\]
but in fact the early records have $R_i$ much smaller than $n$, so the
bound is tighter.  For the first gap alone, $\Pr(G_1 \neq 1) = 1/2$,
so with just two independent opportunities, $\Pr(\text{both}
\neq 1) \le 1/2 \cdot 2/3 = 1/3$.  The probability decays rapidly and
converges to zero.
\end{proof}

\begin{remark}[Why traps persist across scales]
\label{rem:traps-persist}
\Cref{prop:record-gap-scaling} explains the empirical finding of
\cref{sec:scale-behaviour}: even though the number of records grows as
$\ln n$, the gaps between them include arbitrarily small values (down
to $1$), and the minimum gap does not grow with $n$.  The trap mechanism
$\mingap < k$ remains active because the minimum gap is $O(1)$, not
$O(n / \ln n)$.  Increasing $n$ adds more records with more
opportunities for small gaps, rather than spacing existing records
further apart.
\end{remark}

\begin{remark}[I.i.d.\ versus bursty weights]
\label{rem:iid-vs-bursty}
The proof of \cref{prop:record-gap-scaling} uses i.i.d.\ continuous
weights as a reference model.  The bursty generators used empirically
are \emph{not} i.i.d.: they impose a front-loading structure in which
higher weights are concentrated among earlier events (controlled by the
parameter~$\varepsilon$).  The empirical trap rates are \emph{higher}
than the i.i.d.\ prediction because front-loading concentrates records
early in the sequence, precisely where the inter-record gaps are most
likely to be small.

Specifically, front-loading increases $\Pr(G_1 = 1)$ beyond the i.i.d.\
value of $1/2$, because early focal events are more likely to have high
weights, making consecutive records in the first few positions more
probable.  The i.i.d.\ analysis therefore provides a \emph{lower bound}
on the trap rate for bursty sequences.
\end{remark}


% ══════════════════════════════════════════════════════════════
\section{Deferred Commitment}\label{sec:deferred-commitment}
% ══════════════════════════════════════════════════════════════

The commit-now policy's fundamental weakness is that it commits
irrevocably at each pivot shift, before the pivot has stabilised.
Deferring commitment---waiting until a fraction $f$ of the sequence
has been observed before locking in the pivot---trades latency for
validity.  This section characterises the quality--latency Pareto
frontier.

% ── Quality-latency Pareto curve ──────────────────────────────
\subsection{The Quality--Latency Pareto Curve}
\label{subsec:pareto-curve}

Experiment~48c evaluates the buffered policy
(\cref{def:buffered-policy}) across patience values $f \in
\{0, 0.10, 0.25, 0.50\}$ and compares against the finite (offline)
solver as a ceiling.  All evaluations use TP-consistent scoring
(see \cref{subsec:tp-consistent}).

\begin{table}[ht]
\centering
\caption{Quality--latency Pareto curve.  Effective validity, score, and
         regret for the commit-now policy, deferred policies at three
         patience levels, and the offline ceiling.  Deferred commitment
         at $f = 0.25$ recovers $80.1\%$ validity versus commit-now's
         $39.4\%$.}
\label{tab:pareto-curve}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Policy}
  & \textbf{Eff.\ Valid\%}
  & \textbf{Eff.\ Score}
  & \textbf{Eff.\ Regret} \\
\midrule
Commit-now ($f = 0$)       & $39.4$ & $6.68$  & $9.90$ \\
Deferred $f = 0.10$        & $62.6$ & $10.60$ & $5.99$ \\
Deferred $f = 0.25$        & $80.1$ & $14.17$ & $2.42$ \\
Deferred $f = 0.50$        & $88.2$ & $15.93$ & $0.66$ \\
Finite (offline)           & $91.5$ & $16.58$ & $0.00$ \\
\bottomrule
\end{tabular}
\end{table}

\Cref{tab:pareto-curve} presents the results.  The headline: deferred
commitment at $f = 0.25$ achieves $80.1\%$ validity and a score of
$14.17$, compared with commit-now's $39.4\%$ validity and $6.68$ score.
Deferred commitment dominates commit-now from $f = 0.05$ onward---every
deferred policy along the Pareto frontier achieves both higher validity
and higher score than the commit-now baseline.

At $f = 0.50$, the deferred policy reaches $88.2\%$ validity, within
$3.3$ percentage points of the offline ceiling.  The remaining gap
represents sequences where the global-max pivot arrives after the
halfway mark---a diminishing population as $f$ increases.

% ── TP-consistent scoring ─────────────────────────────────────
\subsection{TP-Consistent Scoring}\label{subsec:tp-consistent}

Comparing streaming and offline policies requires care.  The naive
approach---scoring the streaming output against the offline gold
standard---creates an $\argmax$ inconsistency.  The streaming policy
commits to a pivot $\tp_{\mathrm{stream}}$, while the offline solver
selects a (potentially different) pivot $\tp_{\mathrm{offline}}$.  If we
score the streaming output using $\tp_{\mathrm{offline}}$ as the
reference, we are penalising the streaming policy for a choice it was
forced to make under information constraints.

\emph{TP-consistent scoring} resolves this by evaluating each policy
against its own committed pivot.  When scoring a deferred policy's
output, we demote competing focal events---focal events other than the
policy's committed pivot---so that the evaluation pivot matches the
policy's pivot.  This ensures that differences in score reflect
structural quality (whether the grammar is satisfied, how many
development events precede the pivot) rather than pivot identity
disagreements.

Without TP-consistent scoring, a deferred policy that selects a
slightly weaker pivot but satisfies the grammar perfectly would receive
a lower score than an offline policy that selects the global-max pivot.
The score difference would reflect the weight gap between pivots, not
the structural quality of the output.  TP-consistent scoring removes
this confound.

% ── Why f = 0.25 ──────────────────────────────────────────────
\subsection{Why $f = 0.25$}\label{subsec:why-f-quarter}

The choice $f = 0.25$ is not arbitrary.  It is motivated by the
empirical distribution of the global-max pivot's arrival time.

\begin{remark}[Pivot arrival CDF]
\label{rem:pivot-arrival-cdf}
Let $F(t)$ denote the cumulative distribution function of the position
(as a fraction of $n$) at which the global-max focal event first
appears in the sequence.  Across our bursty generator configurations,
$F(0.25) \approx 0.50$: the global-max pivot arrives before position
$0.25$ in roughly half of all instances.

A deferred policy with $f = 0.25$ therefore captures the correct
pivot (the global max) in approximately $50\%$ of sequences at
commitment time, with no further pivot shifts possible after commitment.
For the remaining $50\%$, the policy commits to a suboptimal pivot, but
the grammar can often still be satisfied because the suboptimal pivot
typically has sufficient pre-pivot development.
\end{remark}

The $f = 0.25$ policy thus represents a favourable trade-off: it waits
only through the first quarter of the event stream, captures the
majority of pivot shifts, and recovers $80\%$ of offline validity.
Further increasing $f$ yields diminishing returns: from $f = 0.25$ to
$f = 0.50$, validity improves by only $8.1$ percentage points at the
cost of doubling the latency.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{%
  figures/test_48c_pareto_curve.png}
\caption{Quality--latency Pareto curve.  Each point represents a
         streaming policy with patience $f$, plotted by effective
         validity (horizontal) and effective score (vertical).  The
         commit-now policy ($f = 0$) occupies the lower-left corner;
         the offline ceiling ($f = 1$) the upper-right.  Deferred
         policies trace a concave Pareto frontier, with $f = 0.25$
         offering the best marginal return per unit of latency.}
\label{fig:pareto-curve}
\end{figure}

\Cref{fig:pareto-curve} plots the full Pareto frontier.


% ══════════════════════════════════════════════════════════════
\section{Exercises}\label{sec:streaming-exercises}
% ══════════════════════════════════════════════════════════════

\begin{exercise}[First inter-record gap]
\label{exer:first-gap}
Prove that for i.i.d.\ $\mathrm{Uniform}[0,1]$ weights, the
probability that the first inter-record gap equals~$1$ is exactly
$1/2$.

\emph{Hint.}\; The first record is $X_1$.  The gap equals $1$ iff
$X_2 > X_1$.  Use exchangeability: for i.i.d.\ continuous random
variables, $\Pr(X_2 > X_1) = 1/2$.  This is a direct consequence of
the proof of \cref{prop:record-gap-scaling}(i).
\end{exercise}

\begin{exercise}[Front-loading and pivot arrival]
\label{exer:frontloading-pivot}
Analyse the effect of the front-loading parameter $\varepsilon$ on the
median pivot arrival fraction.  Specifically:
\begin{enumerate}[label=(\alph*)]
  \item As $\varepsilon$ increases (more weight concentrated among
        early events), what happens to the median fraction of the
        timeline at which the global-max pivot first appears?
  \item Why does early stabilisation of the pivot \emph{not} eliminate
        streaming traps?
\end{enumerate}

\emph{Answer.}\; (a) The median pivot arrival fraction
\textbf{decreases}: the global max occurs earlier because early focal
events receive disproportionately high weights.  The pivot stabilises
sooner.  (b) Early stabilisation means that the gap between the first
and second records is likely to be very small---possibly $1$---because
the first few focal events are packed together with high weights.  The
pivot stabilises quickly, but the initial pivot shifts create small
gaps that spring the trap before stabilisation occurs.
\end{exercise}

\begin{exercise}[Step-by-step trap construction]
\label{exer:trap-construction}
Construct a sequence of $8$ events (with a focal actor $a^*$ and prefix
requirement $k = 2$) in which the commit-now policy is trapped but a
deferred policy with $f = 0.5$ succeeds.  Show the trap step by step:
\begin{enumerate}[label=(\alph*)]
  \item List the $8$ events with their types (focal/non-focal) and
        weights.
  \item Trace the commit-now policy: show the running-max pivot, the
        labels emitted, and the step at which the trap is sprung.
  \item Trace the deferred policy with $f = 0.5$ (buffer until
        step~$4$): show that the pivot stabilises during the buffer
        period and the grammar is satisfied.
\end{enumerate}

\emph{Hint.}\; Use two focal events at positions $1$ and $3$ with
weights $w_1 = 5$ and $w_3 = 10$, with a single non-focal event at
position $2$ between them.  This gives a gap of $1 < k = 2$.  Place
non-focal events at positions $4$--$8$ so the deferred policy has
sufficient development material.
\end{exercise}

\begin{exercise}[Why higher $k$ means more traps]
\label{exer:higher-k}
Explain why increasing $k$ makes streaming traps more likely.  Your
explanation should address:
\begin{enumerate}[label=(\alph*)]
  \item The relationship between $k$ and the trapping condition
        $\mingap < k$.
  \item The $O(1)$ minimum gap result
        (\cref{prop:record-gap-scaling}) and why it implies that the
        trapping condition is easier to satisfy for larger~$k$.
  \item The empirical trap rates from \cref{tab:organic-traps}:
        $38.9\%$ ($k = 1$), $58.0\%$ ($k = 2$), $67.9\%$ ($k = 3$).
\end{enumerate}

\emph{Answer.}\; (a) The trapping condition $\mingap < k$ becomes
\emph{easier} to satisfy as $k$ grows: a gap of $g$ that is harmless
for $k \le g$ becomes a trap for any $k > g$.  (b) Since $\min_i G_i$
is $O(1)$---converging to $1$ with probability $1$ as $n \to \infty$
(\cref{prop:record-gap-scaling})---the minimum gap is typically a small
constant.  For $k = 1$, a trap requires $\mingap < 1$, i.e.,
$\mingap = 0$ (consecutive focal records with no non-focal events
between them), which is less common.  For $k = 2$, $\mingap = 1$
suffices, which occurs with probability $\to 1$.  For $k = 3$,
$\mingap \le 2$ suffices, which is even more likely.  (c) The empirical
rates match this monotonic pattern: $38.9\% < 58.0\% < 67.9\%$.
\end{exercise}
