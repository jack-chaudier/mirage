% ══════════════════════════════════════════════════════════════
%  Chapter 10 — The Narrative Origin Story
% ══════════════════════════════════════════════════════════════
\chapter{The Narrative Origin Story}\label{ch:narrative}

The formal theory developed in
\cref{ch:absorbing-states,ch:context-algebra,ch:absorbing-ideal,ch:mirage}
was not built in the abstract.  It grew out of a specific system---the
Lorien narrative simulation~\citep{gaffney2026narrative}, a story-sifting
system~\citep{garbe2019storysifting}---and a specific set of failures that resisted
every ad~hoc fix we tried.  This chapter returns to the origin.  We
present the three experimental findings that motivated the algebraic
programme, show how each finding connects to a theorem or construction
from the preceding chapters, and demonstrate that the formal theory
retroactively explains every empirical anomaly we observed.

The structure follows three questions, each answered by one section.
\Cref{sec:grammar-regularizer} asks: \emph{why does a stricter grammar
produce better arcs?}  \Cref{sec:phase-collapse} asks: \emph{why does
Diana's arc collapse in exactly 9 out of 50 seeds?}
\Cref{sec:evolution-pacing} asks: \emph{why does agent evolution help in
depleted environments but hurt in fresh ones?}  A final section
(\cref{sec:narrative-formal-connection}) maps each finding to the formal
apparatus.


%% ═══════════════════════════════════════════════════════════════════
\section{Grammar as Regularizer}\label{sec:grammar-regularizer}
%% ═══════════════════════════════════════════════════════════════════

The Lorien arc-extraction pipeline scores candidate arcs with a
\emph{quality metric}~$Q$ that combines tension, irony, and thematic
coherence into a single scalar.  The pipeline then selects the
highest-$Q$ arc that satisfies a \emph{beat grammar}---the monotonic
phase grammar of \cref{sec:phase-grammar-dfa}, with four phases in
strict order:

\begin{center}
\textsc{setup}
\;\;$\longrightarrow$\;\;
\textsc{development}
\;\;$\longrightarrow$\;\;
\textsc{turning\_point}
\;\;$\longrightarrow$\;\;
\textsc{resolution}.
\end{center}

\noindent
The grammar enforces three structural constraints: (i)~no phase
regressions (monotonicity), (ii)~at most a bounded number of turning
points, and (iii)~at least $k \ge 1$ development beats before the
turning point.  We call this the \emph{strict grammar}.

A natural question arose during development: is the grammar unnecessarily
restrictive?  Would relaxing it allow the $Q$-metric to find
higher-quality arcs?  We tested a \emph{relaxed grammar} that permitted
1--2 turning points and allowed up to one phase regression.  The relaxed
grammar is strictly more permissive: every strict-valid arc is also
relaxed-valid.  The results were unambiguous and surprising.

\begin{table}[t]
  \centering
  \begin{tabular}{lcc}
    \toprule
    \textbf{Metric} & \textbf{Strict grammar} & \textbf{Relaxed grammar} \\
    \midrule
    All-valid rate & 88\% & 32\% \\
    \bottomrule
  \end{tabular}
  \caption{%
    Grammar relaxation experiment.  The relaxed grammar is strictly more
    permissive than the strict grammar, yet the all-valid rate collapses
    from 88\% to 32\%.%
  }\label{tab:grammar-relaxation}
\end{table}

The strict grammar achieves an 88\% all-valid rate across seeds.  The
relaxed grammar---which admits every sequence the strict grammar
admits, plus many more---collapses to 32\%.  Permissiveness destroyed
performance.

\subsection{Why Permissiveness Hurts}\label{sec:permissiveness}

The explanation is that the strict grammar acts as a \emph{regularizer}
during search.  With strict constraints, the search algorithm is forced
to select events that form a coherent narrative arc: setup events
precede development events, development events precede the turning
point, and the turning point precedes the resolution.  Each constraint
prunes the search space, channelling the $Q$-maximising selection toward
sequences that are not merely high-scoring but narratively well-formed.

Without strict constraints, the $Q$-metric is free to select
high-scoring \emph{fragments} that are narratively incoherent.  An
early catastrophe followed by sixteen consequence events can score well
on tension and irony---the catastrophe is tense, and the consequences
provide thematic resonance---while lacking the developmental arc that
makes a story function as a story.  The grammar prevents precisely this
failure: it is a structural guard against Goodhart's law~\citep{goodhart1984problems}, in which a
measure that becomes a target ceases to be a good measure---a
phenomenon studied more broadly as reward hacking~\citep{skalse2022defining}.
When $Q$ is the only objective, the search exploits $Q$ at the expense
of narrative structure.  When the grammar co-constrains the search,
the search cannot exploit $Q$ without also satisfying structural
requirements.

\subsection{Single-Constraint Attribution}\label{sec:single-constraint}

To identify which grammatical constraint carries the regularisation
effect, we relaxed each of the four constraint dimensions independently:

\begin{enumerate}[label=(\roman*)]
  \item \textbf{Minimum development beats}
    (\texttt{min\_development\_beats}): relaxed from $\ge 1$ to $\ge 0$.
  \item \textbf{Maximum phase regressions}
    (\texttt{max\_phase\_regressions}): relaxed from $0$ to $1$.
  \item \textbf{Protagonist coverage}
    (\texttt{protagonist\_coverage}): relaxed threshold.
  \item \textbf{Minimum timespan fraction}
    (\texttt{min\_timespan\_fraction}): relaxed threshold.
\end{enumerate}

\noindent
Only \texttt{min\_development\_beats} matters.  When we relax it from
requiring $\ge 1$ to requiring $\ge 0$ development beats, the all-valid
rate jumps from 64\% to 100\%.  The other three dimensions are
\emph{structurally inert}: relaxing any of them produces no change in
validity.  The regularisation effect is concentrated entirely in the
development-beat requirement.

\subsection{The Rescore-Only Test}\label{sec:rescore-test}

A rescore-only test confirms that the failure is one of
\emph{content absence}, not constraint mismatch.  We took all 855
strict-valid sequences from the experimental corpus and validated them
against the relaxed grammar: 855 out of 855 passed (100\%).  We then
took the 45 strict-invalid sequences and validated them against the
relaxed grammar: 0 out of 45 recovered.

\begin{table}[t]
  \centering
  \begin{tabular}{lcc}
    \toprule
    \textbf{Source sequences} & \textbf{Count} &
    \textbf{Relaxed-valid} \\
    \midrule
    Strict-valid & 855 & 855/855 (100\%) \\
    Strict-invalid & 45 & 0/45 (0\%) \\
    \bottomrule
  \end{tabular}
  \caption{%
    Rescore-only test.  Strict-valid sequences universally pass the
    relaxed grammar.  Strict-invalid sequences universally fail it.
    The failure is content absence, not constraint mismatch.%
  }\label{tab:rescore}
\end{table}

If the failure were a constraint mismatch---sequences that are
structurally sound but happen to violate a technicality of the strict
grammar---then rescoring against the relaxed grammar would recover at
least some of them.  The fact that zero recover means the sequences lack
development content entirely.  There are no development events to score,
under any grammar.

\begin{remark}[Endogenous pivot connection]\label{rem:grammar-pivot}
  This finding is a concrete instance of the endogenous pivot problem.
  The search selects events that maximise $Q$.  Without the development
  requirement, $Q$-maximising selections skip development events entirely,
  anchoring the turning point early---typically on the highest-tension
  event, which is often an early catastrophe---and filling the arc with
  high-tension consequences.  The grammar forces the search to include
  development events, which pushes the turning point later in the
  timeline and creates a proper dramatic arc.  The strict grammar does
  not merely filter bad arcs; it reshapes the search landscape so that
  the $Q$-optimal arc within the constrained space is qualitatively
  different from the $Q$-optimal arc in the unconstrained space.
\end{remark}


%% ═══════════════════════════════════════════════════════════════════
\section{Phase Collapse Anatomy: Diana's Arc}\label{sec:phase-collapse}
%% ═══════════════════════════════════════════════════════════════════

\Cref{ch:motivation} introduced Diana's collapsed arc as a motivating
example.  We now dissect the failure in full quantitative detail.

Diana is a peripheral observer (evader archetype) in a six-agent
dinner-party simulation.  Of 50 random seeds under full agent evolution,
9 produce invalid arcs for Diana.  All 9 fail identically: zero
complication or escalation beats, meaning the \textsc{development} phase
is completely absent.

\subsection{Diana Is Not Event-Starved}\label{sec:not-starved}

A natural hypothesis is that Diana's failures reflect insufficient
simulation material---perhaps she simply does not participate in enough
events to form an arc.  The data refute this hypothesis decisively.
\Cref{tab:diana-diagnostic} presents the key diagnostic comparison.

\begin{table}[t]
  \centering
  \begin{tabular}{lcc}
    \toprule
    \textbf{Metric} & \textbf{Valid ($N=41$)} & \textbf{Invalid ($N=9$)} \\
    \midrule
    Simulation involvement   & 43.5 & 42.7 \\
    Arc events               & 20.0 & 20.0 \\
    Complication beats        & 3.3  & 0.0  \\
    Escalation beats          & 2.9  & 0.0  \\
    Consequence beats         & 7.8  & 16.4 \\
    TP position (median)      & 0.69 & 0.13 \\
    Events before TP          & 11.2 & 2.6  \\
    Events after TP           & 7.8  & 16.4 \\
    \bottomrule
  \end{tabular}
  \caption{%
    Diagnostic comparison of Diana's valid and invalid arcs across 50
    seeds.  Invalid arcs have comparable simulation involvement and
    identical arc length, but zero development beats, extremely early
    turning-point position, and a heavily consequence-dominated
    structure.%
  }\label{tab:diana-diagnostic}
\end{table}

Diana's mean simulation involvement in the 9 invalid seeds is 42.7
events---virtually identical to the 43.5-event mean in valid seeds.  The
arc lengths are the same: 20.0 events in both cases.  The event pool is
present; the events are simply the wrong kind.  In invalid arcs, the
entire developmental middle of the story---complication and escalation
beats---is absent.  In its place, 16.4 consequence beats fill the arc
after a turning point that arrives at normalised position~0.13.

\subsection{Turning-Point Anchoring}\label{sec:tp-anchoring}

The turning-point position data reveal a categorical separation, not a
gradual degradation.  Valid arcs have a median turning-point position of
0.69---the classical mid-to-late position that leaves room for both
development and resolution.  Invalid arcs have a median position of
0.13---barely past the opening.

The distributions have \emph{zero overlap}.  No valid arc has a turning
point as early as the latest invalid turning point; no invalid arc has a
turning point as late as the earliest valid one.  This is bimodal
failure: two categorically distinct regimes, not a continuum of
degradation.

\subsection{Candidate Pool Contamination}\label{sec:pool-contamination}

The mechanism becomes clear when we examine the candidate pools.  Across
the 9 invalid seeds, there are 33 total candidate arcs.  Every single
one has zero development beats.  Turning-point positions range from
0.062 to 0.161.  The pools are \emph{entirely degenerate}---every
candidate produces the same early-TP, no-development failure.

In contrast, the valid seeds contain 38 candidates, of which 36
(94.7\%) have at least one development beat.  The valid pools are
overwhelmingly healthy; the invalid pools are uniformly broken.

\subsection{The Dual-Function Injection Mechanism}%
\label{sec:injection-mechanism}

The Lorien pipeline includes a \emph{protagonist-event injection} step:
for peripheral agents like Diana, the system adds the focal actor's
maximum-weight event to the candidate pool.  This injection is
structurally necessary---without it, Diana's solo breadth-first search
yields 0 out of 50 valid arcs, because she has too few
directly-connected events to form a complete arc.

But the injection has a side effect.  The injected events are
high-weight, early-timeline events from the simulation's dramatic
opening: confrontations, revelations, catastrophes that drove the main
plot.  These events contaminate the candidate pool with early pivots.
The greedy search, which selects the turning point as the highest-weight
focal event, locks onto these early catastrophes.  With the turning
point anchored before position~0.20, there is almost no timeline
available for development beats, and the grammar's prefix requirement
becomes unsatisfiable.

\paragraph{Temporal injection filter.}
We tested this diagnosis by excluding injection events that occur before
normalised position~0.20 in the timeline.  The filter recovers 7 of the
9 focal seeds, raising the all-valid rate from 41/50 to 47/50.  This
confirms the mechanism: early-event contamination via the injection step
is the root cause of the collapse.

\subsection{Two-Regime Classification}\label{sec:two-regime}

The 9 failures decompose into two distinct regimes:

\begin{enumerate}[label=(\roman*),itemsep=4pt]
  \item \textbf{Search exploration failure (5/9 seeds).}\;
    Valid mid-arc alternatives exist that outscore the search-selected
    invalid arc.  Better arcs are present in the candidate space, but
    the greedy search does not find them.  The failure is in search
    coverage, not in the metric or the event pool.

  \item \textbf{Metric misalignment (4/9 seeds).}\;
    The invalid arc actually outscores the best valid alternative by
    approximately 0.098 in $Q$.  The quality metric genuinely prefers
    the early kinetic spike---the high-tension catastrophe followed by
    rapid consequences---over a structurally sound arc with proper
    development.  The failure is in the metric itself.
\end{enumerate}

\noindent
Despite this decomposition, both regimes share a single root cause:
premature turning-point anchoring via pool contamination.  In regime~(i),
the contamination blinds the search; in regime~(ii), the contamination
biases the metric.  The upstream mechanism is identical.


%% ═══════════════════════════════════════════════════════════════════
\section{Evolution as Pacing Control}\label{sec:evolution-pacing}
%% ═══════════════════════════════════════════════════════════════════

The Lorien system evolves agent profiles across simulation depth: after
each simulation run, an agent's personality parameters are updated based
on the events it experienced.  This section examines the interaction
between evolution and arc validity.

\subsection{The Quality--Validity Tradeoff}\label{sec:quality-validity}

We measure two metrics as a function of \emph{coalition size}~$c$, the
number of agents whose profiles are evolved (holding all others at
their default profiles):

\begin{itemize}[itemsep=3pt]
  \item \textbf{Mean $Q$}: the raw quality score, averaging tension,
    irony, and thematic coherence.
  \item \textbf{VA} (validity-adjusted score): $Q$ scaled by the
    fraction of arcs that are valid.
\end{itemize}

\noindent
The two metrics diverge systematically.  Mean $Q$ increases monotonically
with coalition size: evolving more agents produces richer, more dramatic
simulations.  But VA drops from its $c = 0$ value of 0.652 to a minimum
of 0.617 at $c = 3$ before partially recovering.  The all-valid rate
drops from 88\% at $c = 0$ (no evolution) to 64\% at $c = 6$ (full
evolution).

Evolution makes the simulation more interesting but makes the arcs
harder to extract.  Evolved agents produce higher-tension events that
cluster early in the timeline, contaminating candidate pools and
anchoring turning points prematurely---the same mechanism we identified
in Diana's collapse.

\subsection{The $\alpha$-Interpolation Experiment}%
\label{sec:alpha-interpolation}

To study the evolution effect at finer resolution, we interpolate between
default and evolved agent profiles using a blending parameter
$\alpha \in [0, 1]$:
\[
  \theta(\alpha) \;=\; (1 - \alpha)\,\theta_{\mathrm{default}}
                       \;+\; \alpha\,\theta_{\mathrm{evolved}}.
\]
For Thorne---the destabilising agent whose evolution most strongly
affects arc validity---VA is sharply peaked at $\alpha = 0.5$, where it
reaches 0.684 with a 95\% all-valid rate.  The optimum is driven
entirely by validity, not raw quality: $Q$ is essentially flat across
$\alpha$.  The peak at $\alpha = 0.5$ reflects a balance point where
Thorne is evolved enough to generate rich events but not so evolved that
his early-timeline catastrophes overwhelm the candidate pools.

\subsection{Factorial Decomposition}\label{sec:factorial}

A $2 \times 3$ factorial design (2 simulation depths $\times$ 3
evolution levels) decomposes the evolution effect into three orthogonal
components:

\begin{enumerate}[label=(\roman*),itemsep=4pt]
  \item \textbf{Amplifier effect}
    (depth~$D_0$, default $\to$ evolved):\;
    $+0.008$ mean $Q$, $-0.028$ VA.\;
    Evolution \emph{hurts} validity in fresh environments.

  \item \textbf{Degradation effect}
    ($D_0 \to D_2$, default profiles):\;
    $-0.007$ mean $Q$, $-0.009$ VA.\;
    Information depletion across simulation depth reduces quality.

  \item \textbf{Repair effect}
    (depth~$D_2$, default $\to$ evolved):\;
    $+0.016$ mean $Q$, $+0.013$ VA.\;
    Evolution \emph{helps} in depleted environments.
\end{enumerate}

\noindent
The repair effect exceeds the amplifier effect by 4~VA points.
Evolution is primarily a \emph{repair mechanism} for depleted
environments, not an improvement mechanism for fresh ones.  In a fresh
simulation ($D_0$), the event landscape is rich enough that the arc
extractor can find valid arcs without assistance; evolution merely adds
high-tension events that contaminate the pools.  In a depleted
simulation ($D_2$), the event landscape has been thinned by repeated
extraction; evolution replenishes the pool with new material, restoring
the development capacity that depletion removed.

\begin{remark}[Scaffolding dependence]\label{rem:scaffolding}
  The depleted canon provides structural scaffolding that constrains
  agent behaviour into extractable patterns.  Evolution \emph{needs}
  that scaffolding to work.  In a fresh environment, the agents have
  no history to constrain them, and evolved profiles produce unconstrained
  high-tension events that are structurally unmoored.  In a depleted
  environment, the accumulated narrative history channels evolved
  behaviour into patterns that the arc extractor can recognise and
  extract.  The scaffolding converts evolution from a destabilising
  amplifier into a targeted repair mechanism.
\end{remark}


%% ═══════════════════════════════════════════════════════════════════
\section{Connection to the Formal Theory}\label{sec:narrative-formal-connection}
%% ═══════════════════════════════════════════════════════════════════

Each of the three empirical findings maps directly to a construction or
theorem from the preceding chapters.  We make these connections
explicit.

\subsection{Phase Collapse and the Absorbing State Theorem}%
\label{sec:collapse-absorbing}

Diana's phase collapse (\cref{sec:phase-collapse}) is a concrete
instance of the prefix-constraint impossibility theorem
(\cref{thm:prefix-impossibility} in \cref{ch:absorbing-states}).  In
Diana's invalid arcs, the development-eligible count is exactly zero:
$\jdev = 0$.  The grammar's prefix requirement is $k = 1$.  Since
$\jdev = 0 < k = 1$, \cref{thm:prefix-impossibility} applies directly:
the greedy policy produces zero valid sequences.  This is not a
statistical tendency or a soft failure mode---it is an exact
impossibility, predicted by a counting argument, confirmed across all 9
invalid seeds without exception.

The theorem also explains \emph{why} the failure is categorical rather
than gradual.  The absorbing-state boundary at $\jdev = k$ is a sharp
threshold: below it, validity is exactly zero; above it, validity is
possible.  The bimodal separation in turning-point position
(\cref{sec:tp-anchoring})---zero overlap between valid and invalid
distributions---is the empirical signature of this sharp boundary.  Seeds
that contaminate Diana's pool with early pivots push $\jdev$ below $k$
and enter the impossibility zone.  Seeds that avoid contamination leave
$\jdev$ well above $k$ and land in the high-validity zone.  There is no
middle ground, exactly as the theorem predicts.

\subsection{Grammar Regularization and the Absorbing Ideal}%
\label{sec:regularization-ideal}

The grammar regularisation result (\cref{sec:grammar-regularizer})
illustrates why the absorbing ideal (\cref{ch:absorbing-ideal}) matters
practically.  The absorbing ideal characterises the algebraic boundary
between feasible and infeasible context elements: an extended context
element $\bar{C} = (\wstar, \dtotal, \dpre, \kappa)$ with $\kappa = 1$
and $\dpre < k$ belongs to the ideal, and no suffix can rescue it.

The grammar's development-beat requirement is the operational
enforcement of the condition $\dpre \ge k$.  When this requirement is
present, the search is forced to include development events, which
ensures that the selected arc's context element satisfies $\dpre \ge k$
and stays outside the absorbing ideal.  When the requirement is relaxed
to $\dpre \ge 0$, the search is free to select arcs whose context
elements fall inside the ideal.  The $Q$-metric, unconstrained by
structural requirements, gravitates toward these ideal elements because
they correspond to early-TP, consequence-heavy arcs that score well on
tension.

The strict grammar does not merely filter outputs; it constrains the
search to operate outside the absorbing ideal.  The rescore-only test
(\cref{sec:rescore-test}) confirms this interpretation: strict-invalid
arcs are not borderline cases that a permissive grammar could rescue.
They are deep inside the ideal---$\jdev = 0$, not $\jdev = k - 1$---and
no relaxation of grammatical constraints can supply the development
content that is entirely absent.

\subsection{Injection Contamination and the Validity Mirage}%
\label{sec:contamination-mirage}

The injection contamination mechanism (\cref{sec:injection-mechanism}) is
a specific pathway to the validity mirage (\cref{ch:mirage}).  The
system \emph{could} produce a valid arc with a different pivot---in 5 of
the 9 failures, valid alternatives exist that outscore the selected
arc.  But the greedy search locks onto the contaminating early pivot and
cannot escape.  From the system's perspective, the arc-extraction
succeeded: it found a turning point (the highest-weight event), assembled
a sequence around it, and reported the result.  From a structural
perspective, the result is broken.

This is precisely the mirage pattern: the system reports success on a
metric (turning-point selection, sequence assembly) while concealing a
structural failure (absent development, premature anchoring).  The
four-metric diagnostic checklist from \cref{sec:checklist}---pivot
preservation, fixed-pivot feasibility, semantic regret, and mirage
gap---would detect this failure immediately.  The contaminating early
pivot fails the fixed-pivot feasibility test (there are not enough
pre-pivot events to satisfy the grammar if we force the original pivot),
and the semantic regret relative to a properly positioned pivot is
catastrophic.

The two-regime classification (\cref{sec:two-regime}) further
illuminates the mirage.  In the search-exploration regime, valid arcs
exist but the search misses them: this is a mirage of search coverage,
where the system appears to have explored the candidate space but has in
fact been trapped by the contaminated pool.  In the metric-misalignment
regime, the $Q$-metric actively prefers the broken arc: this is a mirage
of metric alignment, where the quality score appears to validate the
selection while concealing its structural deficiency.

\bigskip
\noindent
Taken together, these three connections---absorbing states, the
absorbing ideal, and the validity mirage---demonstrate that the formal
theory is not an ex~post rationalisation.  The theory was built to
explain these specific failures, and it explains them exactly: not
approximately, not statistically, but as deductive consequences of the
algebraic structure.  The narrative origin story is, in this precise
sense, the theory's empirical foundation.
