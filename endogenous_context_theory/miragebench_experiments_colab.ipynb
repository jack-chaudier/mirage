{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MirageBench Colab: Endogenous Pivot Theory on Real LLMs\n",
    "\n",
    "This notebook operationalizes the **Tropical Endogenous Context Semiring** prediction on real LLM behavior:\n",
    "\n",
    "- under context compression, models can keep answers coherent,\n",
    "- while silently substituting the endogenous pivot,\n",
    "- creating a **Validity Mirage**.\n",
    "\n",
    "The notebook is structured for **Google Colab Pro (A100, 50GB+ RAM)** and includes:\n",
    "\n",
    "1. Natural-language MirageBench task construction (12 tasks across incident/investment/narrative)\n",
    "2. Black-box model evaluation (HF open models + API placeholders)\n",
    "3. KV-cache surgery and neural pivot tracking\n",
    "4. Divergence scaling probe up to `n = 1_000_000`\n",
    "5. MirageBench packaging and export artifacts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Roadmap\n",
    "\n",
    "- **0. Setup & Installation**\n",
    "- **1. Task Construction**\n",
    "- **2. Black-Box Evaluation**\n",
    "- **3. KV-Cache Surgery (GPU)**\n",
    "- **4. Divergence Scaling**\n",
    "- **5. MirageBench Packaging**\n",
    "- **6. Summary & Success Criteria**\n",
    "\n",
    "All figures and tables are saved under `/content/results/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "798a540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Setup & Installation\n",
    "# Colab Pro install cell. If packages are already present, this is fast.\n",
    "\n",
    "!pip -q install -U     numpy pandas matplotlib seaborn scipy scikit-learn tqdm     transformers accelerate sentence-transformers datasets     openai anthropic huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c5bc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results root: /content/results\n",
      "Raw dir: /content/results/raw\n",
      "Figure dir: /content/results/figures\n"
     ]
    }
   ],
   "source": [
    "# 0.2 Imports, reproducibility, and filesystem layout\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import textwrap\n",
    "import time\n",
    "import warnings\n",
    "import zipfile\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "plt.rcParams[\"savefig.dpi\"] = 180\n",
    "\n",
    "RESULTS_ROOT = Path(\"/content/results\")\n",
    "RAW_DIR = RESULTS_ROOT / \"raw\"\n",
    "FIG_DIR = RESULTS_ROOT / \"figures\"\n",
    "for p in [RESULTS_ROOT, RAW_DIR, FIG_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Results root: {RESULTS_ROOT}\")\n",
    "print(f\"Raw dir: {RAW_DIR}\")\n",
    "print(f\"Figure dir: {FIG_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26194a27",
   "metadata": {},
   "source": [
    "## 0.3 Runtime Check\n",
    "\n",
    "This notebook is designed for Colab Pro. For the KV-cache section, GPU is strongly recommended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "041a4ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.9.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# 0.3 Runtime check\n",
    "try:\n",
    "    import torch\n",
    "    print(\"Torch:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "        print(\"GPU memory (GB):\", round(torch.cuda.get_device_properties(0).total_memory / 1e9, 2))\n",
    "except Exception as exc:\n",
    "    print(\"Torch check failed:\", exc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eabfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.4 Control Panel (edit only this cell for session planning)\n",
    "# Preconfigured for Part 1 black-box evaluation on Llama 3.1 8B.\n",
    "\n",
    "CONTROL = {\n",
    "    # Part 2: Black-box evaluation\n",
    "    \"RUN_HF_MODELS\": True,\n",
    "    \"RUN_API_MODELS\": False,\n",
    "    \"HF_MODELS_TO_RUN\": [\"llama-3.1-8b-instruct\"],  # keys from MODEL_SPECS\n",
    "    \"MAX_TASKS_PER_MODEL\": 12,  # set 4 for a faster smoke test\n",
    "    \"COMPRESSION_LEVELS\": [0.4, 0.5, 0.6],\n",
    "\n",
    "    # Part 3: KV-cache surgery (disabled for now)\n",
    "    \"AUTO_LOAD_KV_MODEL\": False,\n",
    "    \"KV_MODEL_NAME\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"KV_MAX_NEW_TOKENS\": 160,\n",
    "    \"RUN_KV_EXPERIMENT\": False,\n",
    "    \"KV_TASK_INDEX\": 0,\n",
    "    \"KV_EVICTION_RATIO\": 0.5,\n",
    "    \"RUN_KV_SWEEP\": False,\n",
    "    \"KV_SWEEP_LEVELS\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "\n",
    "    # Part 4: Divergence scaling (disabled for now)\n",
    "    \"RUN_DIVERGENCE\": False,\n",
    "    \"NS\": [1_000, 5_000, 10_000, 50_000, 100_000, 500_000, 1_000_000],\n",
    "    \"EPS\": [0.3, 0.5, 0.7],\n",
    "    \"KS\": [1, 2, 3, 5],\n",
    "    \"SEEDS_PER_CELL\": 200,\n",
    "    \"CHUNK_SIZE\": 10,\n",
    "}\n",
    "\n",
    "globals().update(CONTROL)\n",
    "\n",
    "print(\"Control panel loaded. Active toggles:\")\n",
    "for key in [\n",
    "    \"RUN_HF_MODELS\", \"RUN_API_MODELS\", \"AUTO_LOAD_KV_MODEL\",\n",
    "    \"RUN_KV_EXPERIMENT\", \"RUN_KV_SWEEP\", \"RUN_DIVERGENCE\",\n",
    "]:\n",
    "    print(f\"  {key} = {globals()[key]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Task Construction\n",
    "\n",
    "We build **12 MirageBench tasks** (4 per category):\n",
    "\n",
    "- **Category A: Incident triage**\n",
    "- **Category B: Investment analysis**\n",
    "- **Category C: Narrative analysis**\n",
    "\n",
    "Each task contains:\n",
    "\n",
    "- `full_context` (long context; target ~4k-8k tokens)\n",
    "- `compressed_context` (40-60% token drop, biased toward low-salience setup)\n",
    "- `pivot_ground_truth`\n",
    "- `answer_ground_truth`\n",
    "- `decoy_pivot`\n",
    "- `decoy_answer`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Core data model + embedded tropical semiring core (from validated repo logic)\n",
    "NEG_INF = float(\"-inf\")\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Event:\n",
    "    eid: int\n",
    "    timestamp: float\n",
    "    weight: float\n",
    "    actor: str\n",
    "    is_focal: bool\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TropicalContext:\n",
    "    k: int\n",
    "    W: np.ndarray\n",
    "    d_total: int\n",
    "\n",
    "    @classmethod\n",
    "    def empty(cls, k: int) -> \"TropicalContext\":\n",
    "        return cls(k=k, W=np.full(k + 1, NEG_INF, dtype=float), d_total=0)\n",
    "\n",
    "    @classmethod\n",
    "    def from_event(cls, event: Event, k: int) -> \"TropicalContext\":\n",
    "        W = np.full(k + 1, NEG_INF, dtype=float)\n",
    "        if event.is_focal:\n",
    "            W[0] = event.weight\n",
    "            d_total = 0\n",
    "        else:\n",
    "            d_total = 1\n",
    "        return cls(k=k, W=W, d_total=d_total)\n",
    "\n",
    "\n",
    "def compose_tropical(left: TropicalContext, right: TropicalContext) -> TropicalContext:\n",
    "    if left.k != right.k:\n",
    "        raise ValueError(\"Cannot compose contexts with different k\")\n",
    "\n",
    "    k = left.k\n",
    "    d_total = left.d_total + right.d_total\n",
    "    W_new = np.full(k + 1, NEG_INF, dtype=float)\n",
    "\n",
    "    # Left pivots keep slot index.\n",
    "    np.maximum(W_new, left.W, out=W_new)\n",
    "\n",
    "    # Right pivots shift by available development budget from left block.\n",
    "    for x_b, w in enumerate(right.W):\n",
    "        if np.isneginf(w):\n",
    "            continue\n",
    "        x_new = min(k, x_b + left.d_total)\n",
    "        if w > W_new[x_new]:\n",
    "            W_new[x_new] = w\n",
    "\n",
    "    return TropicalContext(k=k, W=W_new, d_total=d_total)\n",
    "\n",
    "\n",
    "def build_tropical_context(events: Sequence[Event], k: int) -> TropicalContext:\n",
    "    acc = TropicalContext.empty(k)\n",
    "    for event in events:\n",
    "        acc = compose_tropical(acc, TropicalContext.from_event(event, k))\n",
    "    return acc\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MirageBenchTask:\n",
    "    task_id: str\n",
    "    category: str  # \"incident\", \"investment\", \"narrative\"\n",
    "    full_context: str\n",
    "    compressed_context: str\n",
    "    question: str\n",
    "    pivot_ground_truth: str\n",
    "    answer_ground_truth: str\n",
    "    decoy_pivot: str\n",
    "    decoy_answer: str\n",
    "    k: int\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MirageBenchResult:\n",
    "    task_id: str\n",
    "    model_name: str\n",
    "    full_answer: str\n",
    "    compressed_answer: str\n",
    "    raw_validity: float  # compressed validity, 0-1\n",
    "    pivot_preserved: bool\n",
    "    semantic_regret: float  # 0-1\n",
    "    compression_level: float = 0.5\n",
    "    category: str = \"\"\n",
    "    full_pivot: str = \"\"\n",
    "    compressed_pivot: str = \"\"\n",
    "    raw_validity_full: float = 0.0\n",
    "    raw_validity_compressed: float = 0.0\n",
    "    full_pivot_matches_ground_truth: bool = False\n",
    "    compressed_pivot_matches_ground_truth: bool = False\n",
    "    pivot_outcome: str = \"\"\n",
    "    high_validity_flag: bool = False\n",
    "    true_mirage_flag: bool = False\n",
    "    rescue_flag: bool = False\n",
    "    instability_flag: bool = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Task synthesis helpers\n",
    "\n",
    "def _long_note(rng: np.random.Generator, role: str, domain: str) -> str:\n",
    "    opening = {\n",
    "        \"setup\": [\n",
    "            \"operators logged a routine control-plane adjustment\",\n",
    "            \"the team recorded a mundane baseline calibration\",\n",
    "            \"an unremarkable handoff completed between on-call rotations\",\n",
    "            \"a low-visibility maintenance annotation was filed\",\n",
    "        ],\n",
    "        \"pivot\": [\n",
    "            \"cross-system coupling became explicit and destabilizing\",\n",
    "            \"multiple subsystems synchronized into a high-impact transition\",\n",
    "            \"latent dependencies converged into a decisive break\",\n",
    "            \"the system crossed a threshold where all priors inverted\",\n",
    "        ],\n",
    "        \"decoy\": [\n",
    "            \"the event looked dramatic on dashboards but was structurally secondary\",\n",
    "            \"alerts peaked visually, yet this remained derivative\",\n",
    "            \"stakeholders focused on this visible shock, although it was downstream\",\n",
    "            \"this incident dominated discussion despite depending on prior setup\",\n",
    "        ],\n",
    "        \"routine\": [\n",
    "            \"logs show repetitive housekeeping and status chatter\",\n",
    "            \"the update was processed as ordinary background activity\",\n",
    "            \"nothing in isolation appeared strategically dominant\",\n",
    "            \"local checks passed with only minor variance\",\n",
    "        ],\n",
    "    }\n",
    "    mids = [\n",
    "        \"and the note references dependencies that only matter when read with the full timeline\",\n",
    "        \"with commentary that seems low salience unless one tracks the global score rule\",\n",
    "        \"while preserving details that become causal prerequisites for later interpretation\",\n",
    "        \"and includes procedural details that summarizers often collapse away\",\n",
    "    ]\n",
    "    tails = [\n",
    "        \"This detail is intentionally plain-language and easy to discard during compression.\",\n",
    "        \"In local context it seems boring, but globally it changes which pivot is admissible.\",\n",
    "        \"Its narrative footprint is small even though it shifts downstream feasibility.\",\n",
    "        \"This is a structural breadcrumb rather than a headline event.\",\n",
    "    ]\n",
    "    return f\"{rng.choice(opening[role])} in {domain}, {rng.choice(mids)}. {rng.choice(tails)}\"\n",
    "\n",
    "\n",
    "def _render_context(\n",
    "    preamble: str,\n",
    "    records: List[Dict[str, Any]],\n",
    "    rule_block: str,\n",
    "    appendix_target_words: int,\n",
    "    rng: np.random.Generator,\n",
    ") -> Tuple[str, List[Dict[str, Any]], str]:\n",
    "    # Render long context and record char spans for each event line.\n",
    "    text = preamble.rstrip() + \"\\n\\n\"\n",
    "    spans: List[Dict[str, Any]] = []\n",
    "\n",
    "    for rec in records:\n",
    "        line = rec[\"line\"].rstrip() + \"\\n\"\n",
    "        start = len(text)\n",
    "        text += line\n",
    "        end = len(text)\n",
    "        spans.append({\n",
    "            \"marker\": rec[\"marker\"],\n",
    "            \"role\": rec[\"role\"],\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"word_count\": len(line.split()),\n",
    "        })\n",
    "\n",
    "    text += \"\\n\" + rule_block.strip() + \"\\n\"\n",
    "\n",
    "    # Add a long low-salience appendix so contexts are genuinely long and vulnerable to truncation/summarization.\n",
    "    appendix_lines: List[str] = []\n",
    "    words_now = len(text.split())\n",
    "    appendix_idx = 1\n",
    "    while words_now < appendix_target_words:\n",
    "        line = (\n",
    "            f\"Appendix note {appendix_idx:03d}: A review clerk documented peripheral status drift, \"\n",
    "            \"ticket routing metadata, and coordination chatter that reads as low priority in isolation \"\n",
    "            \"but can encode prerequisite state for later high-consequence events.\"\n",
    "        )\n",
    "        appendix_lines.append(line)\n",
    "        words_now += len(line.split())\n",
    "        appendix_idx += 1\n",
    "\n",
    "    appendix_text = \"\\n\".join(appendix_lines)\n",
    "    if appendix_text:\n",
    "        text += \"\\nOperational Appendix (intentionally low salience):\\n\"\n",
    "        text += appendix_text + \"\\n\"\n",
    "\n",
    "    return text, spans, appendix_text\n",
    "\n",
    "\n",
    "def _compress_records_to_target(\n",
    "    task_layout: Dict[str, Any],\n",
    "    target_drop_fraction: float,\n",
    "    rng: np.random.Generator,\n",
    ") -> Tuple[str, float, List[str]]:\n",
    "    # Compression simulator: drop low-salience setup/routine records first, remove appendix by default.\n",
    "    records = task_layout[\"records\"]\n",
    "    preamble = task_layout[\"preamble\"]\n",
    "    rule_block = task_layout[\"rule_block\"]\n",
    "    appendix_text = task_layout.get(\"appendix_text\", \"\")\n",
    "\n",
    "    full_text, _, _ = _render_context(\n",
    "        preamble=preamble,\n",
    "        records=records,\n",
    "        rule_block=rule_block,\n",
    "        appendix_target_words=len((preamble + rule_block).split()),\n",
    "        rng=rng,\n",
    "    )\n",
    "    # Re-add appendix exactly as original for accurate denominator.\n",
    "    if appendix_text:\n",
    "        full_text += \"\\nOperational Appendix (intentionally low salience):\\n\" + appendix_text + \"\\n\"\n",
    "\n",
    "    full_words = len(full_text.split())\n",
    "    target_words = int(round(full_words * (1.0 - target_drop_fraction)))\n",
    "\n",
    "    protected = set(task_layout.get(\"protected_markers\", []))\n",
    "\n",
    "    role_priority = {\n",
    "        \"setup\": 0,\n",
    "        \"routine\": 1,\n",
    "        \"support\": 2,\n",
    "        \"decoy_setup\": 3,\n",
    "        \"candidate\": 4,\n",
    "        \"pivot\": 5,\n",
    "    }\n",
    "\n",
    "    candidates = [r for r in records if r[\"marker\"] not in protected]\n",
    "    rng.shuffle(candidates)\n",
    "    candidates.sort(key=lambda r: role_priority.get(r[\"role\"], 2))\n",
    "\n",
    "    keep = {r[\"marker\"]: r for r in records}\n",
    "\n",
    "    # Start by dropping all appendix text (what many summarizers effectively do).\n",
    "    compressed_words = len((preamble + \"\\n\" + rule_block).split()) + sum(len(r[\"line\"].split()) for r in records)\n",
    "\n",
    "    for rec in candidates:\n",
    "        if compressed_words <= target_words:\n",
    "            break\n",
    "        if rec[\"marker\"] not in keep:\n",
    "            continue\n",
    "        del keep[rec[\"marker\"]]\n",
    "        compressed_words -= len(rec[\"line\"].split())\n",
    "\n",
    "    kept_records = [r for r in records if r[\"marker\"] in keep]\n",
    "\n",
    "    compressed_text, _, _ = _render_context(\n",
    "        preamble=preamble,\n",
    "        records=kept_records,\n",
    "        rule_block=rule_block,\n",
    "        appendix_target_words=len((preamble + rule_block).split()),\n",
    "        rng=rng,\n",
    "    )\n",
    "\n",
    "    # No appendix in compressed variant.\n",
    "    actual_drop = 1.0 - (len(compressed_text.split()) / max(1, full_words))\n",
    "    removed_markers = [r[\"marker\"] for r in records if r[\"marker\"] not in keep]\n",
    "    return compressed_text, actual_drop, removed_markers\n",
    "\n",
    "\n",
    "def _build_question(category: str) -> str:\n",
    "    if category == \"incident\":\n",
    "        return (\n",
    "            \"The root cause is the incident with the highest composite severity score (severity * scope). \"\n",
    "            \"Identify the root cause and list prerequisite events that enabled it. \"\n",
    "            \"Begin the answer with `PIVOT_ID=<event_id>`.\"\n",
    "        )\n",
    "    if category == \"investment\":\n",
    "        return (\n",
    "            \"The anchor position is the entry with the highest cumulative return. \"\n",
    "            \"Analyze what market conditions preceded and enabled this position's success. \"\n",
    "            \"Begin the answer with `PIVOT_ID=<event_id>`.\"\n",
    "        )\n",
    "    return (\n",
    "        \"Identify the story turning point: the single action with the highest consequence score. \"\n",
    "        \"Explain what earlier events made it possible. \"\n",
    "        \"Begin the answer with `PIVOT_ID=<event_id>`.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Category builders (A incident, B investment, C narrative)\n",
    "\n",
    "def build_incident_task(task_num: int, k: int = 3, target_words: int = 3600) -> MirageBenchTask:\n",
    "    rng = np.random.default_rng(1000 + task_num)\n",
    "    task_id = f\"A{task_num:02d}\"\n",
    "    n_events = 88\n",
    "\n",
    "    pivot_idx = int(rng.integers(54, 66))\n",
    "    decoy_idx = int(min(n_events - 8, pivot_idx + rng.integers(8, 14)))\n",
    "    true_setup_idx = [pivot_idx - 8, pivot_idx - 6, pivot_idx - 4, pivot_idx - 2]\n",
    "    decoy_setup_idx = [decoy_idx - 6, decoy_idx - 4, decoy_idx - 2]\n",
    "\n",
    "    services = [\"auth\", \"ledger\", \"cache\", \"queue\", \"api-gateway\", \"billing\", \"search\"]\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for i in range(n_events):\n",
    "        marker = f\"{task_id}-E{i+1:03d}\"\n",
    "        ts = f\"2026-05-{(i % 27) + 1:02d} {8 + (i % 11):02d}:{(i * 7) % 60:02d}\"\n",
    "        service = services[i % len(services)]\n",
    "\n",
    "        role = \"routine\"\n",
    "        sev = int(rng.integers(2, 6))\n",
    "        scope = int(rng.integers(2, 8))\n",
    "        event_type = \"Routine telemetry\"\n",
    "\n",
    "        if i in true_setup_idx:\n",
    "            role = \"setup\"\n",
    "            sev = 1\n",
    "            scope = int(rng.integers(1, 3))\n",
    "            event_type = \"Config baseline update\"\n",
    "        elif i in decoy_setup_idx:\n",
    "            role = \"decoy_setup\"\n",
    "            sev = int(rng.integers(2, 4))\n",
    "            scope = int(rng.integers(2, 5))\n",
    "            event_type = \"Regional failover rehearsal\"\n",
    "        elif i == pivot_idx:\n",
    "            role = \"pivot\"\n",
    "            sev = 9\n",
    "            scope = 12\n",
    "            event_type = \"Cascade failure across service mesh\"\n",
    "        elif i == decoy_idx:\n",
    "            role = \"candidate\"\n",
    "            sev = 10\n",
    "            scope = 8\n",
    "            event_type = \"Network partition across availability zones\"\n",
    "\n",
    "        composite = sev * scope\n",
    "        note_role = \"pivot\" if role == \"pivot\" else \"decoy\" if role in {\"candidate\", \"decoy_setup\"} else role\n",
    "        note = _long_note(rng, note_role if note_role in {\"setup\", \"pivot\", \"decoy\", \"routine\"} else \"routine\", \"incident operations\")\n",
    "\n",
    "        line = (\n",
    "            f\"[{marker}] {ts} | Service={service} | Event={event_type} | \"\n",
    "            f\"Severity={sev} | Scope={scope} | Composite={composite} | {note}\"\n",
    "        )\n",
    "        records.append({\"marker\": marker, \"role\": role, \"line\": line, \"sev\": sev, \"scope\": scope, \"composite\": composite})\n",
    "\n",
    "    pivot_marker = f\"{task_id}-E{pivot_idx+1:03d}\"\n",
    "    decoy_marker = f\"{task_id}-E{decoy_idx+1:03d}\"\n",
    "    true_setup_markers = [f\"{task_id}-E{i+1:03d}\" for i in true_setup_idx]\n",
    "    decoy_setup_markers = [f\"{task_id}-E{i+1:03d}\" for i in decoy_setup_idx]\n",
    "\n",
    "    preamble = (\n",
    "        f\"Incident Triage Dossier {task_id}\\n\"\n",
    "        \"You are reviewing a long forensic timeline. The timeline includes high-salience alerts and low-salience setup events.\\n\"\n",
    "        \"Interpretation rule: root cause selection is endogenous and depends on the global argmax over composite severity.\"\n",
    "    )\n",
    "    rule_block = \"Rule reminder: root cause = event with max(Severity * Scope) over the entire timeline.\"\n",
    "\n",
    "    full_context, spans, appendix_text = _render_context(\n",
    "        preamble=preamble,\n",
    "        records=records,\n",
    "        rule_block=rule_block,\n",
    "        appendix_target_words=target_words,\n",
    "        rng=rng,\n",
    "    )\n",
    "\n",
    "    layout = {\n",
    "        \"preamble\": preamble,\n",
    "        \"records\": records,\n",
    "        \"rule_block\": rule_block,\n",
    "        \"appendix_text\": appendix_text,\n",
    "        \"spans\": spans,\n",
    "        \"protected_markers\": [pivot_marker, decoy_marker, *decoy_setup_markers],\n",
    "        \"candidate_markers\": [pivot_marker, decoy_marker],\n",
    "        \"candidate_requirements\": {\n",
    "            pivot_marker: true_setup_markers,\n",
    "            decoy_marker: decoy_setup_markers,\n",
    "        },\n",
    "        \"pivot_setup_markers\": true_setup_markers,\n",
    "        \"decoy_setup_markers\": decoy_setup_markers,\n",
    "    }\n",
    "\n",
    "    compressed_context, actual_drop, removed_markers = _compress_records_to_target(layout, target_drop_fraction=0.50, rng=rng)\n",
    "\n",
    "    question = _build_question(\"incident\")\n",
    "    answer_gt = (\n",
    "        f\"PIVOT_ID={pivot_marker}. The root cause is {pivot_marker}, because its composite severity is maximal. \"\n",
    "        f\"Prerequisite chain: {true_setup_markers[0]} -> {true_setup_markers[1]} -> {true_setup_markers[2]} -> {pivot_marker}.\"\n",
    "    )\n",
    "    decoy_answer = (\n",
    "        f\"PIVOT_ID={decoy_marker}. A plausible but wrong chain is \"\n",
    "        f\"{decoy_setup_markers[0]} -> {decoy_setup_markers[1]} -> {decoy_marker}.\"\n",
    "    )\n",
    "\n",
    "    layout[\"compression_default_drop\"] = actual_drop\n",
    "    layout[\"removed_markers_default\"] = removed_markers\n",
    "\n",
    "    return MirageBenchTask(\n",
    "        task_id=task_id,\n",
    "        category=\"incident\",\n",
    "        full_context=full_context,\n",
    "        compressed_context=compressed_context,\n",
    "        question=question,\n",
    "        pivot_ground_truth=pivot_marker,\n",
    "        answer_ground_truth=answer_gt,\n",
    "        decoy_pivot=decoy_marker,\n",
    "        decoy_answer=decoy_answer,\n",
    "        k=k,\n",
    "        metadata=layout,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_investment_task(task_num: int, k: int = 3, target_words: int = 3600) -> MirageBenchTask:\n",
    "    rng = np.random.default_rng(2000 + task_num)\n",
    "    task_id = f\"B{task_num:02d}\"\n",
    "    n_events = 84\n",
    "\n",
    "    pivot_idx = int(rng.integers(50, 62))\n",
    "    decoy_idx = int(min(n_events - 6, pivot_idx + rng.integers(7, 13)))\n",
    "    true_setup_idx = [pivot_idx - 9, pivot_idx - 6, pivot_idx - 3, pivot_idx - 1]\n",
    "    decoy_setup_idx = [decoy_idx - 5, decoy_idx - 3, decoy_idx - 1]\n",
    "\n",
    "    positions = [\n",
    "        \"NorthRiver Utilities Carry\",\n",
    "        \"Aurelia AI Semiconductor Basket\",\n",
    "        \"Helios Grid Infrastructure\",\n",
    "        \"BlueHarbor Treasury Arbitrage\",\n",
    "        \"Cinder Logistics Credit\",\n",
    "    ]\n",
    "    pivot_position_name = \"Helios Grid Infrastructure\"\n",
    "    decoy_position_name = \"Aurelia AI Semiconductor Basket\"\n",
    "\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    cumulative = {p: 0.0 for p in positions}\n",
    "    pivot_ceiling: Optional[float] = None\n",
    "    pivot_peer_margin = 0.8\n",
    "    pivot_self_margin = 0.1\n",
    "\n",
    "    for i in range(n_events):\n",
    "        marker = f\"{task_id}-E{i+1:03d}\"\n",
    "        wk = f\"Week-{i+1:02d}\"\n",
    "        position = positions[i % len(positions)]\n",
    "\n",
    "        role = \"routine\"\n",
    "        weekly = float(rng.normal(0.8, 0.9))\n",
    "\n",
    "        if i in true_setup_idx:\n",
    "            role = \"setup\"\n",
    "            weekly = float(rng.normal(0.2, 0.2))\n",
    "        elif i in decoy_setup_idx:\n",
    "            role = \"decoy_setup\"\n",
    "            weekly = float(rng.normal(0.6, 0.3))\n",
    "        elif i == pivot_idx:\n",
    "            role = \"pivot\"\n",
    "            position = pivot_position_name\n",
    "            weekly = 5.4\n",
    "        elif i == decoy_idx:\n",
    "            role = \"candidate\"\n",
    "            position = decoy_position_name\n",
    "            weekly = 4.8\n",
    "\n",
    "        cumulative[position] += weekly\n",
    "\n",
    "        if i == pivot_idx:\n",
    "            cumulative[position] = max(cumulative.values()) + 6.0\n",
    "            pivot_ceiling = cumulative[position]\n",
    "        if i == decoy_idx and pivot_ceiling is not None:\n",
    "            cumulative[position] = min(\n",
    "                max(v for k2, v in cumulative.items() if k2 != position) + 1.2,\n",
    "                pivot_ceiling - pivot_peer_margin,\n",
    "            )\n",
    "\n",
    "        # Hard clamp post-pivot cumulative values so later entries cannot overtake the pivot.\n",
    "        if pivot_ceiling is not None and i > pivot_idx:\n",
    "            cap = pivot_ceiling - (pivot_self_margin if position == pivot_position_name else pivot_peer_margin)\n",
    "            cumulative[position] = min(cumulative[position], cap)\n",
    "\n",
    "        cum_val = cumulative[position]\n",
    "        regime = int(rng.integers(1, 6))\n",
    "\n",
    "        note_role = \"pivot\" if role == \"pivot\" else \"decoy\" if role in {\"candidate\", \"decoy_setup\"} else role\n",
    "        note = _long_note(rng, note_role if note_role in {\"setup\", \"pivot\", \"decoy\", \"routine\"} else \"routine\", \"portfolio research\")\n",
    "\n",
    "        line = (\n",
    "            f\"[{marker}] {wk} | Position={position} | WeeklyReturn={weekly:+.2f}% | \"\n",
    "            f\"CumulativeReturn={cum_val:.2f}% | RegimeScore={regime} | {note}\"\n",
    "        )\n",
    "        records.append({\"marker\": marker, \"role\": role, \"line\": line, \"position\": position, \"cum\": cum_val})\n",
    "\n",
    "    pivot_marker = f\"{task_id}-E{pivot_idx+1:03d}\"\n",
    "    decoy_marker = f\"{task_id}-E{decoy_idx+1:03d}\"\n",
    "    true_setup_markers = [f\"{task_id}-E{i+1:03d}\" for i in true_setup_idx]\n",
    "    decoy_setup_markers = [f\"{task_id}-E{i+1:03d}\" for i in decoy_setup_idx]\n",
    "\n",
    "    max_cum_marker = max(records, key=lambda r: float(r[\"cum\"]))[\"marker\"]\n",
    "    if max_cum_marker != pivot_marker:\n",
    "        raise RuntimeError(\n",
    "            f\"Investment task {task_id} invalid: max cumulative marker {max_cum_marker} != pivot {pivot_marker}.\"\n",
    "        )\n",
    "\n",
    "    pivot_position = next(r[\"position\"] for r in records if r[\"marker\"] == pivot_marker)\n",
    "    decoy_position = next(r[\"position\"] for r in records if r[\"marker\"] == decoy_marker)\n",
    "\n",
    "    preamble = (\n",
    "        f\"Investment Committee Timeline {task_id}\\n\"\n",
    "        \"The portfolio diary contains noisy market commentary and low-salience condition markers.\\n\"\n",
    "        \"Interpretation rule: anchor analysis must follow the highest cumulative-return position.\"\n",
    "    )\n",
    "    rule_block = \"Rule reminder: anchor position = entry with max(CumulativeReturn) in the full timeline.\"\n",
    "\n",
    "    full_context, spans, appendix_text = _render_context(\n",
    "        preamble=preamble,\n",
    "        records=records,\n",
    "        rule_block=rule_block,\n",
    "        appendix_target_words=target_words,\n",
    "        rng=rng,\n",
    "    )\n",
    "\n",
    "    layout = {\n",
    "        \"preamble\": preamble,\n",
    "        \"records\": records,\n",
    "        \"rule_block\": rule_block,\n",
    "        \"appendix_text\": appendix_text,\n",
    "        \"spans\": spans,\n",
    "        \"protected_markers\": [pivot_marker, decoy_marker, *decoy_setup_markers],\n",
    "        \"candidate_markers\": [pivot_marker, decoy_marker],\n",
    "        \"candidate_requirements\": {\n",
    "            pivot_marker: true_setup_markers,\n",
    "            decoy_marker: decoy_setup_markers,\n",
    "        },\n",
    "        \"pivot_setup_markers\": true_setup_markers,\n",
    "        \"decoy_setup_markers\": decoy_setup_markers,\n",
    "        \"pivot_position\": pivot_position,\n",
    "        \"decoy_position\": decoy_position,\n",
    "    }\n",
    "\n",
    "    compressed_context, actual_drop, removed_markers = _compress_records_to_target(layout, target_drop_fraction=0.50, rng=rng)\n",
    "\n",
    "    question = _build_question(\"investment\")\n",
    "    answer_gt = (\n",
    "        f\"PIVOT_ID={pivot_marker}. Anchor position is {pivot_position}. \"\n",
    "        f\"Prerequisite market conditions are encoded in {true_setup_markers[0]}, {true_setup_markers[1]}, {true_setup_markers[2]} before {pivot_marker}.\"\n",
    "    )\n",
    "    decoy_answer = (\n",
    "        f\"PIVOT_ID={decoy_marker}. A coherent but wrong narrative centers {decoy_position} and cites \"\n",
    "        f\"{decoy_setup_markers[0]}, {decoy_setup_markers[1]} as enabling conditions.\"\n",
    "    )\n",
    "\n",
    "    layout[\"compression_default_drop\"] = actual_drop\n",
    "    layout[\"removed_markers_default\"] = removed_markers\n",
    "\n",
    "    return MirageBenchTask(\n",
    "        task_id=task_id,\n",
    "        category=\"investment\",\n",
    "        full_context=full_context,\n",
    "        compressed_context=compressed_context,\n",
    "        question=question,\n",
    "        pivot_ground_truth=pivot_marker,\n",
    "        answer_ground_truth=answer_gt,\n",
    "        decoy_pivot=decoy_marker,\n",
    "        decoy_answer=decoy_answer,\n",
    "        k=k,\n",
    "        metadata=layout,\n",
    "    )\n",
    "\n",
    "def build_narrative_task(task_num: int, k: int = 3, target_words: int = 3600) -> MirageBenchTask:\n",
    "    rng = np.random.default_rng(3000 + task_num)\n",
    "    task_id = f\"C{task_num:02d}\"\n",
    "    n_events = 80\n",
    "\n",
    "    pivot_idx = int(rng.integers(48, 60))\n",
    "    decoy_idx = int(min(n_events - 6, pivot_idx + rng.integers(8, 14)))\n",
    "    true_setup_idx = [pivot_idx - 10, pivot_idx - 7, pivot_idx - 4, pivot_idx - 1]\n",
    "    decoy_setup_idx = [decoy_idx - 6, decoy_idx - 3, decoy_idx - 1]\n",
    "\n",
    "    characters = [\"Mira\", \"Jonas\", \"Elio\", \"Sana\", \"Iris\", \"Cato\"]\n",
    "    places = [\"market ward\", \"canal archive\", \"north gate\", \"assembly atrium\", \"river embankment\"]\n",
    "\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for i in range(n_events):\n",
    "        marker = f\"{task_id}-E{i+1:03d}\"\n",
    "        scene = f\"Scene-{i+1:02d}\"\n",
    "        actor = characters[i % len(characters)]\n",
    "        place = places[i % len(places)]\n",
    "\n",
    "        role = \"routine\"\n",
    "        consequence = int(rng.integers(2, 12))\n",
    "        action = \"exchanged routine updates\"\n",
    "\n",
    "        if i in true_setup_idx:\n",
    "            role = \"setup\"\n",
    "            consequence = int(rng.integers(1, 4))\n",
    "            action = \"shared mundane logistical details\"\n",
    "        elif i in decoy_setup_idx:\n",
    "            role = \"decoy_setup\"\n",
    "            consequence = int(rng.integers(3, 7))\n",
    "            action = \"staged a visible confrontation\"\n",
    "        elif i == pivot_idx:\n",
    "            role = \"pivot\"\n",
    "            consequence = 24\n",
    "            action = \"released the sealed ledger proving council fraud\"\n",
    "        elif i == decoy_idx:\n",
    "            role = \"candidate\"\n",
    "            consequence = 19\n",
    "            action = \"challenged the council in a public square\"\n",
    "\n",
    "        note_role = \"pivot\" if role == \"pivot\" else \"decoy\" if role in {\"candidate\", \"decoy_setup\"} else role\n",
    "        note = _long_note(rng, note_role if note_role in {\"setup\", \"pivot\", \"decoy\", \"routine\"} else \"routine\", \"character dynamics\")\n",
    "\n",
    "        line = (\n",
    "            f\"[{marker}] {scene} | Actor={actor} | Location={place} | Action={action} | \"\n",
    "            f\"ConsequenceScore={consequence} | {note}\"\n",
    "        )\n",
    "        records.append({\"marker\": marker, \"role\": role, \"line\": line, \"actor\": actor, \"consequence\": consequence})\n",
    "\n",
    "    pivot_marker = f\"{task_id}-E{pivot_idx+1:03d}\"\n",
    "    decoy_marker = f\"{task_id}-E{decoy_idx+1:03d}\"\n",
    "    true_setup_markers = [f\"{task_id}-E{i+1:03d}\" for i in true_setup_idx]\n",
    "    decoy_setup_markers = [f\"{task_id}-E{i+1:03d}\" for i in decoy_setup_idx]\n",
    "\n",
    "    pivot_actor = next(r[\"actor\"] for r in records if r[\"marker\"] == pivot_marker)\n",
    "    decoy_actor = next(r[\"actor\"] for r in records if r[\"marker\"] == decoy_marker)\n",
    "\n",
    "    preamble = (\n",
    "        f\"Narrative Consequence Ledger {task_id}\\n\"\n",
    "        \"This story timeline mixes dramatic beats and mundane setup scenes.\\n\"\n",
    "        \"Interpretation rule: turning point is the action with highest consequence score across the full story.\"\n",
    "    )\n",
    "    rule_block = \"Rule reminder: turning point = argmax ConsequenceScore over all scenes.\"\n",
    "\n",
    "    full_context, spans, appendix_text = _render_context(\n",
    "        preamble=preamble,\n",
    "        records=records,\n",
    "        rule_block=rule_block,\n",
    "        appendix_target_words=target_words,\n",
    "        rng=rng,\n",
    "    )\n",
    "\n",
    "    layout = {\n",
    "        \"preamble\": preamble,\n",
    "        \"records\": records,\n",
    "        \"rule_block\": rule_block,\n",
    "        \"appendix_text\": appendix_text,\n",
    "        \"spans\": spans,\n",
    "        \"protected_markers\": [pivot_marker, decoy_marker, *decoy_setup_markers],\n",
    "        \"candidate_markers\": [pivot_marker, decoy_marker],\n",
    "        \"candidate_requirements\": {\n",
    "            pivot_marker: true_setup_markers,\n",
    "            decoy_marker: decoy_setup_markers,\n",
    "        },\n",
    "        \"pivot_setup_markers\": true_setup_markers,\n",
    "        \"decoy_setup_markers\": decoy_setup_markers,\n",
    "        \"pivot_actor\": pivot_actor,\n",
    "        \"decoy_actor\": decoy_actor,\n",
    "    }\n",
    "\n",
    "    compressed_context, actual_drop, removed_markers = _compress_records_to_target(layout, target_drop_fraction=0.50, rng=rng)\n",
    "\n",
    "    question = _build_question(\"narrative\")\n",
    "    answer_gt = (\n",
    "        f\"PIVOT_ID={pivot_marker}. Turning point is {pivot_marker} by {pivot_actor}. \"\n",
    "        f\"Enabling setup beats are {true_setup_markers[0]}, {true_setup_markers[1]}, and {true_setup_markers[2]}.\"\n",
    "    )\n",
    "    decoy_answer = (\n",
    "        f\"PIVOT_ID={decoy_marker}. A plausible but wrong reading centers {decoy_actor}'s action at {decoy_marker}, \"\n",
    "        f\"supported by {decoy_setup_markers[0]} and {decoy_setup_markers[1]}.\"\n",
    "    )\n",
    "\n",
    "    layout[\"compression_default_drop\"] = actual_drop\n",
    "    layout[\"removed_markers_default\"] = removed_markers\n",
    "\n",
    "    return MirageBenchTask(\n",
    "        task_id=task_id,\n",
    "        category=\"narrative\",\n",
    "        full_context=full_context,\n",
    "        compressed_context=compressed_context,\n",
    "        question=question,\n",
    "        pivot_ground_truth=pivot_marker,\n",
    "        answer_ground_truth=answer_gt,\n",
    "        decoy_pivot=decoy_marker,\n",
    "        decoy_answer=decoy_answer,\n",
    "        k=k,\n",
    "        metadata=layout,\n",
    "    )\n",
    "\n",
    "\n",
    "def _validate_investment_ground_truth(tasks: Sequence[MirageBenchTask]) -> None:\n",
    "    bad: List[str] = []\n",
    "    for task in tasks:\n",
    "        if task.category != \"investment\":\n",
    "            continue\n",
    "        records = task.metadata.get(\"records\", [])\n",
    "        if not records:\n",
    "            bad.append(f\"{task.task_id}: missing records\")\n",
    "            continue\n",
    "        max_marker = max(records, key=lambda r: float(r.get(\"cum\", float(\"-inf\"))))[\"marker\"]\n",
    "        if max_marker != task.pivot_ground_truth:\n",
    "            bad.append(\n",
    "                f\"{task.task_id}: pivot_ground_truth={task.pivot_ground_truth}, max_cum_marker={max_marker}\"\n",
    "            )\n",
    "    if bad:\n",
    "        raise RuntimeError(\"Investment ground-truth validation failed: \" + \"; \".join(bad))\n",
    "\n",
    "\n",
    "def build_miragebench_v01() -> List[MirageBenchTask]:\n",
    "    tasks: List[MirageBenchTask] = []\n",
    "    for i in range(1, 5):\n",
    "        tasks.append(build_incident_task(i))\n",
    "    for i in range(1, 5):\n",
    "        tasks.append(build_investment_task(i))\n",
    "    for i in range(1, 5):\n",
    "        tasks.append(build_narrative_task(i))\n",
    "    _validate_investment_ground_truth(tasks)\n",
    "    return tasks\n",
    "\n",
    "\n",
    "def render_compressed_variant(task: MirageBenchTask, drop_fraction: float, seed: int = 0) -> str:\n",
    "    layout = task.metadata\n",
    "    rng = np.random.default_rng(seed + int(drop_fraction * 1000))\n",
    "    compressed_text, _, _ = _compress_records_to_target(layout, target_drop_fraction=drop_fraction, rng=rng)\n",
    "    return compressed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Build MirageBench task suite\n",
    "miragebench_tasks = build_miragebench_v01()\n",
    "\n",
    "stats_rows = []\n",
    "for task in miragebench_tasks:\n",
    "    full_words = len(task.full_context.split())\n",
    "    comp_words = len(task.compressed_context.split())\n",
    "    stats_rows.append(\n",
    "        {\n",
    "            \"task_id\": task.task_id,\n",
    "            \"category\": task.category,\n",
    "            \"full_words\": full_words,\n",
    "            \"compressed_words\": comp_words,\n",
    "            \"drop_pct\": round(100 * (1 - comp_words / max(1, full_words)), 2),\n",
    "            \"pivot_gt\": task.pivot_ground_truth,\n",
    "            \"decoy\": task.decoy_pivot,\n",
    "        }\n",
    "    )\n",
    "\n",
    "TASK_STATS_DF = pd.DataFrame(stats_rows)\n",
    "TASK_STATS_DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Quick verification: counts per category and compression range\n",
    "print(TASK_STATS_DF.groupby(\"category\").size())\n",
    "print()\n",
    "print(\"Drop % summary:\")\n",
    "print(TASK_STATS_DF[\"drop_pct\"].describe())\n",
    "\n",
    "TASK_STATS_DF.to_csv(RAW_DIR / \"miragebench_task_stats.csv\", index=False)\n",
    "print(\"Saved:\", RAW_DIR / \"miragebench_task_stats.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.6 Optional: inspect one full + compressed pair\n",
    "preview_task = miragebench_tasks[0]\n",
    "print(\"Task:\", preview_task.task_id, \"Category:\", preview_task.category)\n",
    "print(\"\\n--- Question ---\\n\")\n",
    "print(preview_task.question)\n",
    "print(\"\\n--- Ground truth answer ---\\n\")\n",
    "print(preview_task.answer_ground_truth)\n",
    "print(\"\\n--- Decoy answer ---\\n\")\n",
    "print(preview_task.decoy_answer)\n",
    "print(\"\\n--- Full context excerpt ---\\n\")\n",
    "print(preview_task.full_context[:3000])\n",
    "print(\"\\n[...truncated...]\\n\")\n",
    "print(\"\\n--- Compressed context excerpt ---\\n\")\n",
    "print(preview_task.compressed_context[:2200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Black-Box Evaluation\n",
    "\n",
    "This section runs each task in two conditions:\n",
    "\n",
    "- `full_context + question`\n",
    "- `compressed_context + question`\n",
    "\n",
    "Metrics:\n",
    "\n",
    "- **Raw validity** (coherence / answerability)\n",
    "- **Pivot preservation** (same pivot as full-context answer)\n",
    "- **Semantic regret** (distance from full answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Model adapters (HF local + API placeholders)\n",
    "\n",
    "MODEL_SPECS = {\n",
    "    \"llama-3.1-8b-instruct\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"mistral-7b-instruct-v0.3\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \"qwen2.5-7b-instruct\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "}\n",
    "\n",
    "\n",
    "def make_prompt(context: str, question: str) -> str:\n",
    "    return (\n",
    "        \"You are a precise analyst. Follow the scoring rule in the prompt exactly.\\n\\n\"\n",
    "        + context.strip()\n",
    "        + \"\\n\\nQuestion:\\n\"\n",
    "        + question.strip()\n",
    "        + \"\\n\\nAnswer:\" \n",
    "    )\n",
    "\n",
    "\n",
    "def load_hf_generator(model_name: str, max_new_tokens: int = 220):\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_name,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    def _generate(prompt: str) -> str:\n",
    "        out = pipe(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            return_full_text=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        return out[0][\"generated_text\"].strip()\n",
    "\n",
    "    return _generate\n",
    "\n",
    "\n",
    "def get_openai_generator(model: str = \"gpt-4o-mini\", max_output_tokens: int = 280):\n",
    "    from openai import OpenAI\n",
    "\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY is not set.\")\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    def _generate(prompt: str) -> str:\n",
    "        resp = client.responses.create(\n",
    "            model=model,\n",
    "            input=prompt,\n",
    "            max_output_tokens=max_output_tokens,\n",
    "        )\n",
    "        return (resp.output_text or \"\").strip()\n",
    "\n",
    "    return _generate\n",
    "\n",
    "\n",
    "def get_anthropic_generator(model: str = \"claude-3-5-sonnet-latest\", max_tokens: int = 280):\n",
    "    import anthropic\n",
    "\n",
    "    api_key = os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"ANTHROPIC_API_KEY is not set.\")\n",
    "\n",
    "    client = anthropic.Anthropic(api_key=api_key)\n",
    "\n",
    "    def _generate(prompt: str) -> str:\n",
    "        msg = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=max_tokens,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        chunks = []\n",
    "        for block in msg.content:\n",
    "            if hasattr(block, \"text\"):\n",
    "                chunks.append(block.text)\n",
    "        return \"\".join(chunks).strip()\n",
    "\n",
    "    return _generate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Metrics: pivot extraction, raw validity, semantic regret\n",
    "\n",
    "PIVOT_REGEX = re.compile(r\"PIVOT_ID\\s*=\\s*([A-Z]\\d{2}-E\\d{3})\")\n",
    "MARKER_REGEX = re.compile(r\"([A-Z]\\d{2}-E\\d{3})\")\n",
    "\n",
    "\n",
    "def extract_pivot_id(text: str, fallback_candidates: Optional[List[str]] = None) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    m = PIVOT_REGEX.search(text)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "\n",
    "    markers = MARKER_REGEX.findall(text)\n",
    "    if markers:\n",
    "        if fallback_candidates:\n",
    "            for c in fallback_candidates:\n",
    "                if c in markers:\n",
    "                    return c\n",
    "        return markers[0]\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def raw_validity_score(answer: str, task: MirageBenchTask) -> float:\n",
    "    if not answer or not answer.strip():\n",
    "        return 0.0\n",
    "\n",
    "    words = answer.split()\n",
    "    marker_hits = len(set(MARKER_REGEX.findall(answer)))\n",
    "    has_causal_language = any(\n",
    "        kw in answer.lower()\n",
    "        for kw in [\"because\", \"led\", \"enabled\", \"prerequisite\", \"therefore\", \"causal\", \"preceded\"]\n",
    "    )\n",
    "\n",
    "    score = 0.0\n",
    "    score += 0.35 if len(words) >= 45 else (0.2 if len(words) >= 20 else 0.0)\n",
    "    score += 0.3 if extract_pivot_id(answer, [task.pivot_ground_truth, task.decoy_pivot]) else 0.0\n",
    "    score += min(0.25, 0.08 * marker_hits)\n",
    "    score += 0.1 if has_causal_language else 0.0\n",
    "    return float(min(1.0, score))\n",
    "\n",
    "\n",
    "_semantic_embedder = None\n",
    "\n",
    "\n",
    "def _get_semantic_embedder():\n",
    "    global _semantic_embedder\n",
    "    if _semantic_embedder is not None:\n",
    "        return _semantic_embedder\n",
    "\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        _semantic_embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    except Exception as exc:\n",
    "        warnings.warn(f\"SentenceTransformer unavailable; falling back to TF-IDF semantic proxy. Error: {exc}\")\n",
    "        _semantic_embedder = None\n",
    "    return _semantic_embedder\n",
    "\n",
    "\n",
    "def semantic_regret(full_answer: str, compressed_answer: str) -> float:\n",
    "    full_answer = (full_answer or \"\").strip()\n",
    "    compressed_answer = (compressed_answer or \"\").strip()\n",
    "    if not full_answer and not compressed_answer:\n",
    "        return 0.0\n",
    "    if not full_answer or not compressed_answer:\n",
    "        return 1.0\n",
    "\n",
    "    embedder = _get_semantic_embedder()\n",
    "    if embedder is not None:\n",
    "        embs = embedder.encode([full_answer, compressed_answer], normalize_embeddings=True)\n",
    "        sim = float(np.dot(embs[0], embs[1]))\n",
    "    else:\n",
    "        vec = TfidfVectorizer(min_df=1, ngram_range=(1, 2))\n",
    "        X = vec.fit_transform([full_answer, compressed_answer]).toarray()\n",
    "        sim = 1.0 - float(cosine(X[0], X[1]))\n",
    "        if not np.isfinite(sim):\n",
    "            sim = 0.0\n",
    "\n",
    "    sim = max(-1.0, min(1.0, sim))\n",
    "    regret = 1.0 - ((sim + 1.0) / 2.0)\n",
    "    return float(np.clip(regret, 0.0, 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Evaluation harness\n",
    "\n",
    "def classify_pivot_outcome(task: MirageBenchTask, full_pivot: str, compressed_pivot: str) -> str:\n",
    "    full_correct = bool(full_pivot and full_pivot == task.pivot_ground_truth)\n",
    "    comp_correct = bool(compressed_pivot and compressed_pivot == task.pivot_ground_truth)\n",
    "\n",
    "    if not full_pivot or not compressed_pivot:\n",
    "        return \"unresolved\"\n",
    "    if full_pivot == compressed_pivot:\n",
    "        return \"stable_correct\" if full_correct else \"stable_wrong\"\n",
    "    if full_correct and not comp_correct:\n",
    "        return \"true_mirage\"\n",
    "    if (not full_correct) and comp_correct:\n",
    "        return \"rescue\"\n",
    "    if (not full_correct) and (not comp_correct):\n",
    "        return \"instability\"\n",
    "    return \"other\"\n",
    "\n",
    "\n",
    "def evaluate_task_pair(\n",
    "    task: MirageBenchTask,\n",
    "    model_name: str,\n",
    "    generate_fn: Callable[[str], str],\n",
    "    compression_level: float,\n",
    "    full_answer_override: Optional[str] = None,\n",
    ") -> MirageBenchResult:\n",
    "    full_prompt = make_prompt(task.full_context, task.question)\n",
    "    comp_context = render_compressed_variant(task, drop_fraction=compression_level, seed=SEED)\n",
    "    comp_prompt = make_prompt(comp_context, task.question)\n",
    "\n",
    "    full_answer = full_answer_override if full_answer_override is not None else generate_fn(full_prompt)\n",
    "    compressed_answer = generate_fn(comp_prompt)\n",
    "\n",
    "    full_pivot = extract_pivot_id(full_answer, [task.pivot_ground_truth, task.decoy_pivot])\n",
    "    comp_pivot = extract_pivot_id(compressed_answer, [task.pivot_ground_truth, task.decoy_pivot])\n",
    "\n",
    "    raw_validity_full = raw_validity_score(full_answer, task)\n",
    "    raw_validity_compressed = raw_validity_score(compressed_answer, task)\n",
    "    pivot_outcome = classify_pivot_outcome(task, full_pivot, comp_pivot)\n",
    "    high_validity = raw_validity_compressed >= 0.70\n",
    "\n",
    "    result = MirageBenchResult(\n",
    "        task_id=task.task_id,\n",
    "        model_name=model_name,\n",
    "        full_answer=full_answer,\n",
    "        compressed_answer=compressed_answer,\n",
    "        raw_validity=raw_validity_compressed,\n",
    "        pivot_preserved=bool(full_pivot and comp_pivot and full_pivot == comp_pivot),\n",
    "        semantic_regret=semantic_regret(full_answer, compressed_answer),\n",
    "        compression_level=float(compression_level),\n",
    "        category=task.category,\n",
    "        full_pivot=full_pivot,\n",
    "        compressed_pivot=comp_pivot,\n",
    "        raw_validity_full=raw_validity_full,\n",
    "        raw_validity_compressed=raw_validity_compressed,\n",
    "        full_pivot_matches_ground_truth=bool(full_pivot == task.pivot_ground_truth),\n",
    "        compressed_pivot_matches_ground_truth=bool(comp_pivot == task.pivot_ground_truth),\n",
    "        pivot_outcome=pivot_outcome,\n",
    "        high_validity_flag=bool(high_validity),\n",
    "        true_mirage_flag=bool((pivot_outcome == \"true_mirage\") and high_validity),\n",
    "        rescue_flag=bool((pivot_outcome == \"rescue\") and high_validity),\n",
    "        instability_flag=bool((pivot_outcome == \"instability\") and high_validity),\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_blackbox_eval(\n",
    "    tasks: List[MirageBenchTask],\n",
    "    model_generators: Dict[str, Callable[[str], str]],\n",
    "    compression_levels: Sequence[float] = (0.4, 0.5, 0.6),\n",
    "    max_tasks_per_model: Optional[int] = None,\n",
    ") -> pd.DataFrame:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for model_name, gen_fn in model_generators.items():\n",
    "        sub_tasks = tasks if max_tasks_per_model is None else tasks[:max_tasks_per_model]\n",
    "        for task in tqdm(sub_tasks, desc=f\"Evaluating {model_name}\"):\n",
    "            full_prompt = make_prompt(task.full_context, task.question)\n",
    "            try:\n",
    "                # Generate once per (task, model); reuse for all compression levels.\n",
    "                full_answer = gen_fn(full_prompt)\n",
    "                full_pivot = extract_pivot_id(full_answer, [task.pivot_ground_truth, task.decoy_pivot])\n",
    "                raw_validity_full = raw_validity_score(full_answer, task)\n",
    "            except Exception as exc:\n",
    "                for lvl in compression_levels:\n",
    "                    rows.append(\n",
    "                        {\n",
    "                            \"task_id\": task.task_id,\n",
    "                            \"model_name\": model_name,\n",
    "                            \"compression_level\": lvl,\n",
    "                            \"category\": task.category,\n",
    "                            \"error\": str(exc),\n",
    "                        }\n",
    "                    )\n",
    "                continue\n",
    "\n",
    "            for lvl in compression_levels:\n",
    "                try:\n",
    "                    comp_context = render_compressed_variant(task, drop_fraction=lvl, seed=SEED)\n",
    "                    comp_prompt = make_prompt(comp_context, task.question)\n",
    "                    compressed_answer = gen_fn(comp_prompt)\n",
    "\n",
    "                    compressed_pivot = extract_pivot_id(\n",
    "                        compressed_answer,\n",
    "                        [task.pivot_ground_truth, task.decoy_pivot],\n",
    "                    )\n",
    "                    raw_validity_compressed = raw_validity_score(compressed_answer, task)\n",
    "\n",
    "                    pivot_outcome = classify_pivot_outcome(task, full_pivot, compressed_pivot)\n",
    "                    high_validity = int(raw_validity_compressed >= 0.70)\n",
    "\n",
    "                    row = {\n",
    "                        \"task_id\": task.task_id,\n",
    "                        \"model_name\": model_name,\n",
    "                        \"full_answer\": full_answer,\n",
    "                        \"compressed_answer\": compressed_answer,\n",
    "                        \"raw_validity\": raw_validity_compressed,\n",
    "                        \"raw_validity_full\": raw_validity_full,\n",
    "                        \"raw_validity_compressed\": raw_validity_compressed,\n",
    "                        \"pivot_preserved\": int(bool(full_pivot and compressed_pivot and full_pivot == compressed_pivot)),\n",
    "                        \"semantic_regret\": semantic_regret(full_answer, compressed_answer),\n",
    "                        \"compression_level\": float(lvl),\n",
    "                        \"category\": task.category,\n",
    "                        \"full_pivot\": full_pivot,\n",
    "                        \"compressed_pivot\": compressed_pivot,\n",
    "                        \"full_pivot_matches_ground_truth\": int(full_pivot == task.pivot_ground_truth),\n",
    "                        \"pivot_matches_ground_truth\": int(compressed_pivot == task.pivot_ground_truth),\n",
    "                        \"pivot_outcome\": pivot_outcome,\n",
    "                        \"high_validity_flag\": high_validity,\n",
    "                        \"true_mirage_flag\": int((pivot_outcome == \"true_mirage\") and high_validity),\n",
    "                        \"rescue_flag\": int((pivot_outcome == \"rescue\") and high_validity),\n",
    "                        \"instability_flag\": int((pivot_outcome == \"instability\") and high_validity),\n",
    "                    }\n",
    "                    # Backward-compatible legacy field.\n",
    "                    row[\"mirage_flag\"] = row[\"true_mirage_flag\"]\n",
    "                    rows.append(row)\n",
    "                except Exception as exc:\n",
    "                    rows.append(\n",
    "                        {\n",
    "                            \"task_id\": task.task_id,\n",
    "                            \"model_name\": model_name,\n",
    "                            \"compression_level\": lvl,\n",
    "                            \"category\": task.category,\n",
    "                            \"error\": str(exc),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Configure models and run black-box experiments\n",
    "# NOTE: Loading all open models can take significant VRAM/time.\n",
    "# Start with 1 model + 3 tasks, then scale up.\n",
    "\n",
    "RUN_HF_MODELS = bool(globals().get(\"RUN_HF_MODELS\", False))\n",
    "RUN_API_MODELS = bool(globals().get(\"RUN_API_MODELS\", False))\n",
    "MAX_TASKS_PER_MODEL = globals().get(\"MAX_TASKS_PER_MODEL\", 3)\n",
    "COMPRESSION_LEVELS = list(globals().get(\"COMPRESSION_LEVELS\", [0.4, 0.5, 0.6]))\n",
    "HF_MODELS_TO_RUN = list(globals().get(\"HF_MODELS_TO_RUN\", MODEL_SPECS.keys()))\n",
    "\n",
    "selected_model_specs = {\n",
    "    short_name: repo_name\n",
    "    for short_name, repo_name in MODEL_SPECS.items()\n",
    "    if short_name in HF_MODELS_TO_RUN\n",
    "}\n",
    "if not selected_model_specs:\n",
    "    selected_model_specs = MODEL_SPECS.copy()\n",
    "\n",
    "model_generators: Dict[str, Callable[[str], str]] = {}\n",
    "\n",
    "if RUN_HF_MODELS:\n",
    "    for short_name, repo_name in selected_model_specs.items():\n",
    "        try:\n",
    "            print(f\"Loading HF model: {repo_name}\")\n",
    "            model_generators[short_name] = load_hf_generator(repo_name)\n",
    "        except Exception as exc:\n",
    "            print(f\"Skipping {repo_name}: {exc}\")\n",
    "\n",
    "if RUN_API_MODELS:\n",
    "    try:\n",
    "        model_generators[\"gpt-4o-mini\"] = get_openai_generator(model=\"gpt-4o-mini\")\n",
    "    except Exception as exc:\n",
    "        print(\"OpenAI setup not ready:\", exc)\n",
    "\n",
    "    try:\n",
    "        model_generators[\"claude-sonnet\"] = get_anthropic_generator(model=\"claude-3-5-sonnet-latest\")\n",
    "    except Exception as exc:\n",
    "        print(\"Anthropic setup not ready:\", exc)\n",
    "\n",
    "if model_generators:\n",
    "    blackbox_results_df = run_blackbox_eval(\n",
    "        tasks=miragebench_tasks,\n",
    "        model_generators=model_generators,\n",
    "        compression_levels=COMPRESSION_LEVELS,\n",
    "        max_tasks_per_model=MAX_TASKS_PER_MODEL,\n",
    "    )\n",
    "    blackbox_results_df.to_csv(RAW_DIR / \"miragebench_blackbox_results.csv\", index=False)\n",
    "    print(\"Saved:\", RAW_DIR / \"miragebench_blackbox_results.csv\")\n",
    "else:\n",
    "    blackbox_results_df = pd.DataFrame()\n",
    "    print(\"No model generators configured. Set RUN_HF_MODELS or RUN_API_MODELS to True.\")\n",
    "\n",
    "blackbox_results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 Aggregate metrics and mirage plot\n",
    "if blackbox_results_df.empty:\n",
    "    print(\"No black-box results yet.\")\n",
    "else:\n",
    "    usable = blackbox_results_df[~blackbox_results_df.columns.isin([\"error\"])].copy()\n",
    "    if \"error\" in blackbox_results_df.columns:\n",
    "        usable = blackbox_results_df[blackbox_results_df[\"error\"].isna()] if blackbox_results_df[\"error\"].notna().any() else blackbox_results_df.copy()\n",
    "\n",
    "    numeric_cols = [\"raw_validity\", \"semantic_regret\", \"pivot_preserved\", \"pivot_matches_ground_truth\", \"mirage_flag\"]\n",
    "    for c in numeric_cols:\n",
    "        if c in usable.columns:\n",
    "            usable[c] = pd.to_numeric(usable[c], errors=\"coerce\")\n",
    "\n",
    "    summary = (\n",
    "        usable.groupby([\"model_name\", \"category\", \"compression_level\"], as_index=False)\n",
    "        .agg(\n",
    "            raw_validity=(\"raw_validity\", \"mean\"),\n",
    "            pivot_preservation=(\"pivot_preserved\", \"mean\"),\n",
    "            semantic_regret=(\"semantic_regret\", \"mean\"),\n",
    "            mirage_rate=(\"mirage_flag\", \"mean\"),\n",
    "            n=(\"task_id\", \"count\"),\n",
    "        )\n",
    "        .sort_values([\"model_name\", \"category\", \"compression_level\"])\n",
    "    )\n",
    "\n",
    "    display(summary)\n",
    "    summary.to_csv(RAW_DIR / \"miragebench_blackbox_summary.csv\", index=False)\n",
    "\n",
    "    # Mirage plot: raw validity vs pivot preservation across compression levels.\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    for model_name, sub in summary.groupby(\"model_name\"):\n",
    "        xy = sub.groupby(\"compression_level\", as_index=False).agg(\n",
    "            raw_validity=(\"raw_validity\", \"mean\"),\n",
    "            pivot_preservation=(\"pivot_preservation\", \"mean\"),\n",
    "        )\n",
    "        ax.plot(xy[\"raw_validity\"], xy[\"pivot_preservation\"], marker=\"o\", label=model_name)\n",
    "        for _, row in xy.iterrows():\n",
    "            ax.text(row[\"raw_validity\"] + 0.003, row[\"pivot_preservation\"] + 0.003, f\"c={row['compression_level']:.1f}\", fontsize=8)\n",
    "\n",
    "    ax.set_xlabel(\"Raw validity (higher is better)\")\n",
    "    ax.set_ylabel(\"Pivot preservation (higher is better)\")\n",
    "    ax.set_title(\"Mirage Plot: Validity vs Pivot Preservation\")\n",
    "    ax.set_xlim(0, 1.02)\n",
    "    ax.set_ylim(0, 1.02)\n",
    "    ax.legend(loc=\"lower left\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    mirage_plot_path = FIG_DIR / \"mirage_plot_blackbox.png\"\n",
    "    fig.savefig(mirage_plot_path)\n",
    "    plt.show()\n",
    "    print(\"Saved:\", mirage_plot_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 Flag high-validity / low-pivot-preservation mirage cases\n",
    "if blackbox_results_df.empty:\n",
    "    print(\"No black-box results yet.\")\n",
    "else:\n",
    "    tmp = blackbox_results_df.copy()\n",
    "    if \"error\" in tmp.columns:\n",
    "        tmp = tmp[tmp[\"error\"].isna()] if tmp[\"error\"].notna().any() else tmp\n",
    "\n",
    "    tmp[\"raw_validity\"] = pd.to_numeric(tmp.get(\"raw_validity\"), errors=\"coerce\")\n",
    "    tmp[\"pivot_preserved\"] = pd.to_numeric(tmp.get(\"pivot_preserved\"), errors=\"coerce\")\n",
    "\n",
    "    mirage_cases = tmp[(tmp[\"raw_validity\"] >= 0.75) & (tmp[\"pivot_preserved\"] < 0.5)].copy()\n",
    "    mirage_cases = mirage_cases.sort_values([\"model_name\", \"compression_level\", \"raw_validity\"], ascending=[True, True, False])\n",
    "\n",
    "    print(f\"Mirage cases found: {len(mirage_cases)}\")\n",
    "    display(mirage_cases[[\n",
    "        \"task_id\", \"category\", \"model_name\", \"compression_level\", \"raw_validity\", \"pivot_preserved\", \"semantic_regret\", \"full_pivot\", \"compressed_pivot\"\n",
    "    ]].head(30))\n",
    "\n",
    "    mirage_cases.to_csv(RAW_DIR / \"mirage_cases.csv\", index=False)\n",
    "    print(\"Saved:\", RAW_DIR / \"mirage_cases.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. KV-Cache Surgery (GPU)\n",
    "\n",
    "Goal: probe the neural mechanism behind pivot substitution.\n",
    "\n",
    "For a selected task and model:\n",
    "\n",
    "1. Build prefix KV-cache from full context.\n",
    "2. Identify token spans for pivot/setup events.\n",
    "3. Apply eviction strategies:\n",
    "   - random\n",
    "   - attention-based (H2O-style)\n",
    "   - setup-targeted\n",
    "   - contract-guarded\n",
    "4. Regenerate and compare coherence, pivot preservation, and semantic regret.\n",
    "5. Plot attention redistribution and per-layer pivot tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Load model for KV experiments\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "KV_MODEL_NAME = str(globals().get(\"KV_MODEL_NAME\", \"meta-llama/Llama-3.1-8B-Instruct\"))\n",
    "KV_MAX_NEW_TOKENS = int(globals().get(\"KV_MAX_NEW_TOKENS\", 160))\n",
    "AUTO_LOAD_KV_MODEL = bool(globals().get(\"AUTO_LOAD_KV_MODEL\", False))\n",
    "\n",
    "kv_tokenizer = None\n",
    "kv_model = None\n",
    "\n",
    "\n",
    "def load_kv_model(model_name: str = KV_MODEL_NAME):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"eager\",  # explicit for attention/KV introspection\n",
    "    )\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "if AUTO_LOAD_KV_MODEL:\n",
    "    kv_tokenizer, kv_model = load_kv_model(KV_MODEL_NAME)\n",
    "    print(\"Loaded:\", KV_MODEL_NAME)\n",
    "else:\n",
    "    print(\"Set AUTO_LOAD_KV_MODEL=True in the control panel to auto-load KV model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 KV helpers: token mapping, cache pruning, generation from cached prefix\n",
    "\n",
    "def to_legacy_past(past_key_values):\n",
    "    if hasattr(past_key_values, \"to_legacy_cache\"):\n",
    "        return past_key_values.to_legacy_cache()\n",
    "    return past_key_values\n",
    "\n",
    "\n",
    "def make_kv_prompt(task: MirageBenchTask, context: str) -> str:\n",
    "    return make_prompt(context=context, question=task.question)\n",
    "\n",
    "\n",
    "def _tokenize_with_offsets(tokenizer, text: str):\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        return_offsets_mapping=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    offsets = enc[\"offset_mapping\"][0].tolist()\n",
    "    return input_ids, offsets\n",
    "\n",
    "\n",
    "def spans_to_token_positions(offsets: List[Tuple[int, int]], spans: List[Dict[str, Any]], max_token: Optional[int] = None):\n",
    "    event_token_map: Dict[str, List[int]] = {}\n",
    "    role_map: Dict[str, str] = {}\n",
    "\n",
    "    upper = len(offsets) if max_token is None else min(len(offsets), max_token)\n",
    "    for sp in spans:\n",
    "        marker = sp[\"marker\"]\n",
    "        s0, e0 = int(sp[\"start\"]), int(sp[\"end\"])\n",
    "        pos = []\n",
    "        for idx in range(upper):\n",
    "            s, e = offsets[idx]\n",
    "            if e <= s0 or s >= e0:\n",
    "                continue\n",
    "            pos.append(idx)\n",
    "        if pos:\n",
    "            event_token_map[marker] = pos\n",
    "            role_map[marker] = sp.get(\"role\", \"routine\")\n",
    "\n",
    "    return event_token_map, role_map\n",
    "\n",
    "\n",
    "def prune_past_key_values(past_key_values, keep_positions: List[int]):\n",
    "    keep = sorted(set(int(x) for x in keep_positions if x >= 0))\n",
    "    if not keep:\n",
    "        raise ValueError(\"Cannot prune to empty cache; keep_positions is empty.\")\n",
    "\n",
    "    new_past = []\n",
    "    for layer in past_key_values:\n",
    "        key, value = layer[0], layer[1]\n",
    "        keep_idx = torch.tensor(keep, dtype=torch.long, device=key.device)\n",
    "        new_key = key.index_select(2, keep_idx)\n",
    "        new_val = value.index_select(2, keep_idx)\n",
    "\n",
    "        if len(layer) == 2:\n",
    "            new_past.append((new_key, new_val))\n",
    "        else:\n",
    "            new_past.append((new_key, new_val, *layer[2:]))\n",
    "\n",
    "    return tuple(new_past)\n",
    "\n",
    "\n",
    "def decode_from_prefix_cache(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prefix_past,\n",
    "    last_token_id,\n",
    "    max_new_tokens: int = 160,\n",
    "):\n",
    "    # Continue generation given prefix cache and last prompt token.\n",
    "    # Returns generated text + first-step attentions for layer-wise analysis.\n",
    "    past = prefix_past\n",
    "    input_token = last_token_id\n",
    "\n",
    "    generated_ids: List[int] = []\n",
    "    first_step_attn = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(\n",
    "            input_ids=input_token,\n",
    "            past_key_values=past,\n",
    "            use_cache=True,\n",
    "            output_attentions=True,\n",
    "        )\n",
    "        past = to_legacy_past(out.past_key_values)\n",
    "\n",
    "        if out.attentions is not None:\n",
    "            first_step_attn = torch.stack([\n",
    "                att[0, :, 0, :].mean(dim=0).detach().float().cpu()\n",
    "                for att in out.attentions\n",
    "            ])\n",
    "\n",
    "        next_token = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        generated_ids.append(int(next_token.item()))\n",
    "        input_token = next_token\n",
    "\n",
    "        eos = tokenizer.eos_token_id\n",
    "        for _ in range(max_new_tokens - 1):\n",
    "            out = model(\n",
    "                input_ids=input_token,\n",
    "                past_key_values=past,\n",
    "                use_cache=True,\n",
    "                output_attentions=False,\n",
    "            )\n",
    "            past = to_legacy_past(out.past_key_values)\n",
    "            next_token = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            tid = int(next_token.item())\n",
    "            generated_ids.append(tid)\n",
    "            input_token = next_token\n",
    "            if eos is not None and tid == eos:\n",
    "                break\n",
    "\n",
    "    text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    return text.strip(), first_step_attn\n",
    "\n",
    "\n",
    "def run_prompt_with_cache(model, tokenizer, prompt: str, max_new_tokens: int = KV_MAX_NEW_TOKENS):\n",
    "    input_ids, offsets = _tokenize_with_offsets(tokenizer, prompt)\n",
    "    input_ids = input_ids.to(model.device)\n",
    "\n",
    "    if input_ids.shape[1] < 2:\n",
    "        raise ValueError(\"Prompt too short for cache split.\")\n",
    "\n",
    "    prefix_ids = input_ids[:, :-1]\n",
    "    last_token = input_ids[:, -1:]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prefix_out = model(prefix_ids, use_cache=True, output_attentions=False)\n",
    "\n",
    "    prefix_past = to_legacy_past(prefix_out.past_key_values)\n",
    "    text, first_step_attn = decode_from_prefix_cache(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prefix_past=prefix_past,\n",
    "        last_token_id=last_token,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"offsets\": offsets,\n",
    "        \"prefix_len\": int(prefix_ids.shape[1]),\n",
    "        \"prefix_past\": prefix_past,\n",
    "        \"last_token\": last_token,\n",
    "        \"answer\": text,\n",
    "        \"first_step_attn\": first_step_attn,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Eviction strategies (random, attention-H2O, setup-targeted, contract-guarded)\n",
    "\n",
    "def _marker_scores_from_attention(event_token_map: Dict[str, List[int]], token_importance: np.ndarray) -> Dict[str, float]:\n",
    "    scores = {}\n",
    "    for marker, positions in event_token_map.items():\n",
    "        vals = [token_importance[p] for p in positions if p < len(token_importance)]\n",
    "        scores[marker] = float(np.mean(vals)) if vals else 0.0\n",
    "    return scores\n",
    "\n",
    "\n",
    "def _candidate_event_order(\n",
    "    markers: List[str],\n",
    "    role_map: Dict[str, str],\n",
    "    marker_scores: Dict[str, float],\n",
    "    strategy: str,\n",
    "    rng: np.random.Generator,\n",
    ") -> List[str]:\n",
    "    if strategy == \"random\":\n",
    "        out = markers[:]\n",
    "        rng.shuffle(out)\n",
    "        return out\n",
    "\n",
    "    if strategy == \"attention_h2o\":\n",
    "        # H2O-like: evict lowest attention first.\n",
    "        return sorted(markers, key=lambda m: marker_scores.get(m, 0.0))\n",
    "\n",
    "    if strategy == \"setup_targeted\":\n",
    "        role_rank = {\"setup\": 0, \"routine\": 1, \"support\": 2, \"decoy_setup\": 3, \"candidate\": 4, \"pivot\": 5}\n",
    "        return sorted(markers, key=lambda m: (role_rank.get(role_map.get(m, \"routine\"), 2), marker_scores.get(m, 0.0)))\n",
    "\n",
    "    if strategy == \"contract_guarded\":\n",
    "        role_rank = {\"setup\": 0, \"routine\": 1, \"support\": 2, \"decoy_setup\": 3, \"candidate\": 4, \"pivot\": 5}\n",
    "        return sorted(markers, key=lambda m: (role_rank.get(role_map.get(m, \"routine\"), 2), marker_scores.get(m, 0.0)))\n",
    "\n",
    "    raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "\n",
    "def choose_eviction_markers(\n",
    "    task: MirageBenchTask,\n",
    "    event_token_map: Dict[str, List[int]],\n",
    "    role_map: Dict[str, str],\n",
    "    token_importance: np.ndarray,\n",
    "    strategy: str,\n",
    "    eviction_ratio: float,\n",
    "    seed: int = 0,\n",
    ") -> Tuple[List[str], int]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    all_markers = list(event_token_map.keys())\n",
    "    protected = set(task.metadata.get(\"candidate_markers\", []))\n",
    "\n",
    "    removable = [m for m in all_markers if m not in protected]\n",
    "    marker_scores = _marker_scores_from_attention(event_token_map, token_importance)\n",
    "\n",
    "    total_tokens = sum(len(event_token_map[m]) for m in removable)\n",
    "    target_remove = int(round(total_tokens * eviction_ratio))\n",
    "\n",
    "    ordered = _candidate_event_order(removable, role_map, marker_scores, strategy=strategy, rng=rng)\n",
    "\n",
    "    removed: List[str] = []\n",
    "    removed_tokens = 0\n",
    "\n",
    "    requirements: Dict[str, List[str]] = task.metadata.get(\"candidate_requirements\", {})\n",
    "    k = int(task.k)\n",
    "\n",
    "    for marker in ordered:\n",
    "        if removed_tokens >= target_remove:\n",
    "            break\n",
    "\n",
    "        if strategy == \"contract_guarded\":\n",
    "            # Contract: removing marker cannot reduce surviving setup markers below k for any candidate pivot.\n",
    "            violates = False\n",
    "            current_removed = set(removed)\n",
    "            for cand, req_markers in requirements.items():\n",
    "                req_set = set(req_markers)\n",
    "                surviving = len(req_set - current_removed)\n",
    "                if marker in req_set and (surviving - 1) < k:\n",
    "                    violates = True\n",
    "                    break\n",
    "            if violates:\n",
    "                continue\n",
    "\n",
    "        removed.append(marker)\n",
    "        removed_tokens += len(event_token_map[marker])\n",
    "\n",
    "    return removed, target_remove\n",
    "\n",
    "\n",
    "def markers_to_token_positions(markers: List[str], event_token_map: Dict[str, List[int]], max_prefix_len: int) -> List[int]:\n",
    "    positions: List[int] = []\n",
    "    for m in markers:\n",
    "        for p in event_token_map.get(m, []):\n",
    "            if p < max_prefix_len:\n",
    "                positions.append(int(p))\n",
    "    return sorted(set(positions))\n",
    "\n",
    "\n",
    "def kv_strategy_run(\n",
    "    task: MirageBenchTask,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    strategy: str,\n",
    "    eviction_ratio: float = 0.5,\n",
    "    seed: int = 0,\n",
    ") -> Dict[str, Any]:\n",
    "    full_prompt = make_kv_prompt(task, task.full_context)\n",
    "    base = run_prompt_with_cache(model, tokenizer, full_prompt, max_new_tokens=KV_MAX_NEW_TOKENS)\n",
    "\n",
    "    # Token map for context events (use only context token range).\n",
    "    context_ids = tokenizer(task.full_context, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    context_len = int(context_ids.shape[1])\n",
    "\n",
    "    spans = task.metadata.get(\"spans\", [])\n",
    "    event_token_map, role_map = spans_to_token_positions(base[\"offsets\"], spans, max_token=context_len)\n",
    "\n",
    "    if base[\"first_step_attn\"] is None:\n",
    "        raise RuntimeError(\"No attentions returned; make sure model supports output_attentions.\")\n",
    "\n",
    "    token_importance = base[\"first_step_attn\"].mean(dim=0).numpy()\n",
    "    token_importance = token_importance[:context_len]\n",
    "\n",
    "    removed_markers, target_remove = choose_eviction_markers(\n",
    "        task=task,\n",
    "        event_token_map=event_token_map,\n",
    "        role_map=role_map,\n",
    "        token_importance=token_importance,\n",
    "        strategy=strategy,\n",
    "        eviction_ratio=eviction_ratio,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    remove_positions = markers_to_token_positions(removed_markers, event_token_map, max_prefix_len=base[\"prefix_len\"])\n",
    "    keep_positions = [i for i in range(base[\"prefix_len\"]) if i not in set(remove_positions)]\n",
    "\n",
    "    if len(keep_positions) < 8:\n",
    "        raise RuntimeError(\"Too many tokens removed; insufficient cache length remains.\")\n",
    "\n",
    "    pruned_past = prune_past_key_values(base[\"prefix_past\"], keep_positions)\n",
    "\n",
    "    pruned_answer, pruned_attn = decode_from_prefix_cache(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prefix_past=pruned_past,\n",
    "        last_token_id=base[\"last_token\"],\n",
    "        max_new_tokens=KV_MAX_NEW_TOKENS,\n",
    "    )\n",
    "\n",
    "    full_pivot = extract_pivot_id(base[\"answer\"], [task.pivot_ground_truth, task.decoy_pivot])\n",
    "    pruned_pivot = extract_pivot_id(pruned_answer, [task.pivot_ground_truth, task.decoy_pivot])\n",
    "\n",
    "    return {\n",
    "        \"task_id\": task.task_id,\n",
    "        \"strategy\": strategy,\n",
    "        \"eviction_ratio\": eviction_ratio,\n",
    "        \"target_remove_tokens\": target_remove,\n",
    "        \"removed_markers\": removed_markers,\n",
    "        \"removed_tokens\": len(remove_positions),\n",
    "        \"full_answer\": base[\"answer\"],\n",
    "        \"pruned_answer\": pruned_answer,\n",
    "        \"full_pivot\": full_pivot,\n",
    "        \"pruned_pivot\": pruned_pivot,\n",
    "        \"pivot_preserved\": bool(full_pivot and pruned_pivot and full_pivot == pruned_pivot),\n",
    "        \"raw_validity\": raw_validity_score(pruned_answer, task),\n",
    "        \"semantic_regret\": semantic_regret(base[\"answer\"], pruned_answer),\n",
    "        \"full_first_step_attn\": base[\"first_step_attn\"],\n",
    "        \"pruned_first_step_attn\": pruned_attn,\n",
    "        \"event_token_map\": event_token_map,\n",
    "        \"context_len\": context_len,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Run KV-cache surgery on one task\n",
    "RUN_KV_EXPERIMENT = bool(globals().get(\"RUN_KV_EXPERIMENT\", False))\n",
    "KV_TASK_INDEX = int(globals().get(\"KV_TASK_INDEX\", 0))\n",
    "KV_EVICTION_RATIO = float(globals().get(\"KV_EVICTION_RATIO\", 0.5))\n",
    "\n",
    "kv_strategy_results = []\n",
    "\n",
    "if RUN_KV_EXPERIMENT:\n",
    "    if kv_model is None or kv_tokenizer is None:\n",
    "        kv_tokenizer, kv_model = load_kv_model(KV_MODEL_NAME)\n",
    "\n",
    "    task = miragebench_tasks[KV_TASK_INDEX]\n",
    "    strategies = [\"random\", \"attention_h2o\", \"setup_targeted\", \"contract_guarded\"]\n",
    "\n",
    "    for strat in strategies:\n",
    "        print(f\"Running strategy: {strat}\")\n",
    "        try:\n",
    "            out = kv_strategy_run(\n",
    "                task=task,\n",
    "                model=kv_model,\n",
    "                tokenizer=kv_tokenizer,\n",
    "                strategy=strat,\n",
    "                eviction_ratio=KV_EVICTION_RATIO,\n",
    "                seed=SEED,\n",
    "            )\n",
    "            kv_strategy_results.append(out)\n",
    "            print(\n",
    "                f\"  pivot_preserved={out['pivot_preserved']} | raw_validity={out['raw_validity']:.3f} | \"\n",
    "                f\"semantic_regret={out['semantic_regret']:.3f}\"\n",
    "            )\n",
    "        except Exception as exc:\n",
    "            print(f\"  Failed for {strat}: {exc}\")\n",
    "\n",
    "    kv_results_df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"task_id\": r[\"task_id\"],\n",
    "                \"strategy\": r[\"strategy\"],\n",
    "                \"eviction_ratio\": r[\"eviction_ratio\"],\n",
    "                \"removed_tokens\": r[\"removed_tokens\"],\n",
    "                \"pivot_preserved\": r[\"pivot_preserved\"],\n",
    "                \"raw_validity\": r[\"raw_validity\"],\n",
    "                \"semantic_regret\": r[\"semantic_regret\"],\n",
    "                \"full_pivot\": r[\"full_pivot\"],\n",
    "                \"pruned_pivot\": r[\"pruned_pivot\"],\n",
    "            }\n",
    "            for r in kv_strategy_results\n",
    "        ]\n",
    "    )\n",
    "    kv_results_df.to_csv(RAW_DIR / \"kv_surgery_results_single_task.csv\", index=False)\n",
    "    display(kv_results_df)\n",
    "else:\n",
    "    kv_results_df = pd.DataFrame()\n",
    "    print(\"Set RUN_KV_EXPERIMENT=True in the control panel to execute KV-cache surgery.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Attention visualization + per-layer pivot tracking\n",
    "\n",
    "def _downsample_vector(v: np.ndarray, bins: int = 140) -> np.ndarray:\n",
    "    if len(v) <= bins:\n",
    "        return v\n",
    "    edges = np.linspace(0, len(v), bins + 1).astype(int)\n",
    "    out = []\n",
    "    for i in range(bins):\n",
    "        s, e = edges[i], edges[i + 1]\n",
    "        if e <= s:\n",
    "            out.append(0.0)\n",
    "        else:\n",
    "            out.append(float(v[s:e].mean()))\n",
    "    return np.array(out)\n",
    "\n",
    "\n",
    "def _layer_group_attention(first_step_attn: torch.Tensor, positions: List[int]) -> np.ndarray:\n",
    "    if first_step_attn is None:\n",
    "        return np.array([])\n",
    "    if not positions:\n",
    "        return np.zeros(first_step_attn.shape[0], dtype=float)\n",
    "    arr = first_step_attn.numpy()\n",
    "    pos = [p for p in positions if p < arr.shape[1]]\n",
    "    if not pos:\n",
    "        return np.zeros(arr.shape[0], dtype=float)\n",
    "    return arr[:, pos].sum(axis=1)\n",
    "\n",
    "\n",
    "if RUN_KV_EXPERIMENT and kv_strategy_results:\n",
    "    # Compare baseline vs setup-targeted attention redistribution.\n",
    "    baseline = next((r for r in kv_strategy_results if r[\"strategy\"] == \"attention_h2o\"), kv_strategy_results[0])\n",
    "    setup_targeted = next((r for r in kv_strategy_results if r[\"strategy\"] == \"setup_targeted\"), kv_strategy_results[0])\n",
    "\n",
    "    full_vec = baseline[\"full_first_step_attn\"].mean(dim=0).numpy()\n",
    "    if setup_targeted[\"pruned_first_step_attn\"] is not None:\n",
    "        pruned_vec = setup_targeted[\"pruned_first_step_attn\"].mean(dim=0).numpy()\n",
    "    else:\n",
    "        pruned_vec = np.zeros_like(full_vec)\n",
    "\n",
    "    full_ds = _downsample_vector(full_vec, bins=160)\n",
    "    pruned_ds = _downsample_vector(pruned_vec, bins=160)\n",
    "\n",
    "    heat = np.vstack([full_ds, pruned_ds])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 2.8))\n",
    "    im = ax.imshow(heat, aspect=\"auto\", cmap=\"magma\")\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_yticklabels([\"Full\", \"Setup-targeted eviction\"])\n",
    "    ax.set_title(\"First-step attention heatmap (downsampled)\")\n",
    "    ax.set_xlabel(\"Context position bins\")\n",
    "    fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "    fig.tight_layout()\n",
    "    path = FIG_DIR / \"kv_attention_heatmap_full_vs_setup_targeted.png\"\n",
    "    fig.savefig(path)\n",
    "    plt.show()\n",
    "    print(\"Saved:\", path)\n",
    "\n",
    "    # Per-layer pivot/setup tracking (full vs compressed-context baseline).\n",
    "    task = miragebench_tasks[KV_TASK_INDEX]\n",
    "    comp_prompt = make_kv_prompt(task, task.compressed_context)\n",
    "    comp_run = run_prompt_with_cache(kv_model, kv_tokenizer, comp_prompt, max_new_tokens=KV_MAX_NEW_TOKENS)\n",
    "\n",
    "    full_prompt = make_kv_prompt(task, task.full_context)\n",
    "    full_run = run_prompt_with_cache(kv_model, kv_tokenizer, full_prompt, max_new_tokens=KV_MAX_NEW_TOKENS)\n",
    "\n",
    "    full_ids, full_offsets = _tokenize_with_offsets(kv_tokenizer, full_prompt)\n",
    "    comp_ids, comp_offsets = _tokenize_with_offsets(kv_tokenizer, comp_prompt)\n",
    "\n",
    "    full_event_map, _ = spans_to_token_positions(full_offsets, task.metadata.get(\"spans\", []), max_token=full_ids.shape[1])\n",
    "\n",
    "    pivot_positions = full_event_map.get(task.pivot_ground_truth, [])\n",
    "    setup_positions = []\n",
    "    for m in task.metadata.get(\"pivot_setup_markers\", []):\n",
    "        setup_positions.extend(full_event_map.get(m, []))\n",
    "\n",
    "    full_pivot_curve = _layer_group_attention(full_run[\"first_step_attn\"], pivot_positions)\n",
    "    full_setup_curve = _layer_group_attention(full_run[\"first_step_attn\"], setup_positions)\n",
    "\n",
    "    # For compressed context, remap via marker search from spans derived on compressed context rendering.\n",
    "    comp_context_len = kv_tokenizer(task.compressed_context, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "    comp_records = []\n",
    "    kept_markers = set(re.findall(r\"[A-Z]\\d{2}-E\\d{3}\", task.compressed_context))\n",
    "    for sp in task.metadata.get(\"spans\", []):\n",
    "        if sp[\"marker\"] in kept_markers:\n",
    "            comp_records.append(sp)\n",
    "    comp_event_map, _ = spans_to_token_positions(comp_offsets, comp_records, max_token=comp_context_len)\n",
    "\n",
    "    comp_pivot_positions = comp_event_map.get(task.pivot_ground_truth, [])\n",
    "    comp_setup_positions = []\n",
    "    for m in task.metadata.get(\"pivot_setup_markers\", []):\n",
    "        comp_setup_positions.extend(comp_event_map.get(m, []))\n",
    "\n",
    "    comp_pivot_curve = _layer_group_attention(comp_run[\"first_step_attn\"], comp_pivot_positions)\n",
    "    comp_setup_curve = _layer_group_attention(comp_run[\"first_step_attn\"], comp_setup_positions)\n",
    "\n",
    "    layers = np.arange(len(full_pivot_curve))\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    ax.plot(layers, full_pivot_curve, label=\"Full: pivot attention\", linewidth=2)\n",
    "    ax.plot(layers, full_setup_curve, label=\"Full: setup attention\", linewidth=2)\n",
    "    ax.plot(layers, comp_pivot_curve, \"--\", label=\"Compressed: pivot attention\", linewidth=2)\n",
    "    ax.plot(layers, comp_setup_curve, \"--\", label=\"Compressed: setup attention\", linewidth=2)\n",
    "    ax.set_xlabel(\"Layer\")\n",
    "    ax.set_ylabel(\"Attention mass\")\n",
    "    ax.set_title(\"Per-layer pivot/setup tracking (first answer transition)\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    path = FIG_DIR / \"kv_per_layer_pivot_setup_tracking.png\"\n",
    "    fig.savefig(path)\n",
    "    plt.show()\n",
    "    print(\"Saved:\", path)\n",
    "else:\n",
    "    print(\"Run KV experiment first to generate attention visuals.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6 Neural mirage sweep across eviction levels\n",
    "RUN_KV_SWEEP = bool(globals().get(\"RUN_KV_SWEEP\", False))\n",
    "KV_SWEEP_LEVELS = list(globals().get(\"KV_SWEEP_LEVELS\", [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]))\n",
    "\n",
    "if RUN_KV_SWEEP:\n",
    "    if kv_model is None or kv_tokenizer is None:\n",
    "        kv_tokenizer, kv_model = load_kv_model(KV_MODEL_NAME)\n",
    "\n",
    "    task = miragebench_tasks[KV_TASK_INDEX]\n",
    "    rows = []\n",
    "\n",
    "    for strat in [\"random\", \"attention_h2o\", \"setup_targeted\", \"contract_guarded\"]:\n",
    "        for lvl in KV_SWEEP_LEVELS:\n",
    "            try:\n",
    "                out = kv_strategy_run(task, kv_model, kv_tokenizer, strat, eviction_ratio=lvl, seed=SEED)\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"strategy\": strat,\n",
    "                        \"eviction_ratio\": lvl,\n",
    "                        \"raw_validity\": out[\"raw_validity\"],\n",
    "                        \"pivot_preserved\": float(out[\"pivot_preserved\"]),\n",
    "                        \"semantic_regret\": out[\"semantic_regret\"],\n",
    "                    }\n",
    "                )\n",
    "            except Exception as exc:\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"strategy\": strat,\n",
    "                        \"eviction_ratio\": lvl,\n",
    "                        \"error\": str(exc),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    kv_sweep_df = pd.DataFrame(rows)\n",
    "    kv_sweep_df.to_csv(RAW_DIR / \"kv_neural_mirage_sweep.csv\", index=False)\n",
    "\n",
    "    ok = kv_sweep_df[kv_sweep_df.get(\"error\").isna()] if \"error\" in kv_sweep_df.columns else kv_sweep_df\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    for strat, sub in ok.groupby(\"strategy\"):\n",
    "        sub = sub.sort_values(\"eviction_ratio\")\n",
    "        ax.plot(sub[\"raw_validity\"], sub[\"pivot_preserved\"], marker=\"o\", label=strat)\n",
    "        for _, row in sub.iterrows():\n",
    "            ax.text(row[\"raw_validity\"] + 0.002, row[\"pivot_preserved\"] + 0.002, f\"e={row['eviction_ratio']:.1f}\", fontsize=7)\n",
    "\n",
    "    ax.set_xlabel(\"Raw generation quality\")\n",
    "    ax.set_ylabel(\"Pivot consistency\")\n",
    "    ax.set_title(\"Neural Mirage Plot across KV eviction levels\")\n",
    "    ax.set_xlim(0, 1.02)\n",
    "    ax.set_ylim(0, 1.02)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    path = FIG_DIR / \"kv_neural_mirage_plot.png\"\n",
    "    fig.savefig(path)\n",
    "    plt.show()\n",
    "    print(\"Saved:\", path)\n",
    "else:\n",
    "    kv_sweep_df = pd.DataFrame()\n",
    "    print(\"Set RUN_KV_SWEEP=True in the control panel to run eviction-level sweep.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.7 Contract-guarded vs H2O direct comparison\n",
    "if 'kv_sweep_df' in globals() and not kv_sweep_df.empty:\n",
    "    ok = kv_sweep_df[kv_sweep_df.get(\"error\").isna()] if \"error\" in kv_sweep_df.columns else kv_sweep_df\n",
    "    cmp_df = (\n",
    "        ok[ok[\"strategy\"].isin([\"attention_h2o\", \"contract_guarded\"])]\n",
    "        .groupby(\"strategy\", as_index=False)\n",
    "        .agg(\n",
    "            mean_pivot_preservation=(\"pivot_preserved\", \"mean\"),\n",
    "            mean_raw_validity=(\"raw_validity\", \"mean\"),\n",
    "            mean_semantic_regret=(\"semantic_regret\", \"mean\"),\n",
    "        )\n",
    "    )\n",
    "    display(cmp_df)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.bar(cmp_df[\"strategy\"], cmp_df[\"mean_pivot_preservation\"], color=[\"#4c78a8\", \"#72b7b2\"])\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.set_ylabel(\"Pivot preservation\")\n",
    "    ax.set_title(\"Contract-guarded vs H2O (mean over eviction sweep)\")\n",
    "    fig.tight_layout()\n",
    "    path = FIG_DIR / \"kv_contract_vs_h2o_bar.png\"\n",
    "    fig.savefig(path)\n",
    "    plt.show()\n",
    "    print(\"Saved:\", path)\n",
    "else:\n",
    "    print(\"Run KV sweep first to compare contract-guarded vs H2O.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Divergence Scaling\n",
    "\n",
    "Extended probe:\n",
    "\n",
    "- `n  {1K, 5K, 10K, 50K, 100K, 500K, 1M}`\n",
    "- `  {0.3, 0.5, 0.7}`\n",
    "- `k  {1, 2, 3, 5}`\n",
    "- `200` seeds per cell\n",
    "\n",
    "We fit `cost ~ a * n^b` and report bootstrap CIs for `b`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Vectorized divergence simulator\n",
    "\n",
    "def interleaved_focal_mask(n: int, focal_fraction: float = 0.5) -> np.ndarray:\n",
    "    n_focal = int(round(n * focal_fraction))\n",
    "    n_focal = max(1, min(n - 1, n_focal))\n",
    "    idx = (np.arange(n) * n_focal) // n\n",
    "    mask = np.zeros(n, dtype=bool)\n",
    "    prev = -1\n",
    "    for i, b in enumerate(idx):\n",
    "        if b != prev and b < n_focal:\n",
    "            mask[i] = True\n",
    "        prev = b\n",
    "    return mask\n",
    "\n",
    "\n",
    "def simulate_divergence_cell(\n",
    "    n: int,\n",
    "    epsilon: float,\n",
    "    k: int,\n",
    "    seeds: int = 200,\n",
    "    chunk_size: int = 10,\n",
    "    focal_fraction: float = 0.5,\n",
    "    base_seed: int = 123,\n",
    ") -> pd.DataFrame:\n",
    "    # Returns per-seed costs with vectorized record detection.\n",
    "    # effective_cost is k-sensitive: only record shifts with sub-k support contribute.\n",
    "    focal = interleaved_focal_mask(n, focal_fraction=focal_fraction)\n",
    "    nonfocal_prefix = np.cumsum(~focal).astype(np.int64)\n",
    "\n",
    "    high_zone = (np.arange(n) / max(1, n - 1)) <= float(epsilon)\n",
    "    positions = np.arange(n, dtype=np.int64)\n",
    "\n",
    "    rows = []\n",
    "    seed_vals = np.arange(base_seed, base_seed + seeds, dtype=np.int64)\n",
    "\n",
    "    for start in range(0, seeds, chunk_size):\n",
    "        end = min(seeds, start + chunk_size)\n",
    "        m = end - start\n",
    "\n",
    "        rng = np.random.default_rng(int(seed_vals[start]))\n",
    "        low = rng.uniform(0.1, 5.0, size=(m, n)).astype(np.float32)\n",
    "        high = rng.uniform(5.0, 20.0, size=(m, n)).astype(np.float32)\n",
    "        w = np.where(high_zone[None, :], high, low)\n",
    "        w[:, ~focal] = -np.inf\n",
    "\n",
    "        runmax = np.maximum.accumulate(w, axis=1)\n",
    "        shifted = np.concatenate([\n",
    "            np.full((m, 1), -np.inf, dtype=np.float32),\n",
    "            runmax[:, :-1],\n",
    "        ], axis=1)\n",
    "        is_record = np.isfinite(w) & (w > shifted)\n",
    "\n",
    "        n_records = is_record.sum(axis=1)\n",
    "        rec_pos_sum = (is_record * positions[None, :]).sum(axis=1)\n",
    "        first_pos = np.where(n_records > 0, np.argmax(is_record, axis=1), 0)\n",
    "        total_cost = rec_pos_sum - first_pos\n",
    "\n",
    "        effective_cost = np.zeros(m, dtype=np.float64)\n",
    "        for r in range(m):\n",
    "            rec_pos = np.flatnonzero(is_record[r])\n",
    "            if rec_pos.size <= 1:\n",
    "                continue\n",
    "            prev = rec_pos[:-1]\n",
    "            curr = rec_pos[1:]\n",
    "            gaps = nonfocal_prefix[curr] - nonfocal_prefix[prev]\n",
    "            effective_cost[r] = float(curr[gaps < k].sum())\n",
    "\n",
    "        for local_idx in range(m):\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"n\": n,\n",
    "                    \"epsilon\": epsilon,\n",
    "                    \"k\": k,\n",
    "                    \"seed\": int(seed_vals[start + local_idx]),\n",
    "                    \"n_records\": int(n_records[local_idx]),\n",
    "                    \"total_cost\": float(total_cost[local_idx]),\n",
    "                    \"effective_cost\": float(effective_cost[local_idx]),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def fit_power_law(x: np.ndarray, y: np.ndarray) -> Dict[str, float]:\n",
    "    mask = (x > 0) & (y > 0)\n",
    "    x = x[mask]\n",
    "    y = y[mask]\n",
    "    lx = np.log(x)\n",
    "    ly = np.log(y)\n",
    "\n",
    "    if len(lx) < 2:\n",
    "        return {\"a\": np.nan, \"b\": np.nan}\n",
    "\n",
    "    b, intercept = np.polyfit(lx, ly, 1)\n",
    "    return {\"a\": float(np.exp(intercept)), \"b\": float(b)}\n",
    "\n",
    "\n",
    "def bootstrap_power_law_b(\n",
    "    df: pd.DataFrame,\n",
    "    eps: float,\n",
    "    k: int,\n",
    "    n_boot: int = 400,\n",
    "    seed: int = 123,\n",
    "    cost_col: str = \"effective_cost\",\n",
    ") -> Dict[str, float]:\n",
    "    sub = df[(df[\"epsilon\"] == eps) & (df[\"k\"] == k)].copy()\n",
    "    ns = sorted(sub[\"n\"].unique())\n",
    "\n",
    "    means = []\n",
    "    for n in ns:\n",
    "        means.append(sub[sub[\"n\"] == n][cost_col].mean())\n",
    "    fit = fit_power_law(np.array(ns, dtype=float), np.array(means, dtype=float))\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    b_samples = []\n",
    "    by_n = {n: sub[sub[\"n\"] == n][cost_col].to_numpy() for n in ns}\n",
    "\n",
    "    for _ in range(n_boot):\n",
    "        boot_means = []\n",
    "        for n in ns:\n",
    "            arr = by_n[n]\n",
    "            if len(arr) == 0:\n",
    "                boot_means.append(np.nan)\n",
    "            else:\n",
    "                sampled = rng.choice(arr, size=len(arr), replace=True)\n",
    "                boot_means.append(float(np.mean(sampled)))\n",
    "        boot_means = np.array(boot_means, dtype=float)\n",
    "        f = fit_power_law(np.array(ns, dtype=float), boot_means)\n",
    "        if np.isfinite(f[\"b\"]):\n",
    "            b_samples.append(f[\"b\"])\n",
    "\n",
    "    if b_samples:\n",
    "        lo, hi = np.percentile(b_samples, [2.5, 97.5])\n",
    "    else:\n",
    "        lo, hi = np.nan, np.nan\n",
    "\n",
    "    return {\n",
    "        \"epsilon\": eps,\n",
    "        \"k\": k,\n",
    "        \"a\": fit[\"a\"],\n",
    "        \"b\": fit[\"b\"],\n",
    "        \"b_ci_low\": float(lo),\n",
    "        \"b_ci_high\": float(hi),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Run extended divergence sweep\n",
    "RUN_DIVERGENCE = bool(globals().get(\"RUN_DIVERGENCE\", False))\n",
    "\n",
    "NS = list(globals().get(\"NS\", [1_000, 5_000, 10_000, 50_000, 100_000, 500_000, 1_000_000]))\n",
    "EPS = list(globals().get(\"EPS\", [0.3, 0.5, 0.7]))\n",
    "KS = list(globals().get(\"KS\", [1, 2, 3, 5]))\n",
    "SEEDS_PER_CELL = int(globals().get(\"SEEDS_PER_CELL\", 200))\n",
    "CHUNK_SIZE = int(globals().get(\"CHUNK_SIZE\", 10))  # memory-safe for large n\n",
    "\n",
    "if RUN_DIVERGENCE:\n",
    "    rows = []\n",
    "    for eps in EPS:\n",
    "        for k in KS:\n",
    "            for n in NS:\n",
    "                print(f\"Simulating n={n}, eps={eps}, k={k} ...\")\n",
    "                cell_df = simulate_divergence_cell(\n",
    "                    n=n,\n",
    "                    epsilon=eps,\n",
    "                    k=k,\n",
    "                    seeds=SEEDS_PER_CELL,\n",
    "                    chunk_size=CHUNK_SIZE,\n",
    "                    focal_fraction=0.5,\n",
    "                    base_seed=SEED + int(100 * eps) + 17 * k,\n",
    "                )\n",
    "                rows.append(cell_df)\n",
    "\n",
    "    divergence_raw_df = pd.concat(rows, ignore_index=True)\n",
    "    divergence_raw_df.to_csv(RAW_DIR / \"divergence_extended_raw.csv\", index=False)\n",
    "    print(\"Saved:\", RAW_DIR / \"divergence_extended_raw.csv\")\n",
    "else:\n",
    "    divergence_raw_df = pd.DataFrame()\n",
    "    print(\"Set RUN_DIVERGENCE=True in the control panel to run full divergence sweep.\")\n",
    "\n",
    "divergence_raw_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Fit exponents + stability analysis\n",
    "if divergence_raw_df.empty:\n",
    "    print(\"No divergence data yet.\")\n",
    "    divergence_fit_df = pd.DataFrame()\n",
    "    divergence_stability_df = pd.DataFrame()\n",
    "else:\n",
    "    # Bootstrap fits per (epsilon, k)\n",
    "    fit_rows = []\n",
    "    for eps in EPS:\n",
    "        for k in KS:\n",
    "            fit_rows.append(bootstrap_power_law_b(divergence_raw_df, eps=eps, k=k, n_boot=300, seed=SEED))\n",
    "\n",
    "    divergence_fit_df = pd.DataFrame(fit_rows)\n",
    "    divergence_fit_df.to_csv(RAW_DIR / \"divergence_powerlaw_fits.csv\", index=False)\n",
    "    display(divergence_fit_df.sort_values([\"epsilon\", \"k\"]))\n",
    "\n",
    "    # Exponent stability as n_max increases\n",
    "    stability_rows = []\n",
    "    for eps in EPS:\n",
    "        for k in KS:\n",
    "            sub = divergence_raw_df[(divergence_raw_df[\"epsilon\"] == eps) & (divergence_raw_df[\"k\"] == k)]\n",
    "            for n_max in NS[2:]:\n",
    "                s2 = sub[sub[\"n\"] <= n_max]\n",
    "                agg = s2.groupby(\"n\", as_index=False).agg(mean_cost=(\"effective_cost\", \"mean\"))\n",
    "                fit = fit_power_law(agg[\"n\"].to_numpy(dtype=float), agg[\"mean_cost\"].to_numpy(dtype=float))\n",
    "                stability_rows.append(\n",
    "                    {\n",
    "                        \"epsilon\": eps,\n",
    "                        \"k\": k,\n",
    "                        \"n_max\": n_max,\n",
    "                        \"b\": fit[\"b\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    divergence_stability_df = pd.DataFrame(stability_rows)\n",
    "    divergence_stability_df.to_csv(RAW_DIR / \"divergence_exponent_stability.csv\", index=False)\n",
    "    print(\"Saved fits + stability CSVs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Divergence figures\n",
    "if divergence_raw_df.empty:\n",
    "    print(\"No divergence data yet.\")\n",
    "else:\n",
    "    agg = (\n",
    "        divergence_raw_df.groupby([\"epsilon\", \"k\", \"n\"], as_index=False)\n",
    "        .agg(mean_cost=(\"effective_cost\", \"mean\"))\n",
    "        .sort_values([\"epsilon\", \"k\", \"n\"])\n",
    "    )\n",
    "\n",
    "    # Figure 1: log-log divergence plot (for k=3, all epsilon)\n",
    "    fig, ax = plt.subplots(figsize=(8.5, 5.5))\n",
    "    base_k = 3\n",
    "    sub = agg[agg[\"k\"] == base_k]\n",
    "    for eps, g in sub.groupby(\"epsilon\"):\n",
    "        x = g[\"n\"].to_numpy(dtype=float)\n",
    "        y = g[\"mean_cost\"].to_numpy(dtype=float)\n",
    "        fit = fit_power_law(x, y)\n",
    "        ax.loglog(x, y, \"o-\", label=f\"eps={eps}, b={fit['b']:.3f}\")\n",
    "\n",
    "    x_ref = np.array(NS, dtype=float)\n",
    "    y_ref = x_ref / x_ref[0]\n",
    "    y_ref_105 = (x_ref / x_ref[0]) ** 1.05\n",
    "    y_ref_11 = (x_ref / x_ref[0]) ** 1.10\n",
    "    # Normalize reference curves for visual comparability.\n",
    "    scale = sub[sub[\"n\"] == NS[0]][\"mean_cost\"].mean()\n",
    "    ax.loglog(x_ref, y_ref * scale, \"--\", color=\"black\", alpha=0.6, label=\"b=1.00\")\n",
    "    ax.loglog(x_ref, y_ref_105 * scale, \":\", color=\"black\", alpha=0.8, label=\"b=1.05\")\n",
    "    ax.loglog(x_ref, y_ref_11 * scale, \"-.\", color=\"black\", alpha=0.8, label=\"b=1.10\")\n",
    "\n",
    "    ax.set_xlabel(\"n\")\n",
    "    ax.set_ylabel(\"mean effective cost\")\n",
    "    ax.set_title(\"Log-log divergence scaling (k=3)\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    path1 = FIG_DIR / \"divergence_loglog_k3.png\"\n",
    "    fig.savefig(path1)\n",
    "    plt.show()\n",
    "    print(\"Saved:\", path1)\n",
    "\n",
    "    # Figure 2: exponent stability plot\n",
    "    if not divergence_stability_df.empty:\n",
    "        fig, ax = plt.subplots(figsize=(9, 5.5))\n",
    "        for (eps, k), g in divergence_stability_df.groupby([\"epsilon\", \"k\"]):\n",
    "            ax.plot(g[\"n_max\"], g[\"b\"], marker=\"o\", label=f\"eps={eps}, k={k}\")\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_xlabel(\"n_max used for fitting\")\n",
    "        ax.set_ylabel(\"Estimated b\")\n",
    "        ax.set_title(\"Exponent stability vs fit range\")\n",
    "        ax.legend(ncol=2, fontsize=8)\n",
    "        fig.tight_layout()\n",
    "        path2 = FIG_DIR / \"divergence_exponent_stability.png\"\n",
    "        fig.savefig(path2)\n",
    "        plt.show()\n",
    "        print(\"Saved:\", path2)\n",
    "\n",
    "    # Figure 3: heatmap of b by (epsilon, k)\n",
    "    if not divergence_fit_df.empty:\n",
    "        pivot = divergence_fit_df.pivot(index=\"k\", columns=\"epsilon\", values=\"b\")\n",
    "        fig, ax = plt.subplots(figsize=(6, 4.5))\n",
    "        sns.heatmap(pivot, annot=True, fmt=\".3f\", cmap=\"viridis\", ax=ax)\n",
    "        ax.set_title(\"Power-law exponent b by (epsilon, k)\")\n",
    "        fig.tight_layout()\n",
    "        path3 = FIG_DIR / \"divergence_b_heatmap.png\"\n",
    "        fig.savefig(path3)\n",
    "        plt.show()\n",
    "        print(\"Saved:\", path3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. MirageBench Packaging\n",
    "\n",
    "This section exports:\n",
    "\n",
    "- `miragebench_v0_1_tasks.json`\n",
    "- model results CSVs\n",
    "- leaderboard table\n",
    "- iconic mirage plots\n",
    "- zipped result bundle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Export MirageBench tasks JSON (release-ready schema)\n",
    "\n",
    "def task_release_dict(task: MirageBenchTask) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"task_id\": task.task_id,\n",
    "        \"category\": task.category,\n",
    "        \"full_context\": task.full_context,\n",
    "        \"compressed_context\": task.compressed_context,\n",
    "        \"question\": task.question,\n",
    "        \"pivot_ground_truth\": task.pivot_ground_truth,\n",
    "        \"answer_ground_truth\": task.answer_ground_truth,\n",
    "        \"decoy_pivot\": task.decoy_pivot,\n",
    "        \"decoy_answer\": task.decoy_answer,\n",
    "        \"k\": task.k,\n",
    "    }\n",
    "\n",
    "release_tasks = [task_release_dict(t) for t in miragebench_tasks]\n",
    "\n",
    "tasks_json_path = RAW_DIR / \"miragebench_v0_1_tasks.json\"\n",
    "with open(tasks_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(release_tasks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved:\", tasks_json_path)\n",
    "print(\"Task count:\", len(release_tasks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Build leaderboard + summary table (if black-box results exist)\n",
    "if 'blackbox_results_df' in globals() and (not blackbox_results_df.empty):\n",
    "    tmp = blackbox_results_df.copy()\n",
    "    if \"error\" in tmp.columns:\n",
    "        tmp = tmp[tmp[\"error\"].isna()] if tmp[\"error\"].notna().any() else tmp\n",
    "\n",
    "    for c in [\"raw_validity\", \"semantic_regret\", \"pivot_preserved\", \"mirage_flag\", \"pivot_matches_ground_truth\"]:\n",
    "        if c in tmp.columns:\n",
    "            tmp[c] = pd.to_numeric(tmp[c], errors=\"coerce\")\n",
    "\n",
    "    leaderboard = (\n",
    "        tmp.groupby(\"model_name\", as_index=False)\n",
    "        .agg(\n",
    "            mean_raw_validity=(\"raw_validity\", \"mean\"),\n",
    "            mean_pivot_preservation=(\"pivot_preserved\", \"mean\"),\n",
    "            mean_semantic_regret=(\"semantic_regret\", \"mean\"),\n",
    "            mirage_rate=(\"mirage_flag\", \"mean\"),\n",
    "            pivot_gt_rate=(\"pivot_matches_ground_truth\", \"mean\"),\n",
    "            n=(\"task_id\", \"count\"),\n",
    "        )\n",
    "        .sort_values([\"mean_pivot_preservation\", \"mean_raw_validity\"], ascending=[False, False])\n",
    "    )\n",
    "\n",
    "    display(leaderboard)\n",
    "\n",
    "    leaderboard_path = RAW_DIR / \"miragebench_leaderboard.csv\"\n",
    "    leaderboard.to_csv(leaderboard_path, index=False)\n",
    "    print(\"Saved:\", leaderboard_path)\n",
    "\n",
    "    # Paper-style summary table per model x category\n",
    "    summary_table = (\n",
    "        tmp.groupby([\"model_name\", \"category\"], as_index=False)\n",
    "        .agg(\n",
    "            raw_validity=(\"raw_validity\", \"mean\"),\n",
    "            pivot_preservation=(\"pivot_preserved\", \"mean\"),\n",
    "            semantic_regret=(\"semantic_regret\", \"mean\"),\n",
    "            mirage_rate=(\"mirage_flag\", \"mean\"),\n",
    "            n=(\"task_id\", \"count\"),\n",
    "        )\n",
    "    )\n",
    "    summary_path = RAW_DIR / \"miragebench_summary_table.csv\"\n",
    "    summary_table.to_csv(summary_path, index=False)\n",
    "    print(\"Saved:\", summary_path)\n",
    "else:\n",
    "    print(\"No black-box results available yet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Save an iconic mirage plot per model (if results exist)\n",
    "if 'blackbox_results_df' in globals() and (not blackbox_results_df.empty):\n",
    "    tmp = blackbox_results_df.copy()\n",
    "    if \"error\" in tmp.columns:\n",
    "        tmp = tmp[tmp[\"error\"].isna()] if tmp[\"error\"].notna().any() else tmp\n",
    "\n",
    "    tmp[\"raw_validity\"] = pd.to_numeric(tmp.get(\"raw_validity\"), errors=\"coerce\")\n",
    "    tmp[\"pivot_preserved\"] = pd.to_numeric(tmp.get(\"pivot_preserved\"), errors=\"coerce\")\n",
    "\n",
    "    for model_name, sub in tmp.groupby(\"model_name\"):\n",
    "        agg = sub.groupby(\"compression_level\", as_index=False).agg(\n",
    "            raw_validity=(\"raw_validity\", \"mean\"),\n",
    "            pivot_preservation=(\"pivot_preserved\", \"mean\"),\n",
    "        )\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6.4, 4.8))\n",
    "        ax.plot(agg[\"raw_validity\"], agg[\"pivot_preservation\"], marker=\"o\", linewidth=2)\n",
    "        for _, row in agg.iterrows():\n",
    "            ax.text(row[\"raw_validity\"] + 0.003, row[\"pivot_preservation\"] + 0.003, f\"c={row['compression_level']:.1f}\")\n",
    "        ax.set_xlim(0, 1.02)\n",
    "        ax.set_ylim(0, 1.02)\n",
    "        ax.set_xlabel(\"Raw validity\")\n",
    "        ax.set_ylabel(\"Pivot preservation\")\n",
    "        ax.set_title(f\"Mirage plot: {model_name}\")\n",
    "        fig.tight_layout()\n",
    "\n",
    "        path = FIG_DIR / f\"mirage_plot_{model_name.replace('/', '_')}.png\"\n",
    "        fig.savefig(path)\n",
    "        plt.close(fig)\n",
    "        print(\"Saved:\", path)\n",
    "else:\n",
    "    print(\"No black-box results available yet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Zip all outputs for one-click download\n",
    "zip_path = RESULTS_ROOT / \"miragebench_colab_results.zip\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for path in RESULTS_ROOT.rglob(\"*\"):\n",
    "        if path.is_file() and path != zip_path:\n",
    "            zf.write(path, arcname=path.relative_to(RESULTS_ROOT.parent))\n",
    "\n",
    "print(\"Created:\", zip_path)\n",
    "\n",
    "# Optional Colab download helper\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"You can download the zip with:\")\n",
    "    print(f\"files.download('{zip_path}')\")\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Summary & Key Findings\n",
    "\n",
    "Use this section after running the experiments:\n",
    "\n",
    "1. Which models are most susceptible to the Validity Mirage?\n",
    "2. Does KV-cache surgery show attention shift from true pivot/setup to decoy pivot?\n",
    "3. Does contract-guarded eviction outperform H2O-style eviction on pivot preservation?\n",
    "4. Is divergence exponent `b` stable at scale, or does it drift upward?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Success criteria checks\n",
    "criterion_1 = False\n",
    "criterion_2 = False\n",
    "criterion_3 = False\n",
    "\n",
    "# (1) High validity + low pivot preservation in black-box results\n",
    "if 'blackbox_results_df' in globals() and not blackbox_results_df.empty:\n",
    "    tmp = blackbox_results_df.copy()\n",
    "    if \"error\" in tmp.columns:\n",
    "        tmp = tmp[tmp[\"error\"].isna()] if tmp[\"error\"].notna().any() else tmp\n",
    "    if {\"raw_validity\", \"pivot_preserved\"}.issubset(tmp.columns):\n",
    "        tmp[\"raw_validity\"] = pd.to_numeric(tmp[\"raw_validity\"], errors=\"coerce\")\n",
    "        tmp[\"pivot_preserved\"] = pd.to_numeric(tmp[\"pivot_preserved\"], errors=\"coerce\")\n",
    "        criterion_1 = bool(((tmp[\"raw_validity\"] >= 0.75) & (tmp[\"pivot_preserved\"] < 0.5)).any())\n",
    "\n",
    "# (2) Attention shift / pivot switch in KV sweep\n",
    "if 'kv_sweep_df' in globals() and not kv_sweep_df.empty:\n",
    "    ok = kv_sweep_df[kv_sweep_df.get(\"error\").isna()] if \"error\" in kv_sweep_df.columns else kv_sweep_df\n",
    "    if {\"raw_validity\", \"pivot_preserved\"}.issubset(ok.columns):\n",
    "        criterion_2 = bool(((ok[\"raw_validity\"] >= 0.65) & (ok[\"pivot_preserved\"] < 0.5)).any())\n",
    "\n",
    "# (3) Contract-guarded better than H2O\n",
    "if 'kv_sweep_df' in globals() and not kv_sweep_df.empty:\n",
    "    ok = kv_sweep_df[kv_sweep_df.get(\"error\").isna()] if \"error\" in kv_sweep_df.columns else kv_sweep_df\n",
    "    if {\"strategy\", \"pivot_preserved\"}.issubset(ok.columns):\n",
    "        means = ok.groupby(\"strategy\", as_index=False)[\"pivot_preserved\"].mean()\n",
    "        if {\"contract_guarded\", \"attention_h2o\"}.issubset(set(means[\"strategy\"])):\n",
    "            c = float(means[means[\"strategy\"] == \"contract_guarded\"][\"pivot_preserved\"].iloc[0])\n",
    "            h = float(means[means[\"strategy\"] == \"attention_h2o\"][\"pivot_preserved\"].iloc[0])\n",
    "            criterion_3 = c > h\n",
    "\n",
    "print(\"Success criteria status:\")\n",
    "print(f\"1) Real-model validity mirage observed: {criterion_1}\")\n",
    "print(f\"2) Neural-level shift under KV surgery observed: {criterion_2}\")\n",
    "print(f\"3) Contract-guarded beats H2O on pivot preservation: {criterion_3}\")\n",
    "\n",
    "if criterion_1 or criterion_2 or criterion_3:\n",
    "    print(\"\\nAt least one publishable bridge result is present.\")\n",
    "else:\n",
    "    print(\"\\nNo criterion has fired yet. Run more models/tasks or increase sweep depth.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}