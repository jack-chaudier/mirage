{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mirage-Guarded KV Cache Demo (RoPE-Preserving Attention-Mask Ablation)\n",
    "\n",
    "**Model target:** Llama 3.1 8B on H100 (Colab Pro+)\n",
    "\n",
    "This notebook demonstrates a validity-mirage failure mode under compression:\n",
    "standard structure-blind eviction keeps responses well-formed but can silently\n",
    "substitute the wrong causal hypothesis.\n",
    "\n",
    "## Mathematical Framing (L2 Frontier)\n",
    "\n",
    "Each chunk carries an L1 state `(weight, d_total, d_pre)`:\n",
    "\n",
    "- `weight`: pivot severity if the chunk is a pivot, else `-inf`\n",
    "- `d_total`: predecessor capacity contribution (1 for warning chunks, else 0)\n",
    "- `d_pre`: predecessor count tracked through provenance\n",
    "\n",
    "The L2 streaming scan maintains `W[0..k]` (here `k=3`) with composition:\n",
    "\n",
    "`W_new[j] = max(W_prev[j], W_incoming[max(0, j - d_total_prev)])`\n",
    "\n",
    "This scan is `O(n)` in chunks (vector size `k+1` is constant). We track provenance\n",
    "for `W[k]` so the final protected set is the pivot plus its structural predecessors.\n",
    "The eviction policy enforces this provenance set as a hard budget constraint.\n",
    "\n",
    "The contract `d_total(mu(B)) >= min(d_total(B), k)` is checked after each full scan.\n",
    "\n",
    "## Important Terminology\n",
    "\n",
    "Eviction is applied via **attention-mask ablation** over the full token stream.\n",
    "This preserves RoPE geometry and simulates inaccessible information, but it is\n",
    "not literal `past_key_values` pruning. The limitations section states this\n",
    "explicitly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional one-time compatibility fix (only if transformers import complains about tokenizers):\n",
    "# !python -m pip install --no-deps --force-reinstall tokenizers==0.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOST: f5935bf15b00\n",
      "PID: 11162\n",
      "TORCH: 2.10.0+cu128 CUDA: 12.8 AVAILABLE: True\n",
      "NVIDIA_SMI: /opt/bin/nvidia-smi\n",
      "GPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-5e10842d-d072-6d92-cc1f-e21f668ebffd)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, socket, torch, shutil, subprocess\n",
    "print(\"HOST:\", socket.gethostname())\n",
    "print(\"PID:\", os.getpid())\n",
    "print(\"TORCH:\", torch.__version__, \"CUDA:\", torch.version.cuda, \"AVAILABLE:\", torch.cuda.is_available())\n",
    "print(\"NVIDIA_SMI:\", shutil.which(\"nvidia-smi\"))\n",
    "if shutil.which(\"nvidia-smi\"):\n",
    "    print(subprocess.run([\"nvidia-smi\", \"-L\"], capture_output=True, text=True).stdout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "colab-bootstrap"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Running in Colab: True\n",
      "Artifact directory: /content/drive/MyDrive/mirage_outputs/mirage_rca_demo\n"
     ]
    }
   ],
   "source": [
    "# Colab/Local runtime bootstrap\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "IS_COLAB = False\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = Path('/content').exists()\n",
    "\n",
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    ARTIFACT_DIR = Path('/content/drive/MyDrive/mirage_outputs/mirage_rca_demo')\n",
    "else:\n",
    "    ARTIFACT_DIR = Path('artifacts/mirage_rca_demo')\n",
    "\n",
    "print(f'Running in Colab: {IS_COLAB}')\n",
    "print(f'Artifact directory: {ARTIFACT_DIR}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KERNEL: alive\n",
      "python=3.12.12\n",
      "pid=11162\n",
      "cwd=/content\n",
      "ts=1772061562.2624938\n",
      "Run this first cell whenever execution appears to do nothing\n"
     ]
    }
   ],
   "source": [
    "# ====== Kernel sanity check ======\n",
    "import os, platform, time\n",
    "print(\"KERNEL: alive\")\n",
    "print(f\"python={platform.python_version()}\")\n",
    "print(f\"pid={os.getpid()}\")\n",
    "print(f\"cwd={os.getcwd()}\")\n",
    "print(f\"ts={time.time()}\")\n",
    "print(\"Run this first cell whenever execution appears to do nothing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency check: all pinned packages already present.\n",
      "{\n",
      "  \"timestamp_utc\": \"2026-02-25T23:19:24.086736+00:00\",\n",
      "  \"python\": \"3.12.12\",\n",
      "  \"platform\": \"Linux-6.6.113+-x86_64-with-glibc2.35\",\n",
      "  \"seed\": 20260225,\n",
      "  \"is_colab\": true,\n",
      "  \"torch_version\": \"2.10.0+cu128\",\n",
      "  \"torch_cuda_version\": \"12.8\",\n",
      "  \"has_cuda_runtime\": true,\n",
      "  \"has_cuda_torch\": true,\n",
      "  \"cuda_ready\": true,\n",
      "  \"pinned_packages\": [\n",
      "    \"transformers==4.46.3\",\n",
      "    \"tokenizers==0.20.3\",\n",
      "    \"accelerate==1.1.1\",\n",
      "    \"bitsandbytes==0.45.0\",\n",
      "    \"huggingface_hub==0.26.2\",\n",
      "    \"sentencepiece==0.2.0\",\n",
      "    \"protobuf==5.28.3\",\n",
      "    \"matplotlib==3.9.2\",\n",
      "    \"numpy==2.1.2\"\n",
      "  ],\n",
      "  \"runtime_gpu_visible\": true,\n",
      "  \"runtime_gpus\": [\n",
      "    \"NVIDIA A100-SXM4-80GB\"\n",
      "  ],\n",
      "  \"torch_cuda_available\": true,\n",
      "  \"gpu_name\": \"NVIDIA A100-SXM4-80GB\",\n",
      "  \"compute_capability\": \"8.0\",\n",
      "  \"gpu_mem_gb\": 85.09\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Reproducible environment setup: pin deps, set seeds, and log environment.\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import platform\n",
    "import subprocess\n",
    "import shutil\n",
    "import importlib.metadata as importlib_metadata\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Keep torch out of the pinned install list.\n",
    "# Pinning torch in Colab can accidentally replace the runtime CUDA wheel with CPU-only torch.\n",
    "PINNED_PACKAGES = [\n",
    "    \"transformers==4.46.3\",\n",
    "    \"tokenizers==0.20.3\",\n",
    "    \"accelerate==1.1.1\",\n",
    "    \"bitsandbytes==0.45.0\",\n",
    "    \"huggingface_hub==0.26.2\",\n",
    "    \"sentencepiece==0.2.0\",\n",
    "    \"protobuf==5.28.3\",\n",
    "    \"matplotlib==3.9.2\",\n",
    "    \"numpy==2.1.2\",\n",
    "]\n",
    "\n",
    "def _version_ok(spec: str) -> bool:\n",
    "    name, want = spec.split(\"==\")\n",
    "    try:\n",
    "        return importlib_metadata.version(name) == want\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def ensure_pinned_packages(specs):\n",
    "    to_install = [s for s in specs if not _version_ok(s)]\n",
    "    if not to_install:\n",
    "        print(\"Dependency check: all pinned packages already present.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Dependency check: installing {len(to_install)} package(s)...\")\n",
    "    print(\"  \" + \", \".join(to_install))\n",
    "    # `--no-deps` avoids mutating torch/runtime packages in managed notebooks.\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--no-deps\", *to_install], check=True)\n",
    "    print(\"Dependency check: pin install complete.\")\n",
    "\n",
    "\n",
    "ensure_pinned_packages(PINNED_PACKAGES)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 20260225\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def _detect_runtime_gpus():\n",
    "    if not shutil.which(\"nvidia-smi\"):\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        out = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=name\", \"--format=csv,noheader\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=False,\n",
    "        )\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    if out.returncode != 0:\n",
    "        return []\n",
    "\n",
    "    return [line.strip() for line in out.stdout.splitlines() if line.strip()]\n",
    "\n",
    "\n",
    "RUNTIME_GPUS = _detect_runtime_gpus()\n",
    "RUNTIME_GPU_VISIBLE = bool(RUNTIME_GPUS)\n",
    "TORCH_CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "HAS_CUDA_TORCH = torch.version.cuda is not None\n",
    "HAS_CUDA_READY = RUNTIME_GPU_VISIBLE and TORCH_CUDA_AVAILABLE and HAS_CUDA_TORCH\n",
    "\n",
    "runtime_info = {\n",
    "    \"runtime_gpu_visible\": RUNTIME_GPU_VISIBLE,\n",
    "    \"runtime_gpus\": RUNTIME_GPUS,\n",
    "    \"torch_cuda_available\": TORCH_CUDA_AVAILABLE,\n",
    "}\n",
    "\n",
    "if TORCH_CUDA_AVAILABLE:\n",
    "    runtime_info.update({\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0),\n",
    "        \"compute_capability\": f\"{torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}\",\n",
    "        \"gpu_mem_gb\": round(torch.cuda.get_device_properties(0).total_memory / 1e9, 2),\n",
    "    })\n",
    "elif RUNTIME_GPU_VISIBLE and not HAS_CUDA_TORCH:\n",
    "    runtime_info[\"message\"] = \"GPU runtime is present but torch is CPU-only\"\n",
    "elif not RUNTIME_GPU_VISIBLE:\n",
    "    runtime_info[\"message\"] = \"CUDA device not visible at runtime\"\n",
    "else:\n",
    "    runtime_info[\"message\"] = \"GPU runtime present but torch CUDA is unavailable\"\n",
    "\n",
    "env_info = {\n",
    "    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"python\": platform.python_version(),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"seed\": SEED,\n",
    "    \"is_colab\": bool(globals().get(\"IS_COLAB\", False)),\n",
    "    \"torch_version\": torch.__version__,\n",
    "    \"torch_cuda_version\": torch.version.cuda,\n",
    "    \"has_cuda_runtime\": RUNTIME_GPU_VISIBLE,\n",
    "    \"has_cuda_torch\": HAS_CUDA_TORCH,\n",
    "    \"cuda_ready\": HAS_CUDA_READY,\n",
    "    \"pinned_packages\": PINNED_PACKAGES,\n",
    "    **runtime_info,\n",
    "}\n",
    "\n",
    "print(json.dumps(env_info, indent=2))\n",
    "\n",
    "if not HAS_CUDA_READY:\n",
    "    if RUNTIME_GPU_VISIBLE and not HAS_CUDA_TORCH:\n",
    "        hint = (\n",
    "            \"GPU runtime is attached, but torch is CPU-only. Reinstall CUDA torch and restart runtime:\\n\"\n",
    "            \"python -m pip install --index-url https://download.pytorch.org/whl/cu124 --force-reinstall --no-cache-dir torch\"\n",
    "        )\n",
    "    elif not RUNTIME_GPU_VISIBLE:\n",
    "        hint = (\n",
    "            \"Colab attached a CPU VM (GPU is not visible via nvidia-smi). \"\n",
    "            \"Choose Runtime -> Change runtime type -> GPU, then Runtime -> Restart session. \"\n",
    "            \"If it still comes back CPU, quota/availability is likely exhausted right now.\"\n",
    "        )\n",
    "    elif HAS_CUDA_TORCH and not TORCH_CUDA_AVAILABLE:\n",
    "        hint = \"GPU is present and torch has CUDA build, but CUDA failed to initialize. Restart runtime and rerun from the first cell.\"\n",
    "    else:\n",
    "        hint = \"unknown CUDA mismatch. Restart runtime and retry.\"\n",
    "\n",
    "    runtime_hint = \"in Colab\" if globals().get(\"IS_COLAB\", False) else \"in a local runtime\"\n",
    "    print(f\"No GPU found ({runtime_hint}). {hint}\")\n",
    "    raise RuntimeError(f\"No GPU found ({runtime_hint}). {hint}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Runtime diagnostics ---\n",
      "Python: 3.12.12\n",
      "Torch: 2.10.0+cu128 | torch.cuda: 12.8\n",
      "torch.cuda.is_available(): True\n",
      "Detected CUDA devices: 1\n",
      "Current device: 0\n",
      "GPU name: NVIDIA A100-SXM4-80GB\n",
      "Compute capability: 8.0\n",
      "Total memory: 85.09 GB\n",
      "nvidia-smi:\n",
      "NVIDIA A100-SXM4-80GB, 81920, 6\n"
     ]
    }
   ],
   "source": [
    "# GPU sanity probe (run anytime execution stalls or after env setup)\n",
    "import shutil\n",
    "import subprocess\n",
    "import platform\n",
    "import torch\n",
    "\n",
    "print(\"--- Runtime diagnostics ---\")\n",
    "print(f\"Python: {platform.python_version()}\")\n",
    "print(f\"Torch: {torch.__version__} | torch.cuda: {torch.version.cuda}\")\n",
    "print(f\"torch.cuda.is_available(): {torch.cuda.is_available()}\")\n",
    "print(f\"Detected CUDA devices: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    idx = torch.cuda.current_device()\n",
    "    props = torch.cuda.get_device_properties(idx)\n",
    "    print(f\"Current device: {idx}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(idx)}\")\n",
    "    print(f\"Compute capability: {props.major}.{props.minor}\")\n",
    "    print(f\"Total memory: {props.total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA runtime not exposed to Torch.\")\n",
    "\n",
    "if shutil.which(\"nvidia-smi\"):\n",
    "    try:\n",
    "        out = subprocess.run([\"nvidia-smi\", \"--query-gpu=name,memory.total,memory.used\", \"--format=csv,noheader,nounits\"],\n",
    "                             capture_output=True, text=True, check=False)\n",
    "        print(\"nvidia-smi:\\n\" + (out.stdout.strip() or \"<no output>\"))\n",
    "    except Exception as exc:\n",
    "        print(f\"nvidia-smi failed: {exc}\")\n",
    "else:\n",
    "    print(\"nvidia-smi not available in PATH.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN not found.\n",
      "Notebook will continue; add token later only if model loading prompts for auth.\n",
      "To authenticate: set HF_TOKEN in environment, Colab secret, or run huggingface_hub.login manually when prompted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "if HF_TOKEN is None:\n",
    "    try:\n",
    "        from google.colab import userdata  # type: ignore\n",
    "        HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
    "    except Exception:\n",
    "        HF_TOKEN = None\n",
    "\n",
    "if HF_TOKEN:\n",
    "    try:\n",
    "        login(token=HF_TOKEN, add_to_git_credential=False)\n",
    "        print(\"HF login: authenticated from env var or Colab secret\")\n",
    "    except Exception as e:\n",
    "        print(f\"HF login skipped: token present but login failed ({type(e).__name__}: {e})\")\n",
    "else:\n",
    "    print(\"HF_TOKEN not found.\")\n",
    "    print(\"Notebook will continue; add token later only if model loading prompts for auth.\")\n",
    "    print(\"To authenticate: set HF_TOKEN in environment, Colab secret, or run huggingface_hub.login manually when prompted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN loaded: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    if not os.getenv(\"HF_TOKEN\"):\n",
    "        os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"HF_TOKEN loaded:\", bool(os.getenv(\"HF_TOKEN\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN set: True\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "from huggingface_hub import login\n",
    "\n",
    "if not os.getenv(\"HF_TOKEN\"):\n",
    "    os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HF_TOKEN: \").strip()  # hidden input\n",
    "\n",
    "login(token=os.environ[\"HF_TOKEN\"], add_to_git_credential=False)\n",
    "print(\"HF_TOKEN set:\", bool(os.getenv(\"HF_TOKEN\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9fff31dd6d45b4a49731c62040204d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 8.0B params\n",
      "Pad token: <|eot_id|> (id=128009)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import logging as hf_logging\n",
    "import torch\n",
    "\n",
    "hf_logging.set_verbosity_error()  # silence sampling-config warnings\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "MODEL_DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=MODEL_DTYPE,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\",  # needed for output_attentions=True baselines\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Ensure deterministic decoding settings.\n",
    "model.generation_config.do_sample = False\n",
    "model.generation_config.temperature = 1.0\n",
    "model.generation_config.top_p = 1.0\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "assert tokenizer.is_fast, \"Need fast tokenizer for offset_mapping support\"\n",
    "\n",
    "print(f\"Model loaded: {sum(p.numel() for p in model.parameters()) / 1e9:.1f}B params\")\n",
    "print(f\"Pad token: {tokenizer.pad_token} (id={tokenizer.pad_token_id})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core functions defined ✓\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, tokenizer, text: str, max_new_tokens: int = 80) -> str:\n",
    "    \"\"\"Deterministic generation from raw text.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True).to(model.device)\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    new_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_chunk_masked_inputs(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    full_text: str,\n",
    "    query: str,\n",
    "    chunk_ids: List[str],\n",
    "    chunk_char_spans: List[tuple],\n",
    "    evicted_ids: set,\n",
    ") -> dict:\n",
    "    \"\"\"Tokenize once and return masked inputs that preserve original position IDs.\"\"\"\n",
    "    combined = full_text + \"\\n\\n\" + query\n",
    "    enc = tokenizer(\n",
    "        combined,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "\n",
    "    raw_offsets = enc.pop(\"offset_mapping\")\n",
    "    if hasattr(raw_offsets, \"tolist\"):\n",
    "        raw_offsets = raw_offsets.tolist()\n",
    "    if raw_offsets and isinstance(raw_offsets[0], (list, tuple)) and raw_offsets[0] and isinstance(raw_offsets[0][0], (list, tuple)):\n",
    "        raw_offsets = raw_offsets[0]\n",
    "    offsets = [tuple(pair) for pair in raw_offsets]\n",
    "    input_ids = enc[\"input_ids\"].to(model.device)\n",
    "    attention_mask = enc[\"attention_mask\"].clone().to(model.device)\n",
    "    position_ids = torch.arange(input_ids.shape[1], device=model.device).unsqueeze(0)\n",
    "\n",
    "    evicted_spans = [span for cid, span in zip(chunk_ids, chunk_char_spans) if cid in evicted_ids]\n",
    "    context_end = len(full_text)\n",
    "    context_tokens = 0\n",
    "    masked_tokens = 0\n",
    "\n",
    "    for tok_idx, (tok_start, tok_end) in enumerate(offsets):\n",
    "        if tok_end <= tok_start:\n",
    "            continue\n",
    "        if tok_start >= context_end:\n",
    "            continue\n",
    "\n",
    "        context_tokens += 1\n",
    "        for ch_start, ch_end in evicted_spans:\n",
    "            if tok_end > ch_start and tok_start < ch_end:\n",
    "                attention_mask[0, tok_idx] = 0\n",
    "                masked_tokens += 1\n",
    "                break\n",
    "\n",
    "    if attention_mask.shape[1] > 0:\n",
    "        attention_mask[0, 0] = 1  # never mask BOS\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"position_ids\": position_ids,\n",
    "        \"masked_tokens\": int(masked_tokens),\n",
    "        \"context_tokens\": int(context_tokens),\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_with_chunk_mask(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    full_text: str,\n",
    "    query: str,\n",
    "    chunk_ids: List[str],\n",
    "    chunk_char_spans: List[tuple],\n",
    "    evicted_ids: set,\n",
    "    max_new_tokens: int = 80,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Approximate KV-style eviction without retokenizing surviving text:\n",
    "    keep full token stream, but mask evicted-chunk tokens in attention_mask.\n",
    "    Explicit position_ids are provided so RoPE positions remain contiguous.\n",
    "    \"\"\"\n",
    "    mask_state = build_chunk_masked_inputs(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        full_text=full_text,\n",
    "        query=query,\n",
    "        chunk_ids=chunk_ids,\n",
    "        chunk_char_spans=chunk_char_spans,\n",
    "        evicted_ids=evicted_ids,\n",
    "    )\n",
    "\n",
    "    input_ids = mask_state[\"input_ids\"]\n",
    "    attention_mask = mask_state[\"attention_mask\"]\n",
    "    position_ids = mask_state[\"position_ids\"]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    new_tokens = outputs[0][input_ids.shape[1]:]\n",
    "    return tokenizer.decode(new_tokens, skip_special_tokens=True), int(mask_state[\"masked_tokens\"]), int(mask_state[\"context_tokens\"])\n",
    "@torch.no_grad()\n",
    "\n",
    "def get_chunk_attention(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    full_text: str,\n",
    "    query: Optional[str],\n",
    "    chunk_char_spans: List[tuple],\n",
    "    pool: str = \"max\",\n",
    "    probe_tokens: int = 64,\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Compute per-chunk salience from attention.\n",
    "\n",
    "    If query is provided: score using query-token attention (oracle-ish baseline).\n",
    "    If query is None: score using tail-prefix probe tokens (streaming-style proxy).\n",
    "\n",
    "    pool options: \"max\", \"mean\", \"sum\".\n",
    "    \"\"\"\n",
    "    combined = full_text if query is None else (full_text + \"\\n\\n\" + query)\n",
    "    enc = tokenizer(\n",
    "        combined,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "\n",
    "    raw_offsets = enc.pop(\"offset_mapping\")\n",
    "    if hasattr(raw_offsets, \"tolist\"):\n",
    "        raw_offsets = raw_offsets.tolist()\n",
    "    if raw_offsets and isinstance(raw_offsets[0], (list, tuple)) and raw_offsets[0] and isinstance(raw_offsets[0][0], (list, tuple)):\n",
    "        raw_offsets = raw_offsets[0]\n",
    "    offsets = [tuple(pair) for pair in raw_offsets]\n",
    "    inputs = {k: v.to(model.device) for k, v in enc.items()}\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        output_attentions=True,\n",
    "    )\n",
    "\n",
    "    seq_len = outputs.attentions[0].shape[-1]\n",
    "    n_layers = len(outputs.attentions)\n",
    "    context_end = len(full_text)\n",
    "\n",
    "    valid_tok_idxs = [\n",
    "        i for i, (s, e) in enumerate(offsets)\n",
    "        if e > s\n",
    "    ]\n",
    "    context_tok_idxs = [\n",
    "        i for i, (s, e) in enumerate(offsets)\n",
    "        if e > s and s < context_end\n",
    "    ]\n",
    "\n",
    "    if query is None:\n",
    "        probe_idxs = context_tok_idxs[-min(probe_tokens, len(context_tok_idxs)):] if context_tok_idxs else valid_tok_idxs[-1:]\n",
    "    else:\n",
    "        probe_idxs = [\n",
    "            i for i, (s, e) in enumerate(offsets)\n",
    "            if e > s and s >= context_end\n",
    "        ]\n",
    "        if not probe_idxs:\n",
    "            probe_idxs = context_tok_idxs[-min(probe_tokens, len(context_tok_idxs)):] if context_tok_idxs else valid_tok_idxs[-1:]\n",
    "\n",
    "    attn_sum = torch.zeros(seq_len, device=model.device)\n",
    "    for layer_attn in outputs.attentions:\n",
    "        # layer_attn: [batch=1, heads, q_len, k_len]\n",
    "        probe_attn = layer_attn[0, :, probe_idxs, :].mean(dim=0)  # [probe, k_len]\n",
    "        attn_sum += probe_attn.sum(dim=0)\n",
    "\n",
    "    denom = max(1, n_layers * len(probe_idxs))\n",
    "    attn_avg = (attn_sum / denom).detach().cpu().numpy()\n",
    "\n",
    "    chunk_scores = []\n",
    "    for char_start, char_end in chunk_char_spans:\n",
    "        tok_vals = []\n",
    "        for tok_idx, (tok_start, tok_end) in enumerate(offsets):\n",
    "            if tok_end <= tok_start:\n",
    "                continue\n",
    "            if tok_end > char_start and tok_start < char_end:\n",
    "                tok_vals.append(float(attn_avg[tok_idx]))\n",
    "\n",
    "        if not tok_vals:\n",
    "            chunk_scores.append(0.0)\n",
    "        elif pool == \"sum\":\n",
    "            chunk_scores.append(float(np.sum(tok_vals)))\n",
    "        elif pool == \"mean\":\n",
    "            chunk_scores.append(float(np.mean(tok_vals)))\n",
    "        else:\n",
    "            chunk_scores.append(float(np.max(tok_vals)))  # default: preserve peak signal\n",
    "\n",
    "    return chunk_scores\n",
    "\n",
    "\n",
    "print(\"Core functions defined ✓\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Structured Prompts with Nonce Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_30_110: 200 lines, 1737 context tokens, primary@{'W1': 32, 'W2': 55, 'W3': 78, 'P': 110} decoy@{'W1': 156, 'W2': 162, 'W3': 168, 'P': 175} ✓\n",
      "shift_15_95: 200 lines, 1735 context tokens, primary@{'W1': 15, 'W2': 40, 'W3': 68, 'P': 95} decoy@{'W1': 156, 'W2': 162, 'W3': 168, 'P': 175} ✓\n",
      "shift_50_130: 200 lines, 1736 context tokens, primary@{'W1': 50, 'W2': 75, 'W3': 102, 'P': 130} decoy@{'W1': 156, 'W2': 162, 'W3': 168, 'P': 175} ✓\n",
      "\n",
      "Prompt variants verified ✓\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import random as stdlib_random\n",
    "\n",
    "\n",
    "TOTAL_LINES = 200\n",
    "K = 3\n",
    "\n",
    "QUERY = \"\"\"Identify the highest-severity valid incident. An incident is valid only if exactly 3 prior warnings for that incident appear in the log. Return the incident and its three warnings with their verification codes.\n",
    "\n",
    "Output format requirements:\n",
    "- Output exactly 4 lines and no extra prose.\n",
    "- Use these exact labels: ROOT_CAUSE, WARNING_1, WARNING_2, WARNING_3.\n",
    "- Each value must be a 6-character alphanumeric verification code from the log.\n",
    "\n",
    "ROOT_CAUSE: <6-char code>\n",
    "WARNING_1: <6-char code>\n",
    "WARNING_2: <6-char code>\n",
    "WARNING_3: <6-char code>\"\"\"\n",
    "\n",
    "VARIANT_LAYOUTS = [\n",
    "    {\n",
    "        \"variant\": \"base_30_110\",\n",
    "        \"description\": \"Primary arc centered early (approx lines 30-110).\",\n",
    "        \"primary_lines\": {\"W1\": 32, \"W2\": 55, \"W3\": 78, \"P\": 110},\n",
    "    },\n",
    "    {\n",
    "        \"variant\": \"shift_15_95\",\n",
    "        \"description\": \"Primary arc shifted earlier (approx lines 15-95).\",\n",
    "        \"primary_lines\": {\"W1\": 15, \"W2\": 40, \"W3\": 68, \"P\": 95},\n",
    "    },\n",
    "    {\n",
    "        \"variant\": \"shift_50_130\",\n",
    "        \"description\": \"Primary arc shifted later (approx lines 50-130).\",\n",
    "        \"primary_lines\": {\"W1\": 50, \"W2\": 75, \"W3\": 102, \"P\": 130},\n",
    "    },\n",
    "]\n",
    "\n",
    "DECOY_LINES = {\"W1\": 156, \"W2\": 162, \"W3\": 168, \"P\": 175}\n",
    "\n",
    "\n",
    "def make_nonce(seed_str: str) -> str:\n",
    "    return hashlib.sha256(seed_str.encode()).hexdigest()[:6].upper()\n",
    "\n",
    "\n",
    "def noise_text(line_no: int) -> str:\n",
    "    routine = [\n",
    "        \"heartbeat check passed\",\n",
    "        \"directory sync completed\",\n",
    "        \"endpoint telemetry batch uploaded\",\n",
    "        \"nightly policy bundle validated\",\n",
    "        \"service mesh latency within baseline\",\n",
    "    ]\n",
    "    mid = [\n",
    "        \"interactive login observed\",\n",
    "        \"backup verification completed\",\n",
    "        \"scheduled patch window opened\",\n",
    "        \"IAM token refresh succeeded\",\n",
    "        \"routine vulnerability scan queued\",\n",
    "    ]\n",
    "    evening = [\n",
    "        \"change-control ticket archived\",\n",
    "        \"regional mirror lag normalized\",\n",
    "        \"SIEM digest exported\",\n",
    "        \"off-peak replication checkpointed\",\n",
    "        \"maintenance daemon heartbeat stable\",\n",
    "    ]\n",
    "    generic = [\n",
    "        \"audit log rotation completed\",\n",
    "        \"SLA probe response nominal\",\n",
    "        \"queue depth returned to baseline\",\n",
    "        \"diagnostic ping timeout recovered\",\n",
    "        \"routine service health reported\",\n",
    "    ]\n",
    "\n",
    "    if line_no <= 30:\n",
    "        msg = routine[line_no % len(routine)]\n",
    "    elif 111 <= line_no <= 154:\n",
    "        msg = mid[line_no % len(mid)]\n",
    "    elif 176 <= line_no <= 200:\n",
    "        msg = evening[line_no % len(evening)]\n",
    "    else:\n",
    "        msg = generic[line_no % len(generic)]\n",
    "\n",
    "    return f\"L{line_no:03d} INFO {msg}.\"\n",
    "\n",
    "\n",
    "def build_variant_prompt(layout: dict):\n",
    "    variant = layout[\"variant\"]\n",
    "    primary = layout[\"primary_lines\"]\n",
    "\n",
    "    primary_nonces = {\n",
    "        \"ROOT_CAUSE\": make_nonce(f\"{variant}_primary_root\"),\n",
    "        \"WARNING_1\": make_nonce(f\"{variant}_primary_w1\"),\n",
    "        \"WARNING_2\": make_nonce(f\"{variant}_primary_w2\"),\n",
    "        \"WARNING_3\": make_nonce(f\"{variant}_primary_w3\"),\n",
    "    }\n",
    "    decoy_nonces = {\n",
    "        \"ROOT_CAUSE\": make_nonce(f\"{variant}_decoy_root\"),\n",
    "        \"WARNING_1\": make_nonce(f\"{variant}_decoy_w1\"),\n",
    "        \"WARNING_2\": make_nonce(f\"{variant}_decoy_w2\"),\n",
    "        \"WARNING_3\": make_nonce(f\"{variant}_decoy_w3\"),\n",
    "    }\n",
    "\n",
    "    by_line = {}\n",
    "\n",
    "    # Primary arc (severity 95)\n",
    "    by_line[primary[\"W1\"]] = {\n",
    "        \"role\": \"predecessor\",\n",
    "        \"event_type\": \"warning\",\n",
    "        \"severity\": None,\n",
    "        \"text\": f\"L{primary['W1']:03d} WARN phishing payload executed on HR-LAPTOP (verification code: {primary_nonces['WARNING_1']}).\",\n",
    "    }\n",
    "    by_line[primary[\"W2\"]] = {\n",
    "        \"role\": \"predecessor\",\n",
    "        \"event_type\": \"warning\",\n",
    "        \"severity\": None,\n",
    "        \"text\": f\"L{primary['W2']:03d} WARN lateral movement detected to DB-SERVER (verification code: {primary_nonces['WARNING_2']}).\",\n",
    "    }\n",
    "    by_line[primary[\"W3\"]] = {\n",
    "        \"role\": \"predecessor\",\n",
    "        \"event_type\": \"warning\",\n",
    "        \"severity\": None,\n",
    "        \"text\": f\"L{primary['W3']:03d} WARN admin privilege escalation confirmed on DB-SERVER (verification code: {primary_nonces['WARNING_3']}).\",\n",
    "    }\n",
    "    by_line[primary[\"P\"]] = {\n",
    "        \"role\": \"pivot\",\n",
    "        \"event_type\": \"pivot\",\n",
    "        \"severity\": 95.0,\n",
    "        \"text\": f\"L{primary['P']:03d} CRITICAL APT data breach declared; severity score 95; 2.1TB exfiltrated (verification code: {primary_nonces['ROOT_CAUSE']}).\",\n",
    "    }\n",
    "\n",
    "    # Decoy arc (severity 38)\n",
    "    by_line[DECOY_LINES[\"W1\"]] = {\n",
    "        \"role\": \"decoy_predecessor\",\n",
    "        \"event_type\": \"warning\",\n",
    "        \"severity\": None,\n",
    "        \"text\": f\"L{DECOY_LINES['W1']:03d} WARN CPU temperature spike on cache node (verification code: {decoy_nonces['WARNING_1']}).\",\n",
    "    }\n",
    "    by_line[DECOY_LINES[\"W2\"]] = {\n",
    "        \"role\": \"decoy_predecessor\",\n",
    "        \"event_type\": \"warning\",\n",
    "        \"severity\": None,\n",
    "        \"text\": f\"L{DECOY_LINES['W2']:03d} WARN fan RPM dropping, thermal alert triggered (verification code: {decoy_nonces['WARNING_2']}).\",\n",
    "    }\n",
    "    by_line[DECOY_LINES[\"W3\"]] = {\n",
    "        \"role\": \"decoy_predecessor\",\n",
    "        \"event_type\": \"warning\",\n",
    "        \"severity\": None,\n",
    "        \"text\": f\"L{DECOY_LINES['W3']:03d} WARN thermal throttling engaged on cache node (verification code: {decoy_nonces['WARNING_3']}).\",\n",
    "    }\n",
    "    by_line[DECOY_LINES[\"P\"]] = {\n",
    "        \"role\": \"decoy_pivot\",\n",
    "        \"event_type\": \"pivot\",\n",
    "        \"severity\": 38.0,\n",
    "        \"text\": f\"L{DECOY_LINES['P']:03d} CRITICAL cache node overheated; severity score 38; service degraded (verification code: {decoy_nonces['ROOT_CAUSE']}).\",\n",
    "    }\n",
    "\n",
    "    sentences = []\n",
    "    for line_no in range(1, TOTAL_LINES + 1):\n",
    "        if line_no in by_line:\n",
    "            row = by_line[line_no]\n",
    "            sentences.append({\n",
    "                \"id\": f\"{variant}_L{line_no:03d}\",\n",
    "                \"line_no\": line_no,\n",
    "                \"text\": row[\"text\"],\n",
    "                \"role\": row[\"role\"],\n",
    "                \"event_type\": row[\"event_type\"],\n",
    "                \"severity\": row[\"severity\"],\n",
    "            })\n",
    "        else:\n",
    "            sentences.append({\n",
    "                \"id\": f\"{variant}_L{line_no:03d}\",\n",
    "                \"line_no\": line_no,\n",
    "                \"text\": noise_text(line_no),\n",
    "                \"role\": \"noise\",\n",
    "                \"event_type\": \"noise\",\n",
    "                \"severity\": None,\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"name\": f\"CyberOps Log ({variant})\",\n",
    "        \"variant\": variant,\n",
    "        \"variant_description\": layout[\"description\"],\n",
    "        \"sentences\": sentences,\n",
    "        \"query\": QUERY,\n",
    "        \"k\": K,\n",
    "        \"primary_nonces\": primary_nonces,\n",
    "        \"decoy_nonces\": decoy_nonces,\n",
    "        \"primary_lines\": primary,\n",
    "        \"decoy_lines\": dict(DECOY_LINES),\n",
    "    }\n",
    "\n",
    "\n",
    "def build_text_and_spans(sentences):\n",
    "    parts, spans, pos = [], [], 0\n",
    "    sep = \"\\n\"\n",
    "    for i, sent in enumerate(sentences):\n",
    "        text = sent[\"text\"]\n",
    "        if i > 0:\n",
    "            parts.append(sep)\n",
    "            pos += len(sep)\n",
    "        start = pos\n",
    "        parts.append(text)\n",
    "        pos += len(text)\n",
    "        spans.append((start, pos))\n",
    "    return \"\".join(parts), spans\n",
    "\n",
    "\n",
    "PROMPTS = [build_variant_prompt(layout) for layout in VARIANT_LAYOUTS]\n",
    "\n",
    "for p in PROMPTS:\n",
    "    sents = p[\"sentences\"]\n",
    "    full_text, spans = build_text_and_spans(sents)\n",
    "    ids = [s[\"id\"] for s in sents]\n",
    "    assert len(ids) == len(set(ids)), f\"Duplicate IDs in {p['variant']}\"\n",
    "\n",
    "    for sent, (start, end) in zip(sents, spans):\n",
    "        assert full_text[start:end] == sent[\"text\"], f\"Span mismatch at {sent['id']}\"\n",
    "\n",
    "    for lbl, code in p[\"primary_nonces\"].items():\n",
    "        assert code in full_text, f\"Missing primary nonce {lbl} in {p['variant']}\"\n",
    "    for lbl, code in p[\"decoy_nonces\"].items():\n",
    "        assert code in full_text, f\"Missing decoy nonce {lbl} in {p['variant']}\"\n",
    "\n",
    "    n_tok = len(tokenizer.encode(full_text, add_special_tokens=False))\n",
    "    print(f\"{p['variant']}: {len(sents)} lines, {n_tok} context tokens, primary@{p['primary_lines']} decoy@{p['decoy_lines']} ✓\")\n",
    "\n",
    "print(\"\\nPrompt variants verified ✓\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Eviction Policies & Label-Sensitive Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 frontier + policies + deterministic RCA scorer defined ✓\n",
      "Scorer checks ✓\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Chunk:\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    role: str\n",
    "    event_type: str\n",
    "    l1_weight: float\n",
    "    l1_d_total: int\n",
    "    l1_d_pre: int\n",
    "    line_no: int\n",
    "    char_start: int\n",
    "    char_end: int\n",
    "    token_count: int = 0\n",
    "    prefix_attention_score: float = 0.0\n",
    "    query_attention_score: float = 0.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class L2Summary:\n",
    "    d_total: int\n",
    "    W: List[float]\n",
    "    provenance: List[Optional[dict]]\n",
    "    predecessor_tail: List[str]\n",
    "\n",
    "\n",
    "def identity_summary(k: int) -> L2Summary:\n",
    "    return L2Summary(\n",
    "        d_total=0,\n",
    "        W=[float('-inf')] * (k + 1),\n",
    "        provenance=[None] * (k + 1),\n",
    "        predecessor_tail=[],\n",
    "    )\n",
    "\n",
    "\n",
    "def chunk_summary(chunk: Chunk, k: int) -> L2Summary:\n",
    "    W = [float('-inf')] * (k + 1)\n",
    "    provenance = [None] * (k + 1)\n",
    "\n",
    "    if np.isfinite(chunk.l1_weight):\n",
    "        W[0] = float(chunk.l1_weight)\n",
    "        provenance[0] = {\n",
    "            \"pivot_id\": chunk.chunk_id,\n",
    "            \"pred_ids\": [],\n",
    "            \"weight\": float(chunk.l1_weight),\n",
    "        }\n",
    "\n",
    "    predecessor_tail = [chunk.chunk_id] if chunk.l1_d_total > 0 else []\n",
    "\n",
    "    return L2Summary(\n",
    "        d_total=min(k, chunk.l1_d_total),\n",
    "        W=W,\n",
    "        provenance=provenance,\n",
    "        predecessor_tail=predecessor_tail,\n",
    "    )\n",
    "\n",
    "\n",
    "def compose_summaries(prev: L2Summary, incoming: L2Summary, k: int) -> L2Summary:\n",
    "    \"\"\"\n",
    "    Theorem-style composition:\n",
    "      W_new[j] = max(W_prev[j], W_incoming[max(0, j - d_total_prev)])\n",
    "    with provenance tracking for W[k].\n",
    "    \"\"\"\n",
    "    new_d_total = min(k, prev.d_total + incoming.d_total)\n",
    "    new_W = [float('-inf')] * (k + 1)\n",
    "    new_prov = [None] * (k + 1)\n",
    "\n",
    "    for j in range(k + 1):\n",
    "        cand_prev = prev.W[j]\n",
    "        prov_prev = prev.provenance[j]\n",
    "\n",
    "        idx = max(0, j - prev.d_total)\n",
    "        cand_incoming = incoming.W[idx]\n",
    "        prov_incoming = None\n",
    "\n",
    "        if np.isfinite(cand_incoming) and incoming.provenance[idx] is not None:\n",
    "            needed_from_prev = j - idx\n",
    "            left_preds = prev.predecessor_tail[-needed_from_prev:] if needed_from_prev > 0 else []\n",
    "            pred_ids = left_preds + incoming.provenance[idx][\"pred_ids\"]\n",
    "            if j > 0:\n",
    "                pred_ids = pred_ids[-j:]\n",
    "            else:\n",
    "                pred_ids = []\n",
    "\n",
    "            prov_incoming = {\n",
    "                \"pivot_id\": incoming.provenance[idx][\"pivot_id\"],\n",
    "                \"pred_ids\": pred_ids,\n",
    "                \"weight\": incoming.provenance[idx][\"weight\"],\n",
    "            }\n",
    "\n",
    "        if cand_incoming > cand_prev:\n",
    "            new_W[j] = cand_incoming\n",
    "            new_prov[j] = prov_incoming\n",
    "        else:\n",
    "            new_W[j] = cand_prev\n",
    "            new_prov[j] = prov_prev\n",
    "\n",
    "    new_tail = (prev.predecessor_tail + incoming.predecessor_tail)[-k:]\n",
    "\n",
    "    return L2Summary(\n",
    "        d_total=new_d_total,\n",
    "        W=new_W,\n",
    "        provenance=new_prov,\n",
    "        predecessor_tail=new_tail,\n",
    "    )\n",
    "\n",
    "\n",
    "def l2_frontier_scan(chunks: List[Chunk], k: int) -> L2Summary:\n",
    "    s = identity_summary(k)\n",
    "    for c in chunks:\n",
    "        s = compose_summaries(s, chunk_summary(c, k), k)\n",
    "    return s\n",
    "\n",
    "\n",
    "def l2_protected_set(chunks: List[Chunk], k: int):\n",
    "    summary = l2_frontier_scan(chunks, k)\n",
    "    prov = summary.provenance[k]\n",
    "\n",
    "    protected = set()\n",
    "    if prov is not None:\n",
    "        protected.add(prov[\"pivot_id\"])\n",
    "        protected.update(prov[\"pred_ids\"])\n",
    "\n",
    "    raw_d_total = sum(c.l1_d_total for c in chunks)\n",
    "    contract_rhs = min(raw_d_total, k)\n",
    "    contract_ok = summary.d_total >= contract_rhs\n",
    "    return protected, summary, contract_ok\n",
    "\n",
    "\n",
    "# ── Eviction policies (token-budgeted) ─────────────────────────────────────\n",
    "\n",
    "def _take_until_token_budget(ranked_chunks: List[Chunk], target_evict_tokens: int):\n",
    "    evicted, total_tokens = [], 0\n",
    "    for c in ranked_chunks:\n",
    "        if total_tokens >= target_evict_tokens:\n",
    "            break\n",
    "        evicted.append(c)\n",
    "        total_tokens += c.token_count\n",
    "    return {c.chunk_id for c in evicted}, total_tokens\n",
    "\n",
    "\n",
    "def evict_recency(chunks: List[Chunk], target_evict_tokens: int):\n",
    "    \"\"\"StreamingLLM-style: keep tail, evict earliest chunks first.\"\"\"\n",
    "    earliest_first = sorted(chunks, key=lambda c: c.line_no)\n",
    "    return _take_until_token_budget(earliest_first, target_evict_tokens)\n",
    "\n",
    "\n",
    "def evict_by_score(chunks: List[Chunk], target_evict_tokens: int, score_attr: str):\n",
    "    ranked = sorted(chunks, key=lambda c: getattr(c, score_attr))\n",
    "    return _take_until_token_budget(ranked, target_evict_tokens)\n",
    "\n",
    "\n",
    "def evict_structure_aware(chunks: List[Chunk], target_evict_tokens: int, protected_ids: set):\n",
    "    \"\"\"\n",
    "    L2-guarded policy: evict by lowest prefix score from non-protected pool first.\n",
    "    Breach protected set only if budget exceeds all non-protected tokens.\n",
    "    \"\"\"\n",
    "    non_protected = sorted(\n",
    "        [c for c in chunks if c.chunk_id not in protected_ids],\n",
    "        key=lambda c: c.prefix_attention_score,\n",
    "    )\n",
    "    protected = sorted(\n",
    "        [c for c in chunks if c.chunk_id in protected_ids],\n",
    "        key=lambda c: c.prefix_attention_score,\n",
    "    )\n",
    "\n",
    "    evicted_ids, evicted_tokens = _take_until_token_budget(non_protected, target_evict_tokens)\n",
    "    breached = []\n",
    "\n",
    "    if evicted_tokens < target_evict_tokens:\n",
    "        overflow_target = target_evict_tokens - evicted_tokens\n",
    "        overflow_ids, overflow_tokens = _take_until_token_budget(protected, overflow_target)\n",
    "        evicted_ids |= overflow_ids\n",
    "        evicted_tokens += overflow_tokens\n",
    "        breached = [c.chunk_id for c in protected if c.chunk_id in overflow_ids]\n",
    "\n",
    "    return evicted_ids, evicted_tokens, breached\n",
    "\n",
    "\n",
    "def reconstruct(chunks: List[Chunk], evicted_ids: set) -> str:\n",
    "    return \"\\\\n\".join(c.text for c in chunks if c.chunk_id not in evicted_ids)\n",
    "\n",
    "\n",
    "# ── Deterministic nonce scoring ─────────────────────────────────────────────\n",
    "\n",
    "def parse_rca_lines(response: str) -> dict:\n",
    "    \"\"\"Parse model output into canonical labels, tolerating minor format drift.\"\"\"\n",
    "    pat = r'(?im)^\\s*(?:[-*]\\s*|\\d+[\\.)]\\s*)?([A-Z][A-Z0-9 _-]*?)\\s*[:\\-]\\s*\\[?\\s*([A-Z0-9]{6})\\s*\\]?\\s*$'\n",
    "    pairs = re.findall(pat, response)\n",
    "    out = {}\n",
    "    for raw_label, raw_code in pairs:\n",
    "        norm_label = re.sub(r\"[\\s\\-]+\", \"_\", raw_label.strip().upper())\n",
    "        norm_label = re.sub(r\"_+\", \"_\", norm_label)\n",
    "\n",
    "        if norm_label in {\"ROOTCAUSE\", \"ROOT_CAUSE\"}:\n",
    "            key = \"ROOT_CAUSE\"\n",
    "        else:\n",
    "            m = re.fullmatch(r\"WARNING_?([123])\", norm_label)\n",
    "            if not m:\n",
    "                continue\n",
    "            key = f\"WARNING_{m.group(1)}\"\n",
    "\n",
    "        if key not in out:\n",
    "            out[key] = raw_code.upper()\n",
    "    return out\n",
    "\n",
    "\n",
    "def score_rca_nonce(response: str, primary_nonces: dict, decoy_nonces: dict) -> dict:\n",
    "    labels = [\"ROOT_CAUSE\", \"WARNING_1\", \"WARNING_2\", \"WARNING_3\"]\n",
    "    got = parse_rca_lines(response)\n",
    "\n",
    "    raw_valid = all(lbl in got and re.fullmatch(r\"[A-Z0-9]{6}\", got[lbl]) for lbl in labels)\n",
    "    pivot_preservation = int(raw_valid and got[\"ROOT_CAUSE\"] == primary_nonces[\"ROOT_CAUSE\"])\n",
    "\n",
    "    primary_full = int(raw_valid and all(got[lbl] == primary_nonces[lbl] for lbl in labels))\n",
    "    decoy_full = int(raw_valid and all(got[lbl] == decoy_nonces[lbl] for lbl in labels))\n",
    "    semantic_valid = int(primary_full or decoy_full)\n",
    "\n",
    "    mirage_detected = int(decoy_full)\n",
    "\n",
    "    if primary_full:\n",
    "        mode = \"primary_correct\"\n",
    "    elif decoy_full:\n",
    "        mode = \"mirage_substitution\"\n",
    "    elif raw_valid and pivot_preservation == 0:\n",
    "        mode = \"valid_wrong\"\n",
    "    elif raw_valid:\n",
    "        mode = \"valid_partial\"\n",
    "    else:\n",
    "        mode = \"invalid_format\"\n",
    "\n",
    "    return {\n",
    "        \"raw_validity\": int(raw_valid),\n",
    "        \"pivot_preservation\": pivot_preservation,\n",
    "        \"mirage_detected\": mirage_detected,\n",
    "        \"primary_full\": primary_full,\n",
    "        \"decoy_full\": decoy_full,\n",
    "        \"semantic_valid\": semantic_valid,\n",
    "        \"got\": got,\n",
    "        \"mode\": mode,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"L2 frontier + policies + deterministic RCA scorer defined ✓\")\n",
    "\n",
    "\n",
    "# Scorer sanity checks\n",
    "p = {\"ROOT_CAUSE\": \"AAAAAA\", \"WARNING_1\": \"BBBBBB\", \"WARNING_2\": \"CCCCCC\", \"WARNING_3\": \"DDDDDD\"}\n",
    "d = {\"ROOT_CAUSE\": \"111111\", \"WARNING_1\": \"222222\", \"WARNING_2\": \"333333\", \"WARNING_3\": \"444444\"}\n",
    "\n",
    "s1 = score_rca_nonce(\"ROOT_CAUSE: AAAAAA\\nWARNING_1: BBBBBB\\nWARNING_2: CCCCCC\\nWARNING_3: DDDDDD\", p, d)\n",
    "assert s1[\"raw_validity\"] == 1 and s1[\"primary_full\"] == 1 and s1[\"mirage_detected\"] == 0\n",
    "\n",
    "s2 = score_rca_nonce(\"ROOT_CAUSE: 111111\\nWARNING_1: 222222\\nWARNING_2: 333333\\nWARNING_3: 444444\", p, d)\n",
    "assert s2[\"raw_validity\"] == 1 and s2[\"decoy_full\"] == 1 and s2[\"primary_full\"] == 0 and s2[\"mirage_detected\"] == 1\n",
    "\n",
    "s3 = score_rca_nonce(\"root_cause: aaaaaa\\nwarning_1: bbbbbb\\nwarning_2: cccccc\\nwarning_3: dddddd\", p, d)\n",
    "assert s3[\"raw_validity\"] == 1 and s3[\"primary_full\"] == 1 and s3[\"mirage_detected\"] == 0\n",
    "\n",
    "s4 = score_rca_nonce(\"1) ROOT-CAUSE: AAAAAA\\n2) WARNING 1: BBBBBB\\n3) WARNING-2: CCCCCC\\n4) WARNING_3: DDDDDD\", p, d)\n",
    "assert s4[\"raw_validity\"] == 1 and s4[\"primary_full\"] == 1 and s4[\"mirage_detected\"] == 0\n",
    "\n",
    "print(\"Scorer checks ✓\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the Experiment\n",
    "\n",
    "- Eviction fractions: `0, 20, 35, 50, 60, 70, 75, 80, 85, 90%`\n",
    "- Policies: `Recency`, `H2O-proxy`, `Query-aware Attention`, `Structure-aware (L2-guarded)`\n",
    "- Application channel: **attention-mask ablation with explicit position IDs (RoPE-aware simulation)**\n",
    "- Variants: as many as `PROMPTS` entries (current configuration in code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Prompt variants=3 is small for reliable Wilson interval estimates; publishable studies typically need many more prompts.\n",
      "\n",
      "================================================================================================\n",
      "CyberOps Log (base_30_110) :: base_30_110 :: Primary arc centered early (approx lines 30-110).\n",
      "================================================================================================\n",
      "Context tokens=1737, attributed tokens=1737, unassigned=0\n",
      "L2 contract check: OK\n",
      "Protected lines (k=3): [32, 55, 78, 110]\n",
      "Primary structural floor (safe eviction <=): 94.8%\n",
      "\n",
      "  Fraction 0%: target evict 0/1737 tokens\n",
      "    Recency                  | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=100.0% | mode=invalid_format\n",
      "      invalid-format example: Here is the solution in Python code | ```python | def find_incidentif __main(): | warnings = None | warnings = [] | for line in log = []\n",
      "      parsed labels: []\n",
      "    H2O-proxy                | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=100.0% | mode=invalid_format\n",
      "    Query-aware Attention    | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=100.0% | mode=invalid_format\n",
      "    Structure-aware (L2-guarded) | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=100.0% | mode=invalid_format\n",
      "\n",
      "  Fraction 20%: target evict 347/1737 tokens\n",
      "    Recency                  | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=80.0% | mode=invalid_format\n",
      "    H2O-proxy                | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=80.0% | mode=invalid_format\n",
      "    Query-aware Attention    | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=79.8% | mode=invalid_format\n",
      "    Structure-aware (L2-guarded) | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=80.0% | mode=invalid_format\n",
      "\n",
      "  Fraction 35%: target evict 608/1737 tokens\n",
      "    Recency                  | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=64.9% | mode=invalid_format\n",
      "    H2O-proxy                | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=64.7% | mode=invalid_format\n",
      "    Query-aware Attention    | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=64.5% | mode=invalid_format\n",
      "    Structure-aware (L2-guarded) | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=64.7% | mode=invalid_format\n",
      "\n",
      "  Fraction 50%: target evict 868/1737 tokens\n",
      "    Recency                  | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=49.7% | mode=invalid_format\n",
      "    H2O-proxy                | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=49.7% | mode=invalid_format\n",
      "    Query-aware Attention    | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=49.9% | mode=invalid_format\n",
      "    Structure-aware (L2-guarded) | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=49.7% | mode=invalid_format\n",
      "\n",
      "  Fraction 60%: target evict 1042/1737 tokens\n",
      "    Recency                  | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=39.8% | mode=invalid_format\n",
      "    H2O-proxy                | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=40.0% | mode=invalid_format\n",
      "    Query-aware Attention    | raw=0 | pivot=0 | primary=0 | decoy=0 | mirage=0 | ret=39.0% | mode=invalid_format\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def wilson_ci(successes: int, n: int, z: float = 1.96):\n",
    "    if n == 0:\n",
    "        return (0.0, 0.0)\n",
    "    p = successes / n\n",
    "    denom = 1 + z * z / n\n",
    "    center = (p + z * z / (2 * n)) / denom\n",
    "    margin = z * math.sqrt((p * (1 - p) + z * z / (4 * n)) / n) / denom\n",
    "    return max(0.0, center - margin), min(1.0, center + margin)\n",
    "\n",
    "\n",
    "def annotate_chunk_token_counts(tokenizer, full_text: str, query: str, chunks: List[Chunk]):\n",
    "    \"\"\"\n",
    "    Exact per-chunk token attribution from a shared tokenization view used by masking.\n",
    "    Token budgeting and masking both use the same add_special_tokens + prompt+query encoding.\n",
    "    \"\"\"\n",
    "    combined = full_text + \"\\n\\n\" + query\n",
    "    enc = tokenizer(\n",
    "        combined,\n",
    "        add_special_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "    raw_offsets = enc[\"offset_mapping\"]\n",
    "    if hasattr(raw_offsets, \"tolist\"):\n",
    "        raw_offsets = raw_offsets.tolist()\n",
    "    if raw_offsets and isinstance(raw_offsets[0], (list, tuple)) and raw_offsets[0] and isinstance(raw_offsets[0][0], (list, tuple)):\n",
    "        raw_offsets = raw_offsets[0]\n",
    "    offsets = [tuple(pair) for pair in raw_offsets]\n",
    "\n",
    "    spans = [(c.char_start, c.char_end) for c in chunks]\n",
    "    counts = [0] * len(chunks)\n",
    "    unassigned = 0\n",
    "    context_tokens = 0\n",
    "    context_end = len(full_text)\n",
    "\n",
    "    for tok_start, tok_end in offsets:\n",
    "        if tok_end <= tok_start:\n",
    "            continue\n",
    "        if tok_start >= context_end:\n",
    "            continue\n",
    "\n",
    "        context_tokens += 1\n",
    "        best_idx = None\n",
    "        best_overlap = 0\n",
    "        for idx, (ch_start, ch_end) in enumerate(spans):\n",
    "            overlap = min(tok_end, ch_end) - max(tok_start, ch_start)\n",
    "            if overlap > best_overlap:\n",
    "                best_overlap = overlap\n",
    "                best_idx = idx\n",
    "        if best_idx is None or best_overlap <= 0:\n",
    "            unassigned += 1\n",
    "        else:\n",
    "            counts[best_idx] += 1\n",
    "\n",
    "    for c, cnt in zip(chunks, counts):\n",
    "        c.token_count = cnt\n",
    "\n",
    "    return context_tokens, sum(counts), unassigned\n",
    "\n",
    "\n",
    "EVICTION_FRACS = [0.00, 0.20, 0.35, 0.50, 0.60, 0.70, 0.75, 0.80, 0.85, 0.90]\n",
    "\n",
    "POLICY_NAMES = [\n",
    "    \"Recency\",\n",
    "    \"H2O-proxy\",\n",
    "    \"Query-aware Attention\",\n",
    "    \"Structure-aware (L2-guarded)\",\n",
    "]\n",
    "\n",
    "artifact_dir = ARTIFACT_DIR\n",
    "artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "summary_rows = []\n",
    "retention_floors = {}\n",
    "num_prompt_variants = len(PROMPTS)\n",
    "if num_prompt_variants < 10:\n",
    "    print(f\"[WARNING] Prompt variants={num_prompt_variants} is small for reliable Wilson interval estimates; publishable studies typically need many more prompts.\")\n",
    "\n",
    "\n",
    "for prompt in PROMPTS:\n",
    "    print(f\"\\n{'=' * 96}\")\n",
    "    print(f\"{prompt['name']} :: {prompt['variant']} :: {prompt['variant_description']}\")\n",
    "    print(f\"{'=' * 96}\")\n",
    "\n",
    "    full_text, char_spans = build_text_and_spans(prompt[\"sentences\"])\n",
    "\n",
    "    chunks = []\n",
    "    for sent, (cs, ce) in zip(prompt[\"sentences\"], char_spans):\n",
    "        severity = sent[\"severity\"] if sent[\"severity\"] is not None else float('-inf')\n",
    "        l1_d_total = 1 if sent[\"event_type\"] == \"warning\" else 0\n",
    "        l1_d_pre = l1_d_total\n",
    "\n",
    "        chunks.append(Chunk(\n",
    "            chunk_id=sent[\"id\"],\n",
    "            text=sent[\"text\"],\n",
    "            role=sent[\"role\"],\n",
    "            event_type=sent[\"event_type\"],\n",
    "            l1_weight=float(severity),\n",
    "            l1_d_total=l1_d_total,\n",
    "            l1_d_pre=l1_d_pre,\n",
    "            line_no=sent[\"line_no\"],\n",
    "            char_start=cs,\n",
    "            char_end=ce,\n",
    "        ))\n",
    "\n",
    "    context_tokens, evictable_tokens, unassigned = annotate_chunk_token_counts(tokenizer, full_text, prompt[\"query\"], chunks)\n",
    "    print(f\"Context tokens={context_tokens}, attributed tokens={evictable_tokens}, unassigned={unassigned}\")\n",
    "    if evictable_tokens <= 0:\n",
    "        print(\"  No evictable context tokens found after attribution; skipping this prompt.\")\n",
    "        continue\n",
    "\n",
    "    # L2 protected set from numeric L1 states only\n",
    "    protected_ids, l2_summary, contract_ok = l2_protected_set(chunks, prompt[\"k\"])\n",
    "    protected_lines = sorted([c.line_no for c in chunks if c.chunk_id in protected_ids])\n",
    "    print(f\"L2 contract check: {'OK' if contract_ok else 'FAIL'}\")\n",
    "    print(f\"Protected lines (k={prompt['k']}): {protected_lines}\")\n",
    "\n",
    "    # Theoretical retention floor for primary structural chunks\n",
    "    primary_struct = [c for c in chunks if c.role in {\"predecessor\", \"pivot\"}]\n",
    "    primary_struct_tokens = sum(c.token_count for c in primary_struct)\n",
    "    floor = 1.0 - (primary_struct_tokens / max(1, evictable_tokens))\n",
    "    retention_floors[prompt[\"variant\"]] = floor\n",
    "    print(f\"Primary structural floor (safe eviction <=): {floor:.1%}\")\n",
    "\n",
    "    # One prefix-only and one query-aware attention pass per prompt variant\n",
    "    prefix_scores = get_chunk_attention(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        full_text,\n",
    "        None,\n",
    "        [(c.char_start, c.char_end) for c in chunks],\n",
    "        pool=\"max\",\n",
    "        probe_tokens=64,\n",
    "    )\n",
    "    query_scores = get_chunk_attention(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        full_text,\n",
    "        prompt[\"query\"],\n",
    "        [(c.char_start, c.char_end) for c in chunks],\n",
    "        pool=\"max\",\n",
    "        probe_tokens=64,\n",
    "    )\n",
    "    for c, ps, qs in zip(chunks, prefix_scores, query_scores):\n",
    "        c.prefix_attention_score = ps\n",
    "        c.query_attention_score = qs\n",
    "\n",
    "    shown_invalid_example = False\n",
    "    for frac in EVICTION_FRACS:\n",
    "        budgeted_evict_tokens = int(round(evictable_tokens * frac))\n",
    "        budgeted_evict_tokens = max(0, min(budgeted_evict_tokens, evictable_tokens))\n",
    "        if frac > 0 and budgeted_evict_tokens == 0:\n",
    "            print(f\"\\n  Fraction {frac:.0%}: target evicts {budgeted_evict_tokens}/{evictable_tokens} tokens (rounding no-op).\")\n",
    "        else:\n",
    "            print(f\"\\n  Fraction {frac:.0%}: target evict {budgeted_evict_tokens}/{evictable_tokens} tokens\")\n",
    "\n",
    "        for policy in POLICY_NAMES:\n",
    "            if policy == \"Recency\":\n",
    "                evicted_ids, actual_evict_tokens = evict_recency(chunks, budgeted_evict_tokens)\n",
    "                breach_ids = []\n",
    "            elif policy == \"H2O-proxy\":\n",
    "                evicted_ids, actual_evict_tokens = evict_by_score(chunks, budgeted_evict_tokens, \"prefix_attention_score\")\n",
    "                breach_ids = []\n",
    "            elif policy == \"Query-aware Attention\":\n",
    "                evicted_ids, actual_evict_tokens = evict_by_score(chunks, budgeted_evict_tokens, \"query_attention_score\")\n",
    "                breach_ids = []\n",
    "            else:\n",
    "                evicted_ids, actual_evict_tokens, breach_ids = evict_structure_aware(chunks, budgeted_evict_tokens, protected_ids)\n",
    "\n",
    "            response, realized_evict_tokens, _context_tokens = generate_with_chunk_mask(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                full_text,\n",
    "                prompt[\"query\"],\n",
    "                [c.chunk_id for c in chunks],\n",
    "                [(c.char_start, c.char_end) for c in chunks],\n",
    "                evicted_ids,\n",
    "                max_new_tokens=100,\n",
    "            )\n",
    "            score = score_rca_nonce(response, prompt[\"primary_nonces\"], prompt[\"decoy_nonces\"])\n",
    "\n",
    "            realized_ret = (evictable_tokens - realized_evict_tokens) / max(1, evictable_tokens)\n",
    "\n",
    "            row = {\n",
    "                \"variant\": prompt[\"variant\"],\n",
    "                \"variant_description\": prompt[\"variant_description\"],\n",
    "                \"policy\": policy,\n",
    "                \"fraction\": frac,\n",
    "                \"budgeted_evict_tokens\": budgeted_evict_tokens,\n",
    "                \"target_evict_tokens\": budgeted_evict_tokens,\n",
    "                \"actual_evict_tokens\": realized_evict_tokens,\n",
    "                \"realized_token_retention\": realized_ret,\n",
    "                \"evicted_chunk_ids\": sorted(evicted_ids),\n",
    "                \"l2_breach_chunk_ids\": breach_ids,\n",
    "                \"response\": response,\n",
    "                \"raw_validity\": score[\"raw_validity\"],\n",
    "                \"pivot_preservation\": score[\"pivot_preservation\"],\n",
    "                \"primary_full\": score[\"primary_full\"],\n",
    "                \"decoy_full\": score[\"decoy_full\"],\n",
    "                \"mirage_detected\": score[\"mirage_detected\"],\n",
    "                \"mode\": score[\"mode\"],\n",
    "                \"got_nonces\": score[\"got\"],\n",
    "                \"primary_nonces\": prompt[\"primary_nonces\"],\n",
    "                \"decoy_nonces\": prompt[\"decoy_nonces\"],\n",
    "            }\n",
    "            results.append(row)\n",
    "\n",
    "            print(\n",
    "                f\"    {policy:24s} | raw={score['raw_validity']} | pivot={score['pivot_preservation']} \"\n",
    "                f\"| primary={score['primary_full']} | decoy={score['decoy_full']} | mirage={score['mirage_detected']} | ret={realized_ret:.1%} \"\n",
    "                f\"| mode={score['mode']}\"\n",
    "            )\n",
    "            if score[\"raw_validity\"] == 0 and not shown_invalid_example:\n",
    "                preview = \" | \".join(line.strip() for line in response.strip().splitlines()[:6])\n",
    "                print(f\"      invalid-format example: {preview[:240]}\")\n",
    "                print(f\"      parsed labels: {sorted(score['got'].keys())}\")\n",
    "                shown_invalid_example = True\n",
    "            if policy == \"Structure-aware (L2-guarded)\" and breach_ids:\n",
    "                print(f\"      ⚠ L2 retention floor breached: {breach_ids}\")\n",
    "\n",
    "# Aggregate summary: policy x fraction across prompt variants\n",
    "for policy in POLICY_NAMES:\n",
    "    for frac in EVICTION_FRACS:\n",
    "        subset = [r for r in results if r[\"policy\"] == policy and abs(r[\"fraction\"] - frac) < 1e-12]\n",
    "        if not subset:\n",
    "            continue\n",
    "\n",
    "        n = len(subset)\n",
    "        raw_s = int(sum(r[\"raw_validity\"] for r in subset))\n",
    "        piv_s = int(sum(r[\"pivot_preservation\"] for r in subset))\n",
    "        prim_s = int(sum(r[\"primary_full\"] for r in subset))\n",
    "        dec_s = int(sum(r[\"decoy_full\"] for r in subset))\n",
    "        mir_s = int(sum(r[\"mirage_detected\"] for r in subset))\n",
    "\n",
    "        raw_lo, raw_hi = wilson_ci(raw_s, n)\n",
    "        piv_lo, piv_hi = wilson_ci(piv_s, n)\n",
    "        prim_lo, prim_hi = wilson_ci(prim_s, n)\n",
    "        dec_lo, dec_hi = wilson_ci(dec_s, n)\n",
    "        mir_lo, mir_hi = wilson_ci(mir_s, n)\n",
    "\n",
    "        summary_rows.append({\n",
    "            \"policy\": policy,\n",
    "            \"fraction\": frac,\n",
    "            \"n\": n,\n",
    "            \"raw_validity_mean\": raw_s / n,\n",
    "            \"raw_validity_ci_lo\": raw_lo,\n",
    "            \"raw_validity_ci_hi\": raw_hi,\n",
    "            \"pivot_preservation_mean\": piv_s / n,\n",
    "            \"pivot_preservation_ci_lo\": piv_lo,\n",
    "            \"pivot_preservation_ci_hi\": piv_hi,\n",
    "            \"primary_full_mean\": prim_s / n,\n",
    "            \"primary_full_ci_lo\": prim_lo,\n",
    "            \"primary_full_ci_hi\": prim_hi,\n",
    "            \"decoy_full_mean\": dec_s / n,\n",
    "            \"decoy_full_ci_lo\": dec_lo,\n",
    "            \"decoy_full_ci_hi\": dec_hi,\n",
    "            \"mirage_rate_mean\": mir_s / n,\n",
    "            \"mirage_rate_ci_lo\": mir_lo,\n",
    "            \"mirage_rate_ci_hi\": mir_hi,\n",
    "            \"realized_token_retention_mean\": float(np.mean([r[\"realized_token_retention\"] for r in subset])),\n",
    "        })\n",
    "\n",
    "# Model revision hash best effort\n",
    "model_revision = (\n",
    "    getattr(model.config, \"_commit_hash\", None)\n",
    "    or tokenizer.init_kwargs.get(\"_commit_hash\")\n",
    "    or tokenizer.init_kwargs.get(\"revision\")\n",
    "    or \"unknown\"\n",
    ")\n",
    "\n",
    "config = {\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"model_revision\": model_revision,\n",
    "    \"seed\": SEED,\n",
    "    \"eviction_fractions\": EVICTION_FRACS,\n",
    "    \"policies\": POLICY_NAMES,\n",
    "    \"k\": K,\n",
    "    \"application\": \"attention-mask ablation with explicit position IDs (RoPE-aware simulation)\",\n",
    "    \"prompt_variants\": [\n",
    "        {\n",
    "            \"variant\": p[\"variant\"],\n",
    "            \"description\": p[\"variant_description\"],\n",
    "            \"primary_lines\": p[\"primary_lines\"],\n",
    "            \"decoy_lines\": p[\"decoy_lines\"],\n",
    "        }\n",
    "        for p in PROMPTS\n",
    "    ],\n",
    "    \"env\": env_info,\n",
    "}\n",
    "\n",
    "(artifact_dir / \"config.json\").write_text(json.dumps(config, indent=2))\n",
    "with (artifact_dir / \"results.jsonl\").open(\"w\") as f:\n",
    "    for row in results:\n",
    "        f.write(json.dumps(row) + \"\\n\")\n",
    "with (artifact_dir / \"summary.csv\").open(\"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(\n",
    "        f,\n",
    "        fieldnames=[\n",
    "            \"policy\", \"fraction\", \"n\",\n",
    "            \"raw_validity_mean\", \"raw_validity_ci_lo\", \"raw_validity_ci_hi\",\n",
    "            \"pivot_preservation_mean\", \"pivot_preservation_ci_lo\", \"pivot_preservation_ci_hi\",\n",
    "            \"primary_full_mean\", \"primary_full_ci_lo\", \"primary_full_ci_hi\",\n",
    "            \"decoy_full_mean\", \"decoy_full_ci_lo\", \"decoy_full_ci_hi\",\n",
    "            \"mirage_rate_mean\", \"mirage_rate_ci_lo\", \"mirage_rate_ci_hi\",\n",
    "            \"realized_token_retention_mean\",\n",
    "        ],\n",
    "    )\n",
    "    writer.writeheader()\n",
    "    for row in summary_rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 96)\n",
    "print(\"RUN COMPLETE\")\n",
    "print(f\"Wrote: {(artifact_dir / 'config.json').as_posix()}\")\n",
    "print(f\"Wrote: {(artifact_dir / 'results.jsonl').as_posix()}\")\n",
    "print(f\"Wrote: {(artifact_dir / 'summary.csv').as_posix()}\")\n",
    "print(\"=\" * 96)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 11})\n",
    "\n",
    "COLORS = {\n",
    "    \"Recency\": \"#c0392b\",\n",
    "    \"H2O-proxy\": \"#d35400\",\n",
    "    \"Query-aware Attention\": \"#8e44ad\",\n",
    "    \"Structure-aware (L2-guarded)\": \"#1e8449\",\n",
    "}\n",
    "MARKERS = {\n",
    "    \"Recency\": \"o\",\n",
    "    \"H2O-proxy\": \"s\",\n",
    "    \"Query-aware Attention\": \"^\",\n",
    "    \"Structure-aware (L2-guarded)\": \"D\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_series(policy: str, metric: str, lo: str = None, hi: str = None):\n",
    "    xs, ys, los, his = [], [], [], []\n",
    "    for frac in EVICTION_FRACS:\n",
    "        rows = [r for r in summary_rows if r[\"policy\"] == policy and abs(r[\"fraction\"] - frac) < 1e-12]\n",
    "        if not rows:\n",
    "            continue\n",
    "        row = rows[0]\n",
    "        xs.append(frac)\n",
    "        ys.append(row[metric])\n",
    "        if lo and hi:\n",
    "            los.append(row[lo])\n",
    "            his.append(row[hi])\n",
    "    return xs, ys, los, his\n",
    "\n",
    "\n",
    "# Plot 1: main 2-panel (raw validity + pivot preservation)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "for policy in POLICY_NAMES:\n",
    "    x, y, lo, hi = get_series(policy, \"raw_validity_mean\", \"raw_validity_ci_lo\", \"raw_validity_ci_hi\")\n",
    "    ax.plot(x, y, f\"{MARKERS[policy]}-\", color=COLORS[policy], label=policy, linewidth=2.3, markersize=7)\n",
    "    ax.fill_between(x, lo, hi, color=COLORS[policy], alpha=0.12)\n",
    "ax.set_title(\"Raw Validity vs Eviction\")\n",
    "ax.set_xlabel(\"Eviction Fraction\")\n",
    "ax.set_ylabel(\"Raw Validity\")\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "ax = axes[1]\n",
    "for policy in POLICY_NAMES:\n",
    "    x, y, lo, hi = get_series(policy, \"pivot_preservation_mean\", \"pivot_preservation_ci_lo\", \"pivot_preservation_ci_hi\")\n",
    "    ax.plot(x, y, f\"{MARKERS[policy]}-\", color=COLORS[policy], label=policy, linewidth=2.3, markersize=7)\n",
    "    ax.fill_between(x, lo, hi, color=COLORS[policy], alpha=0.12)\n",
    "ax.set_title(\"Pivot Preservation vs Eviction\")\n",
    "ax.set_xlabel(\"Eviction Fraction\")\n",
    "ax.set_ylabel(\"Pivot Preservation\")\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot1_path = artifact_dir / \"plot1_validity_vs_pivot.png\"\n",
    "plt.savefig(plot1_path, dpi=170, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {plot1_path.as_posix()}\")\n",
    "\n",
    "# Plot 2: mirage rate\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "for policy in POLICY_NAMES:\n",
    "    x, y, lo, hi = get_series(policy, \"mirage_rate_mean\", \"mirage_rate_ci_lo\", \"mirage_rate_ci_hi\")\n",
    "    ax.plot(x, y, f\"{MARKERS[policy]}-\", color=COLORS[policy], label=policy, linewidth=2.3, markersize=7)\n",
    "    ax.fill_between(x, lo, hi, color=COLORS[policy], alpha=0.12)\n",
    "ax.set_title(\"Decoy Substitution Rate (strict coherent substitution)\")\n",
    "ax.set_xlabel(\"Eviction Fraction\")\n",
    "ax.set_ylabel(\"Mirage Rate\")\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plot2_path = artifact_dir / \"plot2_mirage_rate.png\"\n",
    "plt.savefig(plot2_path, dpi=170, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {plot2_path.as_posix()}\")\n",
    "\n",
    "# Plot 3: pivot preservation + retention floor annotation\n",
    "floor_vals = list(retention_floors.values())\n",
    "floor_mean = float(np.mean(floor_vals)) if floor_vals else 0.0\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "for policy in POLICY_NAMES:\n",
    "    x, y, lo, hi = get_series(policy, \"pivot_preservation_mean\", \"pivot_preservation_ci_lo\", \"pivot_preservation_ci_hi\")\n",
    "    ax.plot(x, y, f\"{MARKERS[policy]}-\", color=COLORS[policy], label=policy, linewidth=2.3, markersize=7)\n",
    "    ax.fill_between(x, lo, hi, color=COLORS[policy], alpha=0.10)\n",
    "\n",
    "ax.axvline(floor_mean, color=\"black\", linestyle=\"--\", linewidth=1.4, label=f\"primary retention floor ~ {floor_mean:.2f}\")\n",
    "ax.set_title(\"Pivot Preservation with Theoretical Retention Floor\")\n",
    "ax.set_xlabel(\"Eviction Fraction\")\n",
    "ax.set_ylabel(\"Pivot Preservation\")\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot3_path = artifact_dir / \"plot3_pivot_with_floor.png\"\n",
    "plt.savefig(plot3_path, dpi=170, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {plot3_path.as_posix()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "print(\"=\" * 108)\n",
    "print(f\"TABLE 1: Aggregated Metrics by Policy x Eviction Fraction (n={len(PROMPTS)} variants, Wilson 95% CI)\")\n",
    "print(\"=\" * 108)\n",
    "print(f\"{'Policy':28s} {'Frac':>6s} {'Raw':>16s} {'Primary':>16s} {'Decoy':>16s} {'Mirage':>16s} {'Ret':>8s}\")\n",
    "print(\"-\" * 108)\n",
    "\n",
    "for policy in POLICY_NAMES:\n",
    "    for frac in EVICTION_FRACS:\n",
    "        rows = [r for r in summary_rows if r[\"policy\"] == policy and abs(r[\"fraction\"] - frac) < 1e-12]\n",
    "        if not rows:\n",
    "            continue\n",
    "        r = rows[0]\n",
    "        raw = f\"{r['raw_validity_mean']*100:5.1f}%[{r['raw_validity_ci_lo']*100:4.1f},{r['raw_validity_ci_hi']*100:4.1f}]\"\n",
    "        # pivot formatting retained only when/if needed\n",
    "        prim = f\"{r['primary_full_mean']*100:5.1f}%[{r['primary_full_ci_lo']*100:4.1f},{r['primary_full_ci_hi']*100:4.1f}]\"\n",
    "        dec = f\"{r['decoy_full_mean']*100:5.1f}%[{r['decoy_full_ci_lo']*100:4.1f},{r['decoy_full_ci_hi']*100:4.1f}]\"\n",
    "        mir = f\"{r['mirage_rate_mean']*100:5.1f}%[{r['mirage_rate_ci_lo']*100:4.1f},{r['mirage_rate_ci_hi']*100:4.1f}]\"\n",
    "        print(f\"{policy:28s} {frac:6.0%} {raw:>16s} {prim:>16s} {dec:>16s} {mir:>16s} {r['realized_token_retention_mean']:8.1%}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 108)\n",
    "print(\"TABLE 2: Mode Counts (trial-level)\")\n",
    "print(\"=\" * 108)\n",
    "mode_counts = defaultdict(Counter)\n",
    "for row in results:\n",
    "    mode_counts[row[\"policy\"]][row[\"mode\"]] += 1\n",
    "for policy in POLICY_NAMES:\n",
    "    print(f\"{policy}:\")\n",
    "    for mode, cnt in mode_counts[policy].most_common():\n",
    "        print(f\"  {mode:22s}: {cnt}\")\n",
    "\n",
    "print()\n",
    "print(\"Retention floors by variant:\")\n",
    "for v, f in sorted(retention_floors.items()):\n",
    "    print(f\"  {v:18s}: {f:.2f}\")\n",
    "\n",
    "print()\n",
    "print(\"Artifacts:\")\n",
    "print(f\"  {(artifact_dir / 'config.json').as_posix()}\")\n",
    "print(f\"  {(artifact_dir / 'results.jsonl').as_posix()}\")\n",
    "print(f\"  {(artifact_dir / 'summary.csv').as_posix()}\")\n",
    "print(f\"  {(artifact_dir / 'plot1_validity_vs_pivot.png').as_posix()}\")\n",
    "print(f\"  {(artifact_dir / 'plot2_mirage_rate.png').as_posix()}\")\n",
    "print(f\"  {(artifact_dir / 'plot3_pivot_with_floor.png').as_posix()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interpretation & Limits\n",
    "\n",
    "### What this demo establishes\n",
    "\n",
    "- Structure-blind eviction can keep **raw validity** high while **pivot preservation** drops.\n",
    "- Mirage detection is deterministic (`raw_validity=1`, strict decoy substitution of all 4 fields).\n",
    "- L2-guarded protection is derived from the frontier scan/provenance, not label string matching.\n",
    "\n",
    "### Limitations (explicit)\n",
    "\n",
    "1. This is **attention-mask ablation**, not literal `past_key_values` pruning.\n",
    "2. The prompt set is intentionally small for runtime-constrained demo behavior.\n",
    "3. Query-aware attention is oracle-ish and included as an upper bound, not a streaming deployment policy.\n",
    "\n",
    "### Recommended scale-up path\n",
    "\n",
    "- Expand to many generated incident logs for tighter error bars.\n",
    "- Add real KV-cache pruning implementation for production benchmarks.\n",
    "- Add runtime/cost profiling side-by-side with semantic metrics.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
