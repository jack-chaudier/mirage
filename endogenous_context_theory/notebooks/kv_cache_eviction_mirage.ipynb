{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MirageBench KV Cache Eviction (Colab)\n",
    "\n",
    "This notebook measures how right-truncating KV cache affects MirageBench outputs for `meta-llama/Llama-3.1-8B-Instruct`.\n",
    "\n",
    "Experiment design:\n",
    "- Load MirageBench runtime via `_load_notebook_runtime` + `_patch_runtime_with_methodology_fixes` + `_validate_investment_ground_truth`.\n",
    "- Build the standard 12-task set with `build_miragebench_v01`.\n",
    "- Compute a full-cache baseline answer/pivot for each task.\n",
    "- Re-run each task with KV retentions `[0.7, 0.5, 0.3, 0.1]` by keeping the rightmost cache tokens per layer.\n",
    "- Save per-retention checkpoint CSVs to Google Drive immediately and one final merged CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Install dependencies\n",
    "!pip -q install transformers==4.46.3 accelerate sentence-transformers scikit-learn pandas tqdm\n",
    "print('Dependencies installed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Mount Google Drive + prepare repo path\n",
    "from google.colab import drive\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "REPO_DIR = Path('/content/mirage')\n",
    "if not REPO_DIR.exists():\n",
    "    print('Cloning mirage repo...')\n",
    "    subprocess.run([\n",
    "        'git', 'clone', 'https://github.com/jack-chaudier/mirage.git', str(REPO_DIR)\n",
    "    ], check=True)\n",
    "else:\n",
    "    print(f'Repo already present at {REPO_DIR}')\n",
    "\n",
    "if str(REPO_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_DIR))\n",
    "if str(REPO_DIR / 'endogenous_context_theory') not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_DIR / 'endogenous_context_theory'))\n",
    "\n",
    "print('Python paths configured.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Imports + reproducibility\n",
    "import gc\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "\n",
    "from endogenous_context_theory.scripts.run_miragebench_ollama import (\n",
    "    _load_notebook_runtime,\n",
    "    _patch_runtime_with_methodology_fixes,\n",
    "    _validate_investment_ground_truth,\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "set_seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "if hasattr(torch.backends, 'cudnn'):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f'Torch: {torch.__version__}')\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Load MirageBench runtime exactly as used in blackbox pipeline\n",
    "ROOT = REPO_DIR / 'endogenous_context_theory'\n",
    "NB_PATH = ROOT / 'notebooks' / 'legacy' / 'miragebench_experiments_colab.ipynb'\n",
    "\n",
    "runtime = _load_notebook_runtime(NB_PATH)\n",
    "_patch_runtime_with_methodology_fixes(runtime)\n",
    "\n",
    "build_miragebench_v01 = runtime['build_miragebench_v01']\n",
    "make_prompt = runtime['make_prompt']\n",
    "raw_validity_score = runtime['raw_validity_score']\n",
    "semantic_regret = runtime['semantic_regret']\n",
    "\n",
    "tasks = build_miragebench_v01()\n",
    "_validate_investment_ground_truth(tasks)\n",
    "tasks = tasks[:12]\n",
    "\n",
    "print(f'Loaded {len(tasks)} MirageBench tasks.')\n",
    "print('Task IDs:', [t.task_id for t in tasks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Model + output settings\n",
    "MODEL_ID = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "RETENTIONS = [1.0, 0.7, 0.5, 0.3, 0.1]\n",
    "MAX_NEW_TOKENS = 220\n",
    "SKIP_IF_RETENTION_CSV_EXISTS = True\n",
    "\n",
    "DRIVE_OUT_DIR = Path('/content/drive/MyDrive/miragebench_kv_cache_eviction_mirage')\n",
    "LOCAL_OUT_DIR = Path('/content/miragebench_kv_cache_eviction_mirage')\n",
    "DRIVE_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOCAL_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FINAL_CSV_NAME = 'kv_cache_eviction_mirage_results.csv'\n",
    "SUMMARY_CSV_NAME = 'kv_cache_eviction_mirage_summary_by_retention.csv'\n",
    "\n",
    "print('Model:', MODEL_ID)\n",
    "print('Retentions:', RETENTIONS)\n",
    "print('Drive output:', DRIVE_OUT_DIR)\n",
    "print('Local output:', LOCAL_OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Helpers: pivot extraction, feasibility, cache truncation, greedy decode\n",
    "PIVOT_PRIMARY_RE = re.compile(r'PIVOT_ID\\s*=\\s*([A-Z]\\d{1,4}-E\\d{3})')\n",
    "PIVOT_FALLBACK_RE = re.compile(r'([A-Z]\\d{1,4}-E\\d{3})')\n",
    "\n",
    "def extract_pivot_id(text: str, fallback_candidates: List[str] | None = None) -> str:\n",
    "    if not text:\n",
    "        return ''\n",
    "    m = PIVOT_PRIMARY_RE.search(text)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    markers = PIVOT_FALLBACK_RE.findall(text)\n",
    "    if markers and fallback_candidates:\n",
    "        for c in fallback_candidates:\n",
    "            if c in markers:\n",
    "                return c\n",
    "    return markers[0] if markers else ''\n",
    "\n",
    "def compute_fixed_pivot_feasible(task: Any, full_pivot: str, context_text: str) -> bool:\n",
    "    if not full_pivot:\n",
    "        return False\n",
    "    req_map = task.metadata.get('candidate_requirements', {}) if isinstance(task.metadata, dict) else {}\n",
    "    reqs = req_map.get(full_pivot, [])\n",
    "    if full_pivot not in context_text:\n",
    "        return False\n",
    "    return all(marker in context_text for marker in reqs)\n",
    "\n",
    "def format_chat_prompt(tokenizer, prompt: str) -> str:\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    if hasattr(tokenizer, 'apply_chat_template') and tokenizer.chat_template:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "def to_legacy_past(past_key_values):\n",
    "    if hasattr(past_key_values, 'to_legacy_cache'):\n",
    "        return past_key_values.to_legacy_cache()\n",
    "    return past_key_values\n",
    "\n",
    "def get_model_device(model) -> torch.device:\n",
    "    if hasattr(model, 'device'):\n",
    "        return model.device\n",
    "    return next(model.parameters()).device\n",
    "\n",
    "def past_seq_len(past_key_values) -> int:\n",
    "    return int(past_key_values[0][0].shape[-2])\n",
    "\n",
    "def load_bf16_model(model_id: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "def build_full_prompt_cache(model, tokenizer, prompt: str) -> Dict[str, Any]:\n",
    "    input_text = format_chat_prompt(tokenizer, prompt)\n",
    "    encoded = tokenizer(input_text, return_tensors='pt')\n",
    "    device = get_model_device(model)\n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded.get('attention_mask', torch.ones_like(input_ids)).to(device)\n",
    "    if input_ids.shape[1] < 1:\n",
    "        raise ValueError('Prompt tokenization is empty.')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'last_token': input_ids[:, -1:],\n",
    "        'past_key_values': to_legacy_past(out.past_key_values),\n",
    "    }\n",
    "\n",
    "def truncate_past_middle(past_key_values, retention: float, anchor_len: int = 256):\n",
    "    if retention <= 0 or retention > 1:\n",
    "        raise ValueError(f'retention must be in (0,1], got {retention}')\n",
    "    full_len = past_seq_len(past_key_values)\n",
    "    keep_total = max(anchor_len + 1, int(retention * full_len))\n",
    "    tail_len = keep_total - anchor_len\n",
    "\n",
    "    new_past = []\n",
    "    for layer in past_key_values:\n",
    "        key, value = layer[0], layer[1]\n",
    "        head = key.narrow(-2, 0, anchor_len)\n",
    "        tail = key.narrow(-2, full_len - tail_len, tail_len)\n",
    "        new_key = torch.cat([head, tail], dim=-2).contiguous()\n",
    "\n",
    "        head_v = value.narrow(-2, 0, anchor_len)\n",
    "        tail_v = value.narrow(-2, full_len - tail_len, tail_len)\n",
    "        new_value = torch.cat([head_v, tail_v], dim=-2).contiguous()\n",
    "\n",
    "        if len(layer) == 2:\n",
    "            new_past.append((new_key, new_value))\n",
    "        else:\n",
    "            new_past.append((new_key, new_value, *layer[2:]))\n",
    "    return tuple(new_past), keep_total, full_len\n",
    "\n",
    "\n",
    "def greedy_decode_from_cache(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    past_key_values,\n",
    "    continuation_input_ids,\n",
    "    max_new_tokens: int = MAX_NEW_TOKENS,\n",
    ") -> str:\n",
    "    past = to_legacy_past(past_key_values)\n",
    "    input_token = continuation_input_ids\n",
    "    generated_ids: List[int] = []\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            cache_len = past_seq_len(past)\n",
    "            attention_mask = torch.ones(\n",
    "                (input_token.shape[0], cache_len + input_token.shape[1]),\n",
    "                dtype=torch.long,\n",
    "                device=input_token.device,\n",
    "            )\n",
    "            out = model(\n",
    "                input_ids=input_token,\n",
    "                past_key_values=past,\n",
    "                attention_mask=attention_mask,\n",
    "                use_cache=True,\n",
    "            )\n",
    "            past = to_legacy_past(out.past_key_values)\n",
    "            next_token = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            token_id = int(next_token.item())\n",
    "            generated_ids.append(token_id)\n",
    "            input_token = next_token\n",
    "            if eos_id is not None and token_id == eos_id:\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Load model and compute full-cache baselines for all 12 tasks\n",
    "model, tokenizer = load_bf16_model(MODEL_ID)\n",
    "\n",
    "baseline_by_task: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "for task in tqdm(tasks, desc='Full-cache baseline'):\n",
    "    full_prompt = make_prompt(task.full_context, task.question)\n",
    "    cache = build_full_prompt_cache(model, tokenizer, full_prompt)\n",
    "    full_answer = greedy_decode_from_cache(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        past_key_values=cache['past_key_values'],\n",
    "        continuation_input_ids=cache['last_token'],\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "    )\n",
    "    full_pivot = extract_pivot_id(full_answer, [task.pivot_ground_truth, task.decoy_pivot])\n",
    "    baseline_by_task[task.task_id] = {\n",
    "        'full_prompt': full_prompt,\n",
    "        'full_answer': full_answer,\n",
    "        'full_pivot': full_pivot,\n",
    "    }\n",
    "\n",
    "    del cache\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print('Computed baselines:', len(baseline_by_task))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Retention sweep with per-retention checkpoint CSVs\n",
    "all_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for retention in RETENTIONS:\n",
    "    retention_slug = str(retention).replace('.', 'p')\n",
    "    local_ckpt = LOCAL_OUT_DIR / f'kv_cache_eviction_retention_{retention_slug}.csv'\n",
    "    drive_ckpt = DRIVE_OUT_DIR / f'kv_cache_eviction_retention_{retention_slug}.csv'\n",
    "\n",
    "    if SKIP_IF_RETENTION_CSV_EXISTS and drive_ckpt.exists():\n",
    "        print(f'Loading existing checkpoint for retention={retention}: {drive_ckpt}')\n",
    "        cached_df = pd.read_csv(drive_ckpt)\n",
    "        all_rows.extend(cached_df.to_dict(orient='records'))\n",
    "        continue\n",
    "\n",
    "    retention_rows: List[Dict[str, Any]] = []\n",
    "    for task in tqdm(tasks, desc=f'Retention {retention:.1f}'):\n",
    "        baseline = baseline_by_task[task.task_id]\n",
    "        cache = build_full_prompt_cache(model, tokenizer, baseline['full_prompt'])\n",
    "        truncated_past, kept_len, full_len = truncate_past_middle(cache['past_key_values'], retention=retention)\n",
    "        truncated_answer = greedy_decode_from_cache(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            past_key_values=truncated_past,\n",
    "            continuation_input_ids=cache['last_token'],\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "        )\n",
    "\n",
    "        compressed_pivot = extract_pivot_id(\n",
    "            truncated_answer,\n",
    "            [task.pivot_ground_truth, task.decoy_pivot],\n",
    "        )\n",
    "        row = {\n",
    "            'task_id': task.task_id,\n",
    "            'category': task.category,\n",
    "            'retention': float(retention),\n",
    "            'full_pivot': baseline['full_pivot'],\n",
    "            'compressed_pivot': compressed_pivot,\n",
    "            'has_pivot_header': int(bool(PIVOT_PRIMARY_RE.search(truncated_answer))),\n",
    "            'pivot_preserved': int(bool(baseline['full_pivot'] and compressed_pivot and baseline['full_pivot'] == compressed_pivot)),\n",
    "            'fixed_pivot_feasible': int(compute_fixed_pivot_feasible(task, baseline['full_pivot'], task.full_context)),\n",
    "            'raw_validity': float(raw_validity_score(truncated_answer, task)),\n",
    "            'semantic_regret': float(semantic_regret(baseline['full_answer'], truncated_answer)),\n",
    "            'full_answer': baseline['full_answer'],\n",
    "            'truncated_answer': truncated_answer,\n",
    "            'cache_tokens_full': int(full_len),\n",
    "            'cache_tokens_kept': int(kept_len),\n",
    "        }\n",
    "        retention_rows.append(row)\n",
    "\n",
    "        del cache\n",
    "        del truncated_past\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    retention_df = pd.DataFrame(retention_rows)\n",
    "    retention_df.to_csv(local_ckpt, index=False)\n",
    "    retention_df.to_csv(drive_ckpt, index=False)\n",
    "    print(f'Saved retention checkpoint for {retention:.1f}:')\n",
    "    print('  Local:', local_ckpt)\n",
    "    print('  Drive:', drive_ckpt)\n",
    "\n",
    "    all_rows.extend(retention_rows)\n",
    "\n",
    "results_df = pd.DataFrame(all_rows).sort_values(['retention', 'task_id']).reset_index(drop=True)\n",
    "\n",
    "final_local = LOCAL_OUT_DIR / FINAL_CSV_NAME\n",
    "final_drive = DRIVE_OUT_DIR / FINAL_CSV_NAME\n",
    "results_df.to_csv(final_local, index=False)\n",
    "results_df.to_csv(final_drive, index=False)\n",
    "\n",
    "print('\\nSaved final merged CSV:')\n",
    "print('  Local:', final_local)\n",
    "print('  Drive:', final_drive)\n",
    "print('Rows:', len(results_df))\n",
    "\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Summary by retention\n",
    "# Backfill header-presence metric for legacy checkpoints that predate this column.\n",
    "if 'has_pivot_header' not in results_df.columns:\n",
    "    results_df['has_pivot_header'] = results_df['truncated_answer'].fillna('').str.contains(r'PIVOT_ID\\s*=', regex=True).astype(int)\n",
    "else:\n",
    "    backfill = results_df['truncated_answer'].fillna('').str.contains(r'PIVOT_ID\\s*=', regex=True)\n",
    "    results_df['has_pivot_header'] = results_df['has_pivot_header'].fillna(backfill).astype(int)\n",
    "\n",
    "summary = (\n",
    "    results_df.groupby('retention', as_index=False)\n",
    "    .agg(\n",
    "        has_pivot_header=('has_pivot_header', 'mean'),\n",
    "        pivot_preserved=('pivot_preserved', 'mean'),\n",
    "        fixed_pivot_feasible=('fixed_pivot_feasible', 'mean'),\n",
    "        raw_validity=('raw_validity', 'mean'),\n",
    "        semantic_regret=('semantic_regret', 'mean'),\n",
    "    )\n",
    "    .sort_values('retention', ascending=False)\n",
    ")\n",
    "\n",
    "summary_local = LOCAL_OUT_DIR / SUMMARY_CSV_NAME\n",
    "summary_drive = DRIVE_OUT_DIR / SUMMARY_CSV_NAME\n",
    "summary.to_csv(summary_local, index=False)\n",
    "summary.to_csv(summary_drive, index=False)\n",
    "\n",
    "print('Saved summary CSV:')\n",
    "print('  Local:', summary_local)\n",
    "print('  Drive:', summary_drive)\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KV API Compatibility Notes\n",
    "\n",
    "- This notebook converts cache objects via `to_legacy_cache()` when available so truncation can operate on per-layer `(key, value)` tensors.\n",
    "- If a future `transformers` release changes cache container types/shapes, update `to_legacy_past` and `truncate_past_middle` accordingly.\n",
    "- The decode path intentionally uses minimal continuation input (`last prompt token`) with an attention mask sized as `kept_cache_len + current_input_len` on each step.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}