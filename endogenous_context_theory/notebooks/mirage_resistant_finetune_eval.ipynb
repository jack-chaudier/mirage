{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Mirage-Resistant Finetune + Eval (H100 Overnight)\n\nEnd-to-end Colab pipeline with three phases:\n1. Mine base-model failures into SFT + DPO preference data.\n2. LoRA fine-tune `meta-llama/Llama-3.1-8B-Instruct`.\n3. Evaluate on harder held-out MirageBench-style tasks.\n\nAll generation is greedy (`do_sample=False`, `max_new_tokens=512`), fully checkpointed to Google Drive."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 1 — Install dependencies\n!pip -q install -U \"transformers>=4.46.3\" \"peft>=0.13.2\" \"trl>=0.12.2\" datasets accelerate sentencepiece scipy scikit-learn\nprint('Dependencies installed.')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 2 — Mount Drive and configure repo path\nfrom google.colab import drive\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndrive.mount('/content/drive')\n\nREPO_DIR = Path('/content/mirage')\nZIP_FALLBACK = Path('/content/drive/MyDrive/mirage_repo_for_colab_clean.zip')\n\nif not REPO_DIR.exists() and ZIP_FALLBACK.exists():\n    print(f'Unzipping repo from {ZIP_FALLBACK} ...')\n    subprocess.run(['unzip', '-q', str(ZIP_FALLBACK), '-d', '/content'], check=True)\n\nif not REPO_DIR.exists():\n    raise FileNotFoundError(\n        'Repo not found at /content/mirage. Upload/unzip the repo there, then re-run this cell.'\n    )\n\nif str(REPO_DIR) not in sys.path:\n    sys.path.insert(0, str(REPO_DIR))\nif str(REPO_DIR / 'endogenous_context_theory') not in sys.path:\n    sys.path.insert(0, str(REPO_DIR / 'endogenous_context_theory'))\n\nprint('Repo ready at', REPO_DIR)\nprint('Python paths configured.')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 3 — Imports, seeds, GPU sanity\nimport gc\nimport json\nimport os\nimport random\nimport re\nimport shutil\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom datasets import Dataset, DatasetDict\nfrom tqdm.auto import tqdm\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nprint(f'Torch: {torch.__version__}')\nprint('CUDA available:', torch.cuda.is_available())\nif torch.cuda.is_available():\n    print('GPU:', torch.cuda.get_device_name(0))\n    total_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f'GPU memory: {total_mem_gb:.1f} GB')\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 4 — Load MirageBench runtime + tasks from JSON\nfrom endogenous_context_theory.scripts.run_miragebench_ollama import (\n    _load_notebook_runtime,\n    _patch_runtime_with_methodology_fixes,\n    _validate_investment_ground_truth,\n)\n\nROOT = REPO_DIR / 'endogenous_context_theory'\nNB_PATH = ROOT / 'notebooks' / 'legacy' / 'miragebench_experiments_colab.ipynb'\nTASKS_PATH = ROOT / 'release' / 'miragebench_tasks' / 'miragebench_v01_tasks.json'\n\nruntime = _load_notebook_runtime(NB_PATH)\n_patch_runtime_with_methodology_fixes(runtime)\n\nMirageBenchTask = runtime['MirageBenchTask']\nmake_prompt = runtime['make_prompt']\nrender_compressed_variant = runtime['render_compressed_variant']\nraw_validity_score = runtime['raw_validity_score']\nsemantic_regret = runtime['semantic_regret']\n\nwith open(TASKS_PATH, 'r') as f:\n    task_dicts = json.load(f)\n\ntasks = [MirageBenchTask(**row) for row in task_dicts]\n_validate_investment_ground_truth(tasks)\n\nMODEL_ID = 'meta-llama/Llama-3.1-8B-Instruct'\nRETENTION_LEVELS = [1.0, 0.7, 0.5, 0.4, 0.3]\nMAX_NEW_TOKENS = 512\n\nPHASE1_OUT = Path('/content/mirage_finetune_data')\nPHASE1_DRIVE_OUT = Path('/content/drive/MyDrive/mirage_finetune_data')\nADAPTER_OUT_DIR = Path('/content/drive/MyDrive/mirage_resistant_adapter')\nEVAL_OUT_DIR = Path('/content/drive/MyDrive/mirage_resistant_eval')\n\nfor p in [PHASE1_OUT, PHASE1_DRIVE_OUT, ADAPTER_OUT_DIR, EVAL_OUT_DIR]:\n    p.mkdir(parents=True, exist_ok=True)\n\nprint(f'Loaded {len(tasks)} tasks from {TASKS_PATH}')\nprint('Task IDs:', [t.task_id for t in tasks])\nprint('Retention levels:', RETENTION_LEVELS)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 5 — Load base model + shared helper functions\nPIVOT_PRIMARY_RE = re.compile(r'PIVOT_ID\\s*=\\s*([A-Z]{1,5}\\d{1,4}-E\\d{3})')\nPIVOT_FALLBACK_RE = re.compile(r'([A-Z]{1,5}\\d{1,4}-E\\d{3})')\n\n\ndef gpu_mem_report(label: str) -> None:\n    if not torch.cuda.is_available():\n        return\n    allocated = torch.cuda.memory_allocated() / 1e9\n    reserved = torch.cuda.memory_reserved() / 1e9\n    print(f'[{label}] GPU allocated={allocated:.2f} GB, reserved={reserved:.2f} GB')\n\n\ndef extract_pivot_id(text: str, fallback_candidates: Optional[List[str]] = None) -> str:\n    if not text:\n        return ''\n    m = PIVOT_PRIMARY_RE.search(text)\n    if m:\n        return m.group(1)\n    markers = PIVOT_FALLBACK_RE.findall(text)\n    if markers and fallback_candidates:\n        for candidate in fallback_candidates:\n            if candidate in markers:\n                return candidate\n    return markers[0] if markers else ''\n\n\ndef compute_fixed_pivot_feasible(task: Any, full_pivot: str, context_text: str) -> bool:\n    if not full_pivot:\n        return False\n    req_map = task.metadata.get('candidate_requirements', {}) if isinstance(task.metadata, dict) else {}\n    reqs = req_map.get(full_pivot, [])\n    if full_pivot not in context_text:\n        return False\n    return all(marker in context_text for marker in reqs)\n\n\ndef format_chat_prompt(tokenizer, prompt: str) -> str:\n    messages = [{'role': 'user', 'content': prompt}]\n    if hasattr(tokenizer, 'apply_chat_template') and tokenizer.chat_template:\n        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    return prompt\n\n\ndef generate_response(model, tokenizer, prompt: str, max_new_tokens: int = MAX_NEW_TOKENS) -> str:\n    input_text = format_chat_prompt(tokenizer, prompt)\n    inputs = tokenizer(input_text, return_tensors='pt').to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=False,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n    new_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n    return tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n\n\ndef get_record_line(task: Any, marker: str) -> str:\n    for rec in task.metadata.get('records', []):\n        if rec.get('marker') == marker:\n            return rec.get('line', '')\n    return ''\n\n\ndef summarize_pivot_score(task: Any, marker: str) -> str:\n    line = get_record_line(task, marker)\n    if not line:\n        return 'dominant composite score'\n\n    m = re.search(r'Composite=([0-9]+(?:\\.[0-9]+)?)', line)\n    if m:\n        return f'Composite={m.group(1)}'\n\n    m = re.search(r'CumulativeReturn=([+-]?[0-9]+(?:\\.[0-9]+)?)%', line)\n    if m:\n        return f'CumulativeReturn={m.group(1)}%'\n\n    m = re.search(r'ConsequenceScore=([0-9]+(?:\\.[0-9]+)?)', line)\n    if m:\n        return f'ConsequenceScore={m.group(1)}'\n\n    return 'dominant score'\n\n\ndef build_chosen_completion(task: Any, correct_pivot: str, prereqs: List[str], missing_prereqs: List[str]) -> str:\n    score_text = summarize_pivot_score(task, correct_pivot)\n    prereq_chain = ' -> '.join(prereqs + [correct_pivot]) if prereqs else correct_pivot\n\n    lines = [\n        f'PIVOT_ID={correct_pivot}',\n        f'The correct pivot is {correct_pivot} because it has the highest {score_text} in the full timeline for this task.',\n        f'The prerequisite chain supporting this pivot is {prereq_chain}, which preserves the endogenous turning-point semantics.',\n    ]\n\n    if missing_prereqs:\n        missing_fmt = ', '.join(missing_prereqs)\n        lines.append(\n            f'[PREREQUISITE_GAP: events {{{missing_fmt}}} not present in compressed context. Confidence degraded.]'\n        )\n\n    return '\\n\\n'.join(lines)\n\n\nprint(f'Loading base model: {MODEL_ID}')\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    device_map='auto',\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n)\nbase_model.eval()\n\ngpu_mem_report('after base model load')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 6 — Phase 1: mine negatives and build SFT/DPO dataset\nrows: List[Dict[str, Any]] = []\nby_level = defaultdict(lambda: {'pairs': 0, 'sft_only': 0, 'total': 0})\n\nfor task in tqdm(tasks, desc='Mining base failures (task loop)'):\n    correct_pivot = task.pivot_ground_truth\n    req_map = task.metadata.get('candidate_requirements', {})\n    correct_reqs = list(req_map.get(correct_pivot, []))\n\n    for retention in tqdm(RETENTION_LEVELS, leave=False, desc=f'{task.task_id} retentions'):\n        drop_fraction = max(0.0, min(1.0, 1.0 - retention))\n        context_text = task.full_context if retention >= 0.999 else render_compressed_variant(\n            task,\n            drop_fraction=drop_fraction,\n            seed=SEED,\n        )\n        prompt = make_prompt(context_text, task.question)\n        base_completion = generate_response(base_model, tokenizer, prompt)\n        base_pivot = extract_pivot_id(base_completion, [task.pivot_ground_truth, task.decoy_pivot])\n        base_was_correct = bool(base_pivot == correct_pivot)\n\n        missing = [m for m in correct_reqs if m not in context_text]\n        chosen = build_chosen_completion(task, correct_pivot, correct_reqs, missing)\n        rejected = None if base_was_correct else base_completion\n\n        rows.append(\n            {\n                'prompt': prompt,\n                'chosen': chosen,\n                'rejected': rejected,\n                'task_id': task.task_id,\n                'compression': float(retention),\n                'base_pivot': base_pivot,\n                'correct_pivot': correct_pivot,\n                'base_was_correct': int(base_was_correct),\n            }\n        )\n\n        by_level[retention]['total'] += 1\n        if base_was_correct:\n            by_level[retention]['sft_only'] += 1\n        else:\n            by_level[retention]['pairs'] += 1\n\nphase1_df = pd.DataFrame(rows)\nordered_cols = [\n    'prompt', 'chosen', 'rejected', 'task_id', 'compression',\n    'base_pivot', 'correct_pivot', 'base_was_correct'\n]\nphase1_df = phase1_df[ordered_cols]\n\npair_df = phase1_df[phase1_df['rejected'].notna()].copy()\nsft_df = phase1_df.copy()\n\nphase1_ds = Dataset.from_pandas(phase1_df, preserve_index=False)\npair_ds = Dataset.from_pandas(pair_df, preserve_index=False)\nsft_ds = Dataset.from_pandas(sft_df[['prompt', 'chosen', 'task_id', 'compression']], preserve_index=False)\n\nbundle = DatasetDict({'all': phase1_ds, 'pairs': pair_ds, 'sft': sft_ds})\nif PHASE1_OUT.exists():\n    shutil.rmtree(PHASE1_OUT)\nbundle.save_to_disk(str(PHASE1_OUT))\n\nif PHASE1_DRIVE_OUT.exists():\n    shutil.rmtree(PHASE1_DRIVE_OUT)\nshutil.copytree(PHASE1_OUT, PHASE1_DRIVE_OUT)\n\nphase1_csv = PHASE1_OUT / 'phase1_examples.csv'\nphase1_df.to_csv(phase1_csv, index=False)\nphase1_df.to_csv(PHASE1_DRIVE_OUT / 'phase1_examples.csv', index=False)\n\nprint('=' * 72)\nprint('PHASE 1 SUMMARY')\nprint('=' * 72)\nprint(f'Total examples: {len(phase1_df)}')\nprint(f'Preference pairs: {len(pair_df)}')\nprint(f'SFT-only examples: {len(phase1_df) - len(pair_df)}')\nprint('\\nBy retention level:')\nfor lvl in RETENTION_LEVELS:\n    s = by_level[lvl]\n    print(f'  retention={lvl:.1f}: total={s[\"total\"]}, pairs={s[\"pairs\"]}, sft_only={s[\"sft_only\"]}')\n\nprint(f'\\nSaved HF dataset to: {PHASE1_OUT}')\nprint(f'Backed up to Drive:   {PHASE1_DRIVE_OUT}')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 7 — Phase 2: prepare LoRA + training datasets\nfrom peft import LoraConfig, TaskType, get_peft_model\n\n# Convert chosen completions into chat-formatted SFT text\n\ndef to_chat_text(prompt: str, completion: str) -> str:\n    messages = [\n        {'role': 'user', 'content': prompt},\n        {'role': 'assistant', 'content': completion},\n    ]\n    if hasattr(tokenizer, 'apply_chat_template') and tokenizer.chat_template:\n        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n    return f'User:\\n{prompt}\\n\\nAssistant:\\n{completion}'\n\nsft_train_df = phase1_df[['prompt', 'chosen']].copy()\nsft_train_df['text'] = [to_chat_text(p, c) for p, c in zip(sft_train_df['prompt'], sft_train_df['chosen'])]\nsft_train_dataset = Dataset.from_pandas(sft_train_df[['text']], preserve_index=False)\n\ndpo_train_df = pair_df[['prompt', 'chosen', 'rejected']].copy()\ndpo_train_dataset = Dataset.from_pandas(dpo_train_df, preserve_index=False)\n\nlora_targets = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\nmatched_modules = [\n    name for name, _ in base_model.named_modules()\n    if any(name.endswith(t) for t in lora_targets)\n]\nprint(f'Matched LoRA modules: {len(matched_modules)}')\nif len(matched_modules) == 0:\n    raise RuntimeError('No LoRA target modules matched. Aborting to avoid null training run.')\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    target_modules=lora_targets,\n    task_type=TaskType.CAUSAL_LM,\n    bias='none',\n)\n\nft_model = get_peft_model(base_model, lora_config)\nft_model.print_trainable_parameters()\n\ngpu_mem_report('before training')\nprint(f'DPO candidate pairs: {len(dpo_train_dataset)}')\nprint(f'SFT samples: {len(sft_train_dataset)}')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 8 — Train (DPO if viable) then SFT (TRL-version compatible)\nimport inspect\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n\ntry:\n    from trl import DPOTrainer\n    HAS_DPO = True\nexcept Exception:\n    HAS_DPO = False\n\ntry:\n    from trl import SFTConfig\nexcept Exception:\n    SFTConfig = None\n\ntry:\n    from trl import DPOConfig\nexcept Exception:\n    DPOConfig = None\n\nDPO_MIN_PAIRS = 15\nRUN_DPO = HAS_DPO and (len(dpo_train_dataset) >= DPO_MIN_PAIRS)\nDPO_BETA = 0.1\nWARMUP_STEPS = 10\n\nprint(f'HAS_DPO={HAS_DPO}, RUN_DPO={RUN_DPO}, pair_count={len(dpo_train_dataset)}')\n\nDPO_OUT = ADAPTER_OUT_DIR / 'dpo_stage'\nSFT_OUT = ADAPTER_OUT_DIR / 'sft_stage'\nDPO_OUT.mkdir(parents=True, exist_ok=True)\nSFT_OUT.mkdir(parents=True, exist_ok=True)\n\ndpo_ran = False\nif RUN_DPO:\n    try:\n        if DPOConfig is not None:\n            dpo_args = DPOConfig(\n                output_dir=str(DPO_OUT),\n                num_train_epochs=1,\n                per_device_train_batch_size=1,\n                gradient_accumulation_steps=4,\n                learning_rate=2e-4,\n                lr_scheduler_type='cosine',\n                warmup_steps=WARMUP_STEPS,\n                bf16=True,\n                logging_steps=5,\n                save_strategy='epoch',\n                save_total_limit=3,\n                report_to='none',\n                beta=DPO_BETA,\n                max_length=2560,\n                max_prompt_length=2048,\n            )\n        else:\n            dpo_args = TrainingArguments(\n                output_dir=str(DPO_OUT),\n                num_train_epochs=1,\n                per_device_train_batch_size=1,\n                gradient_accumulation_steps=4,\n                learning_rate=2e-4,\n                lr_scheduler_type='cosine',\n                warmup_steps=WARMUP_STEPS,\n                bf16=True,\n                logging_steps=5,\n                save_strategy='epoch',\n                save_total_limit=3,\n                report_to='none',\n            )\n\n        dpo_kwargs = {\n            'model': ft_model,\n            'ref_model': None,\n            'args': dpo_args,\n            'train_dataset': dpo_train_dataset,\n        }\n\n        dpo_sig = inspect.signature(DPOTrainer.__init__)\n        if 'beta' in dpo_sig.parameters and DPOConfig is None:\n            dpo_kwargs['beta'] = DPO_BETA\n        if 'tokenizer' in dpo_sig.parameters:\n            dpo_kwargs['tokenizer'] = tokenizer\n        elif 'processing_class' in dpo_sig.parameters:\n            dpo_kwargs['processing_class'] = tokenizer\n        if 'max_length' in dpo_sig.parameters and DPOConfig is None:\n            dpo_kwargs['max_length'] = 2560\n        if 'max_prompt_length' in dpo_sig.parameters and DPOConfig is None:\n            dpo_kwargs['max_prompt_length'] = 2048\n\n        dpo_trainer = DPOTrainer(**dpo_kwargs)\n        dpo_trainer.train()\n        ft_model = dpo_trainer.model\n        dpo_ran = True\n        print('DPO stage complete.')\n    except Exception as exc:\n        print(f'DPO stage failed; falling back to SFT-only. Error: {exc}')\n        dpo_ran = False\nelse:\n    print('Skipping DPO stage; running SFT-only path.')\n\nsft_epochs = 1 if dpo_ran else 3\nprint(f'Running SFT for {sft_epochs} epoch(s).')\n\nif SFTConfig is not None:\n    sft_args = SFTConfig(\n        output_dir=str(SFT_OUT),\n        num_train_epochs=sft_epochs,\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        learning_rate=2e-4,\n        lr_scheduler_type='cosine',\n        warmup_steps=WARMUP_STEPS,\n        bf16=True,\n        logging_steps=5,\n        save_strategy='epoch',\n        save_total_limit=5,\n        report_to='none',\n        max_seq_length=2560,\n        dataset_text_field='text',\n    )\nelse:\n    sft_args = TrainingArguments(\n        output_dir=str(SFT_OUT),\n        num_train_epochs=sft_epochs,\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        learning_rate=2e-4,\n        lr_scheduler_type='cosine',\n        warmup_steps=WARMUP_STEPS,\n        bf16=True,\n        logging_steps=5,\n        save_strategy='epoch',\n        save_total_limit=5,\n        report_to='none',\n    )\n\nsft_kwargs = {\n    'model': ft_model,\n    'args': sft_args,\n    'train_dataset': sft_train_dataset,\n}\n\nsft_sig = inspect.signature(SFTTrainer.__init__)\nif 'tokenizer' in sft_sig.parameters:\n    sft_kwargs['tokenizer'] = tokenizer\nelif 'processing_class' in sft_sig.parameters:\n    sft_kwargs['processing_class'] = tokenizer\n\nif 'dataset_text_field' in sft_sig.parameters and SFTConfig is None:\n    sft_kwargs['dataset_text_field'] = 'text'\nif 'max_seq_length' in sft_sig.parameters and SFTConfig is None:\n    sft_kwargs['max_seq_length'] = 2560\n\ntry:\n    sft_trainer = SFTTrainer(**sft_kwargs)\n    sft_trainer.train()\n    ft_model = sft_trainer.model\n    print('SFT stage complete.')\nexcept Exception as exc:\n    print(f'SFTTrainer API mismatch, falling back to transformers.Trainer. Error: {exc}')\n    from transformers import Trainer, DataCollatorForLanguageModeling\n\n    def _tok(batch):\n        return tokenizer(batch['text'], truncation=True, max_length=2560)\n\n    tok_ds = sft_train_dataset.map(_tok, batched=True, remove_columns=['text'])\n\n    def _with_labels(batch):\n        batch['labels'] = batch['input_ids']\n        return batch\n\n    tok_ds = tok_ds.map(_with_labels)\n    collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n    fallback_args = TrainingArguments(\n        output_dir=str(SFT_OUT),\n        num_train_epochs=sft_epochs,\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        learning_rate=2e-4,\n        lr_scheduler_type='cosine',\n        warmup_steps=WARMUP_STEPS,\n        bf16=True,\n        logging_steps=5,\n        save_strategy='epoch',\n        save_total_limit=5,\n        report_to='none',\n    )\n\n    fallback_trainer = Trainer(\n        model=ft_model,\n        args=fallback_args,\n        train_dataset=tok_ds,\n        data_collator=collator,\n    )\n    fallback_trainer.train()\n    ft_model = fallback_trainer.model\n    print('Fallback Trainer stage complete.')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 9 — Save adapter and checkpoint training artifacts\nFINAL_ADAPTER_DIR = ADAPTER_OUT_DIR / 'final'\nFINAL_ADAPTER_DIR.mkdir(parents=True, exist_ok=True)\n\nft_model.save_pretrained(str(FINAL_ADAPTER_DIR))\ntokenizer.save_pretrained(str(FINAL_ADAPTER_DIR))\n\n# Save a compact training summary for reproducibility.\ntraining_summary = {\n    'model_id': MODEL_ID,\n    'seed': SEED,\n    'retention_levels': RETENTION_LEVELS,\n    'phase1_examples': int(len(phase1_df)),\n    'phase1_preference_pairs': int(len(pair_df)),\n    'dpo_ran': bool('dpo_ran' in globals() and dpo_ran),\n}\nwith open(ADAPTER_OUT_DIR / 'training_summary.json', 'w') as f:\n    json.dump(training_summary, f, indent=2)\n\nprint(f'Adapter saved to: {FINAL_ADAPTER_DIR}')\ngpu_mem_report('after training')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 10 — Phase 3: generate 4 hard held-out tasks\n_render_context = runtime['_render_context']\n_compress_records_to_target = runtime['_compress_records_to_target']\n_build_question = runtime['_build_question']\n_long_note = runtime['_long_note']\n\n\nfor fn_name in ['_render_context', '_compress_records_to_target', '_build_question', '_long_note']:\n    if fn_name not in runtime:\n        raise RuntimeError(\n            f'Missing runtime function {fn_name}. Check miragebench_experiments_colab.ipynb exports.'\n        )\n\n\ndef _choose_prereq_indices(rng: np.random.Generator, n_events: int, pivot_idx: int, count: int) -> List[int]:\n    early_pool = np.arange(8, max(12, int(0.35 * n_events)))\n    mid_pool = np.arange(max(12, int(0.35 * n_events)), max(13, pivot_idx - 2))\n\n    early_n = max(2, count // 2)\n    mid_n = count - early_n\n\n    if len(early_pool) < early_n:\n        early_n = max(1, len(early_pool))\n        mid_n = count - early_n\n    if len(mid_pool) < mid_n:\n        mid_n = max(1, len(mid_pool))\n\n    early_pick = rng.choice(early_pool, size=early_n, replace=False) if early_n > 0 else np.array([], dtype=int)\n    mid_pick = rng.choice(mid_pool, size=mid_n, replace=False) if mid_n > 0 else np.array([], dtype=int)\n    picks = sorted(set(int(x) for x in np.concatenate([early_pick, mid_pick])))\n    return picks[:count]\n\n\ndef build_hard_task(category: str, task_num: int) -> Any:\n    rng = np.random.default_rng(9000 + task_num)\n    n_events = int(rng.integers(100, 151))\n    prereq_count = int(rng.integers(5, 8))\n    distractor_count = int(rng.integers(3, 5))\n\n    task_prefix = {'incident': 'HXI', 'investment': 'HXV', 'narrative': 'HXN', 'hybrid': 'HXH'}[category]\n    task_id = f'{task_prefix}{task_num:02d}'\n\n    pivot_idx = int(rng.integers(int(0.70 * n_events), int(0.82 * n_events)))\n    distractor_indices = sorted(\n        int(x) for x in rng.choice(np.arange(pivot_idx + 3, n_events - 2), size=distractor_count, replace=False)\n    )\n    trap_idx = distractor_indices[0]\n\n    pivot_prereq_idx = _choose_prereq_indices(rng, n_events, pivot_idx, prereq_count)\n    trap_prereq_idx = _choose_prereq_indices(rng, n_events, trap_idx, prereq_count)\n\n    other_distractor_prereqs: Dict[int, List[int]] = {}\n    for d_idx in distractor_indices[1:]:\n        d_count = max(3, prereq_count - 2)\n        other_distractor_prereqs[d_idx] = _choose_prereq_indices(rng, n_events, d_idx, d_count)\n\n    records: List[Dict[str, Any]] = []\n\n    if category in {'incident', 'hybrid'}:\n        services = ['auth', 'ledger', 'cache', 'queue', 'api-gateway', 'billing', 'search', 'ingest']\n        for i in range(n_events):\n            marker = f'{task_id}-E{i+1:03d}'\n            ts = f'2026-08-{(i % 27) + 1:02d} {7 + (i % 12):02d}:{(i * 5) % 60:02d}'\n            service = services[i % len(services)]\n\n            role = 'routine'\n            sev = int(rng.integers(2, 6))\n            scope = int(rng.integers(2, 7))\n            event_type = 'Routine signal drift'\n\n            if i in pivot_prereq_idx:\n                role = 'setup'\n                sev = int(rng.integers(1, 4))\n                scope = int(rng.integers(1, 4))\n                event_type = 'Prerequisite latent dependency exposure'\n            elif i in trap_prereq_idx:\n                role = 'decoy_setup'\n                sev = int(rng.integers(2, 5))\n                scope = int(rng.integers(2, 5))\n                event_type = 'Trap prerequisite fully observable'\n            elif any(i in v for v in other_distractor_prereqs.values()):\n                role = 'decoy_setup'\n                sev = int(rng.integers(2, 5))\n                scope = int(rng.integers(2, 5))\n                event_type = 'Distractor precursor'\n            elif i == pivot_idx:\n                role = 'pivot'\n                sev = 11\n                scope = 10\n                event_type = 'Primary cascade root-cause event'\n            elif i in distractor_indices:\n                role = 'candidate'\n                target_scores = [97, 95, 93, 91]\n                score = target_scores[distractor_indices.index(i)]\n                sev = 11\n                scope = max(8, score // sev)\n                event_type = 'Competing high-salience distractor event'\n\n            composite = sev * scope\n            note_role = 'pivot' if role == 'pivot' else 'decoy' if role in {'candidate', 'decoy_setup'} else role\n            note = _long_note(rng, note_role if note_role in {'setup', 'pivot', 'decoy', 'routine'} else 'routine', 'incident operations')\n\n            if category == 'hybrid':\n                line = (\n                    f'[{marker}] {ts} | Domain=infra+portfolio | Service={service} | Event={event_type} | '\n                    f'Severity={sev} | Scope={scope} | Composite={composite} | LiquidityStress={int(rng.integers(1,10))} | {note}'\n                )\n            else:\n                line = (\n                    f'[{marker}] {ts} | Service={service} | Event={event_type} | '\n                    f'Severity={sev} | Scope={scope} | Composite={composite} | {note}'\n                )\n            records.append({'marker': marker, 'role': role, 'line': line, 'composite': composite})\n\n        preamble = (\n            f'Hard {category.title()} Ledger {task_id}\\n'\n            'Long timeline with multiple near-max distractors. Preserve the dominant causal pivot, not the easiest supported decoy.'\n        )\n        rule_block = 'Rule reminder: pivot = global argmax Composite over the full timeline.'\n\n    elif category == 'investment':\n        positions = [\n            'NorthRiver Utilities Carry',\n            'Aurelia AI Semiconductor Basket',\n            'Helios Grid Infrastructure',\n            'BlueHarbor Treasury Arbitrage',\n            'Cinder Logistics Credit',\n            'Orchid Macro Vol Sleeve',\n            'Sierra Transit Debt',\n        ]\n        pivot_position_name = 'Helios Grid Infrastructure'\n        cumulative = {p: 0.0 for p in positions}\n        pivot_ceiling = None\n\n        for i in range(n_events):\n            marker = f'{task_id}-E{i+1:03d}'\n            wk = f'Week-{i+1:03d}'\n            position = positions[i % len(positions)]\n\n            role = 'routine'\n            weekly = float(rng.normal(0.7, 0.8))\n\n            if i in pivot_prereq_idx:\n                role = 'setup'\n                weekly = float(rng.normal(0.2, 0.2))\n            elif i in trap_prereq_idx or any(i in v for v in other_distractor_prereqs.values()):\n                role = 'decoy_setup'\n                weekly = float(rng.normal(0.5, 0.3))\n            elif i == pivot_idx:\n                role = 'pivot'\n                position = pivot_position_name\n                weekly = 3.8\n            elif i in distractor_indices:\n                role = 'candidate'\n                weekly = 3.5\n\n            cumulative[position] += weekly\n\n            if i == pivot_idx:\n                cumulative[position] = max(cumulative.values()) + 8.0\n                pivot_ceiling = cumulative[position]\n            if i in distractor_indices and pivot_ceiling is not None:\n                margin = float(rng.uniform(0.10, 0.15))\n                cumulative[position] = min(cumulative[position], pivot_ceiling * (1.0 - margin))\n\n            if pivot_ceiling is not None and i > pivot_idx:\n                cumulative[position] = min(cumulative[position], pivot_ceiling - 0.4)\n\n            cum_val = cumulative[position]\n            note_role = 'pivot' if role == 'pivot' else 'decoy' if role in {'candidate', 'decoy_setup'} else role\n            note = _long_note(rng, note_role if note_role in {'setup', 'pivot', 'decoy', 'routine'} else 'routine', 'portfolio research')\n\n            line = (\n                f'[{marker}] {wk} | Position={position} | WeeklyReturn={weekly:+.2f}% | '\n                f'CumulativeReturn={cum_val:.2f}% | RegimeScore={int(rng.integers(1,7))} | {note}'\n            )\n            records.append({'marker': marker, 'role': role, 'line': line, 'position': position, 'cum': cum_val})\n\n        preamble = (\n            f'Hard Investment Committee Timeline {task_id}\\n'\n            'Several near-leading positions exist; preserve the true dominant cumulative-return pivot and its prerequisite chain.'\n        )\n        rule_block = 'Rule reminder: pivot = entry with max(CumulativeReturn) over the full timeline.'\n\n    else:\n        actors = ['Mira', 'Jonas', 'Elio', 'Sana', 'Iris', 'Cato', 'Rhea', 'Niko']\n        places = ['market ward', 'canal archive', 'north gate', 'assembly atrium', 'river embankment', 'citadel vault']\n\n        for i in range(n_events):\n            marker = f'{task_id}-E{i+1:03d}'\n            scene = f'Scene-{i+1:03d}'\n            actor = actors[i % len(actors)]\n            place = places[i % len(places)]\n\n            role = 'routine'\n            consequence = int(rng.integers(2, 11))\n            action = 'traded routine information'\n\n            if i in pivot_prereq_idx:\n                role = 'setup'\n                consequence = int(rng.integers(1, 4))\n                action = 'laid subtle causal groundwork'\n            elif i in trap_prereq_idx or any(i in v for v in other_distractor_prereqs.values()):\n                role = 'decoy_setup'\n                consequence = int(rng.integers(3, 7))\n                action = 'built visible but misleading momentum'\n            elif i == pivot_idx:\n                role = 'pivot'\n                consequence = 30\n                action = 'revealed decisive evidence that reconfigured the entire narrative'\n            elif i in distractor_indices:\n                role = 'candidate'\n                consequence = int(rng.integers(26, 28))\n                action = 'staged a high-drama but secondary turning beat'\n\n            note_role = 'pivot' if role == 'pivot' else 'decoy' if role in {'candidate', 'decoy_setup'} else role\n            note = _long_note(rng, note_role if note_role in {'setup', 'pivot', 'decoy', 'routine'} else 'routine', 'character dynamics')\n\n            line = (\n                f'[{marker}] {scene} | Actor={actor} | Location={place} | Action={action} | '\n                f'ConsequenceScore={consequence} | {note}'\n            )\n            records.append({'marker': marker, 'role': role, 'line': line, 'consequence': consequence, 'actor': actor})\n\n        preamble = (\n            f'Hard Narrative Consequence Ledger {task_id}\\n'\n            'Multiple dramatic decoys are close in score; preserve the true consequence-max pivot with its enabling scenes.'\n        )\n        rule_block = 'Rule reminder: pivot = argmax ConsequenceScore over all scenes.'\n\n    pivot_marker = f'{task_id}-E{pivot_idx+1:03d}'\n    distractor_markers = [f'{task_id}-E{i+1:03d}' for i in distractor_indices]\n    trap_marker = f'{task_id}-E{trap_idx+1:03d}'\n\n    pivot_setup_markers = [f'{task_id}-E{i+1:03d}' for i in pivot_prereq_idx]\n    trap_setup_markers = [f'{task_id}-E{i+1:03d}' for i in trap_prereq_idx]\n\n    candidate_requirements = {pivot_marker: pivot_setup_markers, trap_marker: trap_setup_markers}\n    for idx in distractor_indices[1:]:\n        d_marker = f'{task_id}-E{idx+1:03d}'\n        d_reqs = [f'{task_id}-E{i+1:03d}' for i in other_distractor_prereqs[idx]]\n        candidate_requirements[d_marker] = d_reqs\n\n    full_context, spans, appendix_text = _render_context(\n        preamble=preamble,\n        records=records,\n        rule_block=rule_block,\n        appendix_target_words=5200,\n        rng=rng,\n    )\n\n    metadata = {\n        'preamble': preamble,\n        'records': records,\n        'rule_block': rule_block,\n        'appendix_text': appendix_text,\n        'spans': spans,\n        'protected_markers': [pivot_marker, *distractor_markers, *trap_setup_markers],\n        'candidate_markers': [pivot_marker, *distractor_markers],\n        'candidate_requirements': candidate_requirements,\n        'pivot_setup_markers': pivot_setup_markers,\n        'trap_setup_markers': trap_setup_markers,\n        'hard_profile': {\n            'n_events': n_events,\n            'prereq_count': prereq_count,\n            'distractor_count': distractor_count,\n            'trap_marker': trap_marker,\n        },\n    }\n\n    compressed_context, actual_drop, removed_markers = _compress_records_to_target(\n        metadata,\n        target_drop_fraction=0.50,\n        rng=rng,\n    )\n    metadata['compression_default_drop'] = actual_drop\n    metadata['removed_markers_default'] = removed_markers\n\n    question = _build_question(category if category in {'incident', 'investment', 'narrative'} else 'incident')\n    if category == 'hybrid':\n        question = (\n            'Identify the dominant pivot event marker under the stated max-score rule, '\n            'justify with prerequisite support, and report uncertainty if prerequisites are missing.'\n        )\n\n    answer_gt = (\n        f'PIVOT_ID={pivot_marker}. Dominant pivot follows the maximum score rule. '\n        f'Prerequisite chain: {\" -> \".join(pivot_setup_markers + [pivot_marker])}.'\n    )\n    decoy_answer = (\n        f'PIVOT_ID={trap_marker}. Trap distractor appears coherent because all trap prerequisites remain visible after compression.'\n    )\n\n    return MirageBenchTask(\n        task_id=task_id,\n        category=category,\n        full_context=full_context,\n        compressed_context=compressed_context,\n        question=question,\n        pivot_ground_truth=pivot_marker,\n        answer_ground_truth=answer_gt,\n        decoy_pivot=trap_marker,\n        decoy_answer=decoy_answer,\n        k=3,\n        metadata=metadata,\n    )\n\n\nhard_tasks = [\n    build_hard_task('incident', 1),\n    build_hard_task('investment', 2),\n    build_hard_task('narrative', 3),\n    build_hard_task('hybrid', 4),\n]\n\nprint('Generated hard held-out tasks:')\nfor t in hard_tasks:\n    profile = t.metadata.get('hard_profile', {})\n    print(\n        f\"  {t.task_id} ({t.category}) | events={profile.get('n_events')} | \"\n        f\"prereqs={profile.get('prereq_count')} | distractors={profile.get('distractor_count')}\"\n    )\n\nprint('\\nTrap-prerequisite retention check @ retention=0.3:')\nfor t in hard_tasks:\n    drop = 0.7\n    comp = render_compressed_variant(t, drop_fraction=drop, seed=SEED)\n    trap = t.metadata['hard_profile']['trap_marker']\n    trap_reqs = t.metadata['candidate_requirements'][trap]\n    present = sum(1 for m in trap_reqs if m in comp)\n    print(f'  {t.task_id}: {present}/{len(trap_reqs)} trap prerequisites present')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 11 — Evaluate BASE model on hard tasks (checkpoint per task)\n\ndef has_gap_flag(text: str) -> bool:\n    return bool(re.search(r'\\[PREREQUISITE_GAP:', text or ''))\n\n\ndef evaluate_model_on_hard_tasks(model, tokenizer, model_name: str, task_list: List[Any], existing_rows: Optional[List[Dict[str, Any]]] = None):\n    eval_rows = list(existing_rows) if existing_rows is not None else []\n\n    for task in tqdm(task_list, desc=f'{model_name} hard-task eval'):\n        for retention in [1.0, 0.5, 0.3]:\n            drop_fraction = max(0.0, 1.0 - retention)\n            context_text = task.full_context if retention >= 0.999 else render_compressed_variant(\n                task,\n                drop_fraction=drop_fraction,\n                seed=SEED,\n            )\n            prompt = make_prompt(context_text, task.question)\n            answer = generate_response(model, tokenizer, prompt)\n            pivot = extract_pivot_id(answer, [task.pivot_ground_truth, task.decoy_pivot])\n\n            row = {\n                'task_id': task.task_id,\n                'category': task.category,\n                'compression': float(retention),\n                'model': model_name,\n                'extracted_pivot': pivot,\n                'pivot_correct': int(pivot == task.pivot_ground_truth),\n                'has_pivot_header': int('PIVOT_ID=' in (answer or '')),\n                'has_gap_flag': int(has_gap_flag(answer)),\n                'raw_validity': float(raw_validity_score(answer, task)),\n                'answer_first_500_chars': answer[:500],\n            }\n            eval_rows.append(row)\n\n        task_ckpt = EVAL_OUT_DIR / f'eval_checkpoint_{task.task_id}.csv'\n        pd.DataFrame([r for r in eval_rows if r['task_id'] == task.task_id]).to_csv(task_ckpt, index=False)\n\n        merged_ckpt = EVAL_OUT_DIR / 'eval_results_running.csv'\n        pd.DataFrame(eval_rows).to_csv(merged_ckpt, index=False)\n\n    return eval_rows\n\nbase_eval_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    device_map='auto',\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n)\nbase_eval_model.eval()\n\nbase_eval_rows = evaluate_model_on_hard_tasks(base_eval_model, tokenizer, 'base', hard_tasks)\nprint(f'Base eval rows: {len(base_eval_rows)}')\n\ndel base_eval_model\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 12 — Evaluate ADAPTED model on hard tasks (checkpoint per task)\nfrom peft import PeftModel\n\nadapt_backbone = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    device_map='auto',\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n)\nadapt_model = PeftModel.from_pretrained(adapt_backbone, str(FINAL_ADAPTER_DIR))\nadapt_model.eval()\n\nall_eval_rows = evaluate_model_on_hard_tasks(adapt_model, tokenizer, 'adapted', hard_tasks, existing_rows=base_eval_rows)\nprint(f'Total eval rows after adapted run: {len(all_eval_rows)}')\n\ndel adapt_model\ndel adapt_backbone\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 13 — Save merged evaluation results\neval_df = pd.DataFrame(all_eval_rows)\neval_df = eval_df[\n    [\n        'task_id', 'category', 'compression', 'model', 'extracted_pivot',\n        'pivot_correct', 'has_pivot_header', 'has_gap_flag', 'raw_validity',\n        'answer_first_500_chars',\n    ]\n]\n\nfinal_eval_csv = EVAL_OUT_DIR / 'eval_results.csv'\neval_df.to_csv(final_eval_csv, index=False)\n\nprint(f'Saved merged eval results to: {final_eval_csv}')\nprint('Rows by model:', eval_df.groupby('model').size().to_dict())\nprint('Rows by compression:', eval_df.groupby('compression').size().to_dict())"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 14 — Summary grid by compression\nbase_summary = (\n    eval_df[eval_df['model'] == 'base']\n    .groupby('compression', as_index=True)['pivot_correct']\n    .mean()\n    .rename('base_pivot_correct')\n)\n\nadapted_summary = (\n    eval_df[eval_df['model'] == 'adapted']\n    .groupby('compression', as_index=True)['pivot_correct']\n    .mean()\n    .rename('adapted_pivot_correct')\n)\n\nadapted_gap = (\n    eval_df[eval_df['model'] == 'adapted']\n    .groupby('compression', as_index=True)['has_gap_flag']\n    .mean()\n    .rename('adapted_gap_flag_rate')\n)\n\nsummary_grid = pd.concat([base_summary, adapted_summary, adapted_gap], axis=1).reset_index()\nsummary_grid = summary_grid.sort_values('compression', ascending=False)\n\nprint('=' * 84)\nprint('MIRAGE-RESISTANT SUMMARY (HARD HELD-OUT)')\nprint('=' * 84)\nprint(summary_grid.to_string(index=False, float_format=lambda x: f'{x:.3f}'))\n\nsummary_csv = EVAL_OUT_DIR / 'eval_summary_grid.csv'\nsummary_grid.to_csv(summary_csv, index=False)\nprint(f'\\nSaved summary grid to: {summary_csv}')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}