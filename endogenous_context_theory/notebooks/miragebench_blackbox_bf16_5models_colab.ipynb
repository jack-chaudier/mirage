{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MirageBench Blackbox BF16 5-Model Sweep (Colab)\n",
    "\n",
    "This notebook runs the MirageBench blackbox pipeline on the same eval set used for the existing Qwen blackbox run, using native bf16 model loading (no quantization).\n",
    "\n",
    "Models:\n",
    "- Llama 3.1 8B Instruct\n",
    "- Mistral 7B Instruct v0.3\n",
    "- Gemma 2 9B IT\n",
    "- Phi-3 Medium 14B Instruct\n",
    "- Qwen 2.5 14B Instruct\n",
    "\n",
    "Outputs:\n",
    "- One per-model CSV (saved to Google Drive immediately after each model finishes)\n",
    "- One merged CSV with a `model_name` column\n",
    "- One summary table grouped by `model_name` and `compression_level` with metric means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Install dependencies\n",
    "!pip -q install transformers==4.46.3 accelerate sentence-transformers scikit-learn pandas tqdm\n",
    "print('Dependencies installed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Mount Google Drive + prepare repo path\n",
    "from google.colab import drive\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "REPO_DIR = Path('/content/mirage')\n",
    "if not REPO_DIR.exists():\n",
    "    print('Cloning mirage repo...')\n",
    "    subprocess.run([\n",
    "        'git', 'clone', 'https://github.com/jack-chaudier/mirage.git', str(REPO_DIR)\n",
    "    ], check=True)\n",
    "else:\n",
    "    print(f'Repo already present at {REPO_DIR}')\n",
    "\n",
    "sys.path.insert(0, str(REPO_DIR))\n",
    "sys.path.insert(0, str(REPO_DIR / 'endogenous_context_theory'))\n",
    "\n",
    "print('Python paths configured.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Imports + reproducibility\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from endogenous_context_theory.scripts.run_miragebench_ollama import (\n",
    "    _load_notebook_runtime,\n",
    "    _patch_runtime_with_methodology_fixes,\n",
    "    _validate_investment_ground_truth,\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f'Torch: {torch.__version__}')\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Load MirageBench runtime exactly as used in Qwen blackbox pipeline\n",
    "ROOT = REPO_DIR / 'endogenous_context_theory'\n",
    "NB_PATH = ROOT / 'notebooks' / 'legacy' / 'miragebench_experiments_colab.ipynb'\n",
    "\n",
    "runtime = _load_notebook_runtime(NB_PATH)\n",
    "_patch_runtime_with_methodology_fixes(runtime)\n",
    "\n",
    "tasks = runtime['build_miragebench_v01']()\n",
    "_validate_investment_ground_truth(tasks)\n",
    "\n",
    "compression_levels = [0.4, 0.5, 0.6]  # same as blackbox run defaults\n",
    "\n",
    "make_prompt = runtime['make_prompt']\n",
    "render_compressed_variant = runtime['render_compressed_variant']\n",
    "raw_validity_score = runtime['raw_validity_score']\n",
    "semantic_regret = runtime['semantic_regret']\n",
    "\n",
    "print(f'Loaded {len(tasks)} MirageBench tasks.')\n",
    "print('Task IDs:', [t.task_id for t in tasks])\n",
    "print('Compression levels:', compression_levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Model registry + output paths\n",
    "MODELS = {\n",
    "    'Llama-3.1-8B-Instruct': 'meta-llama/Llama-3.1-8B-Instruct',\n",
    "    'Mistral-7B-Instruct-v0.3': 'mistralai/Mistral-7B-Instruct-v0.3',\n",
    "    'Gemma-2-9B-IT': 'google/gemma-2-9b-it',\n",
    "    'Phi-3-Medium-14B-Instruct': 'microsoft/Phi-3-medium-128k-instruct',\n",
    "    'Qwen-2.5-14B-Instruct': 'Qwen/Qwen2.5-14B-Instruct',\n",
    "}\n",
    "\n",
    "MAX_NEW_TOKENS = 220\n",
    "SKIP_IF_MODEL_CSV_EXISTS = True\n",
    "\n",
    "DRIVE_OUT_DIR = Path('/content/drive/MyDrive/miragebench_blackbox_bf16_7model')\n",
    "LOCAL_OUT_DIR = Path('/content/miragebench_blackbox_bf16_7model')\n",
    "DRIVE_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOCAL_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Drive output:', DRIVE_OUT_DIR)\n",
    "print('Local output:', LOCAL_OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Helpers: extraction, feasibility, generation, evaluation loop\n",
    "PIVOT_PRIMARY_RE = re.compile(r'PIVOT_ID\\s*=\\s*([A-Z]\\d{1,4}-E\\d{3})')\n",
    "PIVOT_FALLBACK_RE = re.compile(r'([A-Z]\\d{1,4}-E\\d{3})')\n",
    "\n",
    "\n",
    "def extract_pivot_id(text: str, fallback_candidates: List[str] | None = None) -> str:\n",
    "    if not text:\n",
    "        return ''\n",
    "    m = PIVOT_PRIMARY_RE.search(text)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    markers = PIVOT_FALLBACK_RE.findall(text)\n",
    "    if markers and fallback_candidates:\n",
    "        for c in fallback_candidates:\n",
    "            if c in markers:\n",
    "                return c\n",
    "    return markers[0] if markers else ''\n",
    "\n",
    "\n",
    "def compute_fixed_pivot_feasible(task: Any, full_pivot: str, compressed_context: str) -> bool:\n",
    "    # Feasibility of forcing compressed instance to the full-context pivot.\n",
    "    if not full_pivot:\n",
    "        return False\n",
    "\n",
    "    req_map = task.metadata.get('candidate_requirements', {}) if isinstance(task.metadata, dict) else {}\n",
    "    reqs = req_map.get(full_pivot, [])\n",
    "\n",
    "    if full_pivot not in compressed_context:\n",
    "        return False\n",
    "    return all(marker in compressed_context for marker in reqs)\n",
    "\n",
    "\n",
    "def format_chat_prompt(tokenizer, prompt: str) -> str:\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    if hasattr(tokenizer, 'apply_chat_template') and tokenizer.chat_template:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_answer(model, tokenizer, prompt: str, max_new_tokens: int = MAX_NEW_TOKENS) -> str:\n",
    "    input_text = format_chat_prompt(tokenizer, prompt)\n",
    "    inputs = tokenizer(input_text, return_tensors='pt').to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    new_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    return tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "def load_bf16_model(model_id: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model_name: str, model_id: str) -> pd.DataFrame:\n",
    "    print(f'\\n===== Evaluating {model_name} ({model_id}) =====')\n",
    "\n",
    "    model, tokenizer = load_bf16_model(model_id)\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    try:\n",
    "        for task in tqdm(tasks, desc=f'{model_name} tasks'):\n",
    "            full_prompt = make_prompt(task.full_context, task.question)\n",
    "            full_answer = generate_answer(model, tokenizer, full_prompt)\n",
    "            full_pivot = extract_pivot_id(full_answer, [task.pivot_ground_truth, task.decoy_pivot])\n",
    "            raw_validity_full = raw_validity_score(full_answer, task)\n",
    "\n",
    "            for lvl in compression_levels:\n",
    "                compressed_context = render_compressed_variant(task, drop_fraction=lvl, seed=SEED)\n",
    "                compressed_prompt = make_prompt(compressed_context, task.question)\n",
    "                compressed_answer = generate_answer(model, tokenizer, compressed_prompt)\n",
    "\n",
    "                compressed_pivot = extract_pivot_id(\n",
    "                    compressed_answer,\n",
    "                    [task.pivot_ground_truth, task.decoy_pivot],\n",
    "                )\n",
    "                raw_validity_compressed = raw_validity_score(compressed_answer, task)\n",
    "\n",
    "                row = {\n",
    "                    'model_name': model_name,\n",
    "                    'model_id': model_id,\n",
    "                    'task_id': task.task_id,\n",
    "                    'category': task.category,\n",
    "                    'compression_level': float(lvl),\n",
    "                    'full_pivot': full_pivot,\n",
    "                    'pivot_id_extracted': compressed_pivot,\n",
    "                    'pivot_extracted_flag': int(bool(compressed_pivot)),\n",
    "                    'pivot_preserved': int(bool(full_pivot and compressed_pivot and full_pivot == compressed_pivot)),\n",
    "                    'fixed_pivot_feasible': int(compute_fixed_pivot_feasible(task, full_pivot, compressed_context)),\n",
    "                    'raw_validity': float(raw_validity_compressed),\n",
    "                    'raw_validity_full': float(raw_validity_full),\n",
    "                    'raw_validity_compressed': float(raw_validity_compressed),\n",
    "                    'semantic_regret': float(semantic_regret(full_answer, compressed_answer)),\n",
    "                    'full_pivot_matches_ground_truth': int(full_pivot == task.pivot_ground_truth),\n",
    "                    'pivot_matches_ground_truth': int(compressed_pivot == task.pivot_ground_truth),\n",
    "                    'full_answer': full_answer,\n",
    "                    'compressed_answer': compressed_answer,\n",
    "                }\n",
    "                rows.append(row)\n",
    "\n",
    "        df = pd.DataFrame(rows)\n",
    "        return df\n",
    "\n",
    "    finally:\n",
    "        del model\n",
    "        del tokenizer\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Run all models with per-model checkpoint saves to Google Drive\n",
    "all_frames: List[pd.DataFrame] = []\n",
    "\n",
    "for model_name, model_id in MODELS.items():\n",
    "    model_slug = model_name.lower().replace('/', '_').replace(' ', '_').replace('.', '_').replace('-', '_')\n",
    "    drive_csv = DRIVE_OUT_DIR / f'{model_slug}_results.csv'\n",
    "    local_csv = LOCAL_OUT_DIR / f'{model_slug}_results.csv'\n",
    "\n",
    "    if SKIP_IF_MODEL_CSV_EXISTS and drive_csv.exists():\n",
    "        print(f'Skipping {model_name}; found existing checkpoint: {drive_csv}')\n",
    "        df_model = pd.read_csv(drive_csv)\n",
    "        all_frames.append(df_model)\n",
    "        continue\n",
    "\n",
    "    df_model = evaluate_model(model_name, model_id)\n",
    "    df_model.to_csv(local_csv, index=False)\n",
    "    df_model.to_csv(drive_csv, index=False)\n",
    "    print(f'Saved checkpoint CSV for {model_name}')\n",
    "    print(f'  Local: {local_csv}')\n",
    "    print(f'  Drive: {drive_csv}')\n",
    "\n",
    "    all_frames.append(df_model)\n",
    "\n",
    "if not all_frames:\n",
    "    raise RuntimeError('No model outputs available.')\n",
    "\n",
    "merged = pd.concat(all_frames, ignore_index=True)\n",
    "merged_local = LOCAL_OUT_DIR / 'miragebench_bf16_7model_merged.csv'\n",
    "merged_drive = DRIVE_OUT_DIR / 'miragebench_bf16_7model_merged.csv'\n",
    "merged.to_csv(merged_local, index=False)\n",
    "merged.to_csv(merged_drive, index=False)\n",
    "\n",
    "print('\\nMerged CSV saved:')\n",
    "print('  Local:', merged_local)\n",
    "print('  Drive:', merged_drive)\n",
    "print('Rows:', len(merged))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Summary table: grouped by model and compression level (means of 5 metrics)\n",
    "summary_metrics = [\n",
    "    'raw_validity',\n",
    "    'pivot_extracted_flag',\n",
    "    'pivot_preserved',\n",
    "    'fixed_pivot_feasible',\n",
    "    'semantic_regret',\n",
    "]\n",
    "\n",
    "summary = (\n",
    "    merged.groupby(['model_name', 'compression_level'], as_index=False)[summary_metrics]\n",
    "    .mean()\n",
    "    .sort_values(['model_name', 'compression_level'])\n",
    ")\n",
    "\n",
    "summary_local = LOCAL_OUT_DIR / 'miragebench_bf16_7model_summary_by_model_compression.csv'\n",
    "summary_drive = DRIVE_OUT_DIR / 'miragebench_bf16_7model_summary_by_model_compression.csv'\n",
    "summary.to_csv(summary_local, index=False)\n",
    "summary.to_csv(summary_drive, index=False)\n",
    "\n",
    "print('Summary CSV saved:')\n",
    "print('  Local:', summary_local)\n",
    "print('  Drive:', summary_drive)\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- This notebook uses the same MirageBench task generation path as the existing Qwen blackbox pipeline by loading runtime cells from `miragebench_experiments_colab.ipynb` and applying `_patch_runtime_with_methodology_fixes`.\n",
    "- Models are loaded in native bf16 (`torch_dtype=torch.bfloat16`) with `device_map=\"auto\"`; no bitsandbytes quantization is used in this notebook.\n",
    "- Per-model CSVs are written to Google Drive immediately after each model finishes.\n",
    "- If Colab disconnects, rerun and keep `SKIP_IF_MODEL_CSV_EXISTS=True` to resume from saved checkpoints.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}